running on cuda:0
HetDDI(
  (kg): HGNN(
    (dropout): Dropout(p=0.2, inplace=False)
    (node_embedding): Embedding(97244, 300)
    (gat_layers): ModuleList(
      (0): HetConv(
        (edge_embedding): Embedding(108, 300)
        (bn): Sequential(
          (0): Linear(in_features=300, out_features=300, bias=True)
          (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (leaky_relu): LeakyReLU(negative_slope=0.02)
      )
      (1): HetConv(
        (edge_embedding): Embedding(108, 300)
        (bn): Sequential(
          (0): Linear(in_features=300, out_features=300, bias=True)
          (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (leaky_relu): LeakyReLU(negative_slope=0.02)
      )
      (2): HetConv(
        (edge_embedding): Embedding(108, 300)
        (bn): Sequential(
          (0): Linear(in_features=300, out_features=300, bias=True)
          (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (leaky_relu): LeakyReLU(negative_slope=0.02)
      )
    )
  )
  (kg_fc): Sequential(
    (0): Linear(in_features=300, out_features=300, bias=True)
    (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Dropout(p=0.1, inplace=False)
    (3): ReLU()
    (4): Linear(in_features=300, out_features=300, bias=True)
    (5): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Dropout(p=0.1, inplace=False)
    (7): ReLU()
    (8): Linear(in_features=300, out_features=300, bias=True)
    (9): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Dropout(p=0.1, inplace=False)
    (11): ReLU()
  )
  (mol): Mol(
    (readout): AvgPooling()
    (gnn): HGNN(
      (dropout): Dropout(p=0.2, inplace=False)
      (node_embedding): Embedding(47601, 300)
      (gat_layers): ModuleList(
        (0): HetConv(
          (edge_embedding): Embedding(5, 300)
          (bn): Sequential(
            (0): Linear(in_features=300, out_features=300, bias=True)
            (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (leaky_relu): LeakyReLU(negative_slope=0.02)
        )
        (1): HetConv(
          (edge_embedding): Embedding(5, 300)
          (bn): Sequential(
            (0): Linear(in_features=300, out_features=300, bias=True)
            (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (leaky_relu): LeakyReLU(negative_slope=0.02)
        )
        (2): HetConv(
          (edge_embedding): Embedding(5, 300)
          (bn): Sequential(
            (0): Linear(in_features=300, out_features=300, bias=True)
            (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (leaky_relu): LeakyReLU(negative_slope=0.02)
        )
      )
    )
  )
  (mol_fc): Sequential(
    (0): Linear(in_features=300, out_features=300, bias=True)
    (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Dropout(p=0.1, inplace=False)
    (3): ReLU()
    (4): Linear(in_features=300, out_features=300, bias=True)
    (5): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Dropout(p=0.1, inplace=False)
    (7): ReLU()
    (8): Linear(in_features=300, out_features=300, bias=True)
    (9): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Dropout(p=0.1, inplace=False)
    (11): ReLU()
  )
  (decoder): Mlp(
    (fc_layer): Sequential(
      (0): Linear(in_features=1200, out_features=1200, bias=True)
      (1): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.5, inplace=False)
      (4): Linear(in_features=1200, out_features=1200, bias=True)
      (5): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): ReLU()
      (7): Dropout(p=0.5, inplace=False)
      (8): Linear(in_features=1200, out_features=1200, bias=True)
      (9): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): ReLU()
      (11): Dropout(p=0.5, inplace=False)
    )
    (output_layer): Sequential(
      (0): Linear(in_features=1200, out_features=1, bias=False)
    )
  )
)
fold:0 epoch:0 step:0 train loss:0.723639, train acc:50.928, train f1:49.627, train precision:50.834, train recall:48.476, train auc:51.141
fold:0 epoch:0 step:1 train loss:1.026579, train acc:58.646, train f1:49.020, train precision:63.536, train recall:39.903, train auc:62.431
fold:0 epoch:0 step:2 train loss:0.731259, train acc:54.529, train f1:61.570, train precision:53.422, train recall:72.652, train auc:58.286
fold:0 epoch:0 step:3 train loss:0.649550, train acc:63.474, train f1:68.168, train precision:60.093, train recall:78.751, train auc:70.032
fold:0 epoch:0 step:4 train loss:0.613006, train acc:69.116, train f1:70.516, train precision:67.677, train recall:73.604, train auc:76.014
fold:0 epoch:0 step:5 train loss:0.573742, train acc:70.932, train f1:70.726, train precision:70.468, train recall:70.985, train auc:77.549
fold:0 epoch:0 step:6 train loss:0.569743, train acc:70.657, train f1:68.439, train precision:74.518, train recall:63.278, train auc:78.316
fold:0 epoch:0 step:7 train loss:0.541663, train acc:73.517, train f1:71.805, train precision:77.127, train recall:67.169, train auc:81.149
fold:0 epoch:0 step:8 train loss:0.532788, train acc:75.140, train f1:74.683, train precision:76.373, train recall:73.066, train auc:82.480
fold:0 epoch:0 step:9 train loss:0.507287, train acc:76.887, train f1:77.191, train precision:77.044, train recall:77.339, train auc:83.922
fold:0 epoch:0        valid loss:0.542715, valid acc:76.108, valid f1:74.221, valid precision:80.585, valid recall:68.789, valid auc:84.064
None
====================================================================================================
fold:0 epoch:1 step:0 train loss:0.497951, train acc:77.133, train f1:77.723, train precision:76.336, train recall:79.161, train auc:84.183
fold:0 epoch:1 step:1 train loss:0.501571, train acc:76.718, train f1:77.181, train precision:75.389, train recall:79.061, train auc:83.636
fold:0 epoch:1 step:2 train loss:0.486863, train acc:77.707, train f1:77.990, train precision:77.017, train recall:78.987, train auc:84.765
fold:0 epoch:1 step:3 train loss:0.478316, train acc:78.293, train f1:78.354, train precision:78.100, train recall:78.610, train auc:85.424
fold:0 epoch:1 step:4 train loss:0.472645, train acc:78.967, train f1:79.105, train precision:78.676, train recall:79.539, train auc:85.813
fold:0 epoch:1 step:5 train loss:0.474321, train acc:78.723, train f1:79.116, train precision:78.077, train recall:80.182, train auc:85.856
fold:0 epoch:1 step:6 train loss:0.474329, train acc:78.503, train f1:78.727, train precision:77.165, train recall:80.353, train auc:85.928
fold:0 epoch:1 step:7 train loss:0.468913, train acc:78.967, train f1:79.370, train precision:78.053, train recall:80.733, train auc:86.030
fold:0 epoch:1 step:8 train loss:0.463843, train acc:79.044, train f1:79.341, train precision:77.968, train recall:80.762, train auc:86.233
fold:0 epoch:1 step:9 train loss:0.465403, train acc:78.637, train f1:79.011, train precision:77.927, train recall:80.126, train auc:86.117
fold:0 epoch:1        valid loss:0.593496, valid acc:69.713, valid f1:60.019, valid precision:88.271, valid recall:45.467, valid auc:86.150
None
====================================================================================================
fold:0 epoch:2 step:0 train loss:0.459037, train acc:79.160, train f1:79.318, train precision:78.132, train recall:80.540, train auc:86.603
fold:0 epoch:2 step:1 train loss:0.447928, train acc:80.075, train f1:80.298, train precision:79.239, train recall:81.386, train auc:87.316
fold:0 epoch:2 step:2 train loss:0.447126, train acc:79.715, train f1:80.175, train precision:78.787, train recall:81.614, train auc:87.256
fold:0 epoch:2 step:3 train loss:0.441915, train acc:79.700, train f1:80.238, train precision:78.275, train recall:82.301, train auc:87.520
fold:0 epoch:2 step:4 train loss:0.444652, train acc:79.761, train f1:80.557, train precision:77.412, train recall:83.969, train auc:87.421
fold:0 epoch:2 step:5 train loss:0.434776, train acc:80.267, train f1:81.176, train precision:77.836, train recall:84.816, train auc:87.971
fold:0 epoch:2 step:6 train loss:0.435298, train acc:80.231, train f1:81.119, train precision:77.930, train recall:84.580, train auc:87.978
fold:0 epoch:2 step:7 train loss:0.437815, train acc:80.359, train f1:81.087, train precision:78.504, train recall:83.847, train auc:87.841
fold:0 epoch:2 step:8 train loss:0.436983, train acc:79.868, train f1:80.403, train precision:78.194, train recall:82.740, train auc:87.809
fold:0 epoch:2 step:9 train loss:0.427636, train acc:80.185, train f1:80.552, train precision:77.870, train recall:83.426, train auc:88.287
fold:0 epoch:2        valid loss:0.985222, valid acc:60.238, valid f1:36.228, valid precision:91.445, valid recall:22.588, valid auc:85.962
None
====================================================================================================
fold:0 epoch:3 step:0 train loss:0.422209, train acc:80.786, train f1:81.095, train precision:79.665, train recall:82.578, train auc:88.678
fold:0 epoch:3 step:1 train loss:0.426112, train acc:80.527, train f1:80.853, train precision:79.774, train recall:81.963, train auc:88.433
fold:0 epoch:3 step:2 train loss:0.420564, train acc:80.630, train f1:81.006, train precision:79.430, train recall:82.645, train auc:88.719
fold:0 epoch:3 step:3 train loss:0.416314, train acc:81.064, train f1:81.709, train precision:79.023, train recall:84.583, train auc:88.931
fold:0 epoch:3 step:4 train loss:0.414259, train acc:81.134, train f1:81.887, train precision:78.176, train recall:85.967, train auc:89.057
fold:0 epoch:3 step:5 train loss:0.409970, train acc:81.418, train f1:82.109, train precision:78.534, train recall:86.024, train auc:89.266
fold:0 epoch:3 step:6 train loss:0.404307, train acc:81.641, train f1:82.331, train precision:79.791, train recall:85.038, train auc:89.585
fold:0 epoch:3 step:7 train loss:0.414056, train acc:81.116, train f1:81.715, train precision:79.156, train recall:84.445, train auc:89.060
fold:0 epoch:3 step:8 train loss:0.401863, train acc:81.851, train f1:82.392, train precision:80.642, train recall:84.220, train auc:89.731
fold:0 epoch:3 step:9 train loss:0.396217, train acc:82.032, train f1:82.582, train precision:80.529, train recall:84.742, train auc:89.879
fold:0 epoch:3        valid loss:1.044385, valid acc:55.894, valid f1:22.427, valid precision:92.973, valid recall:12.752, valid auc:85.865
None
====================================================================================================
fold:0 epoch:4 step:0 train loss:0.400925, train acc:81.750, train f1:82.422, train precision:79.451, train recall:85.624, train auc:89.736
fold:0 epoch:4 step:1 train loss:0.399470, train acc:82.025, train f1:82.810, train precision:80.012, train recall:85.810, train auc:89.824
fold:0 epoch:4 step:2 train loss:0.400282, train acc:81.815, train f1:82.466, train precision:79.724, train recall:85.403, train auc:89.729
fold:0 epoch:4 step:3 train loss:0.392692, train acc:82.361, train f1:83.035, train precision:79.807, train recall:86.535, train auc:90.166
fold:0 epoch:4 step:4 train loss:0.386533, train acc:82.776, train f1:83.339, train precision:80.718, train recall:86.136, train auc:90.441
fold:0 epoch:4 step:5 train loss:0.389903, train acc:82.254, train f1:82.738, train precision:79.972, train recall:85.702, train auc:90.244
fold:0 epoch:4 step:6 train loss:0.385464, train acc:82.672, train f1:83.257, train precision:80.687, train recall:85.995, train auc:90.476
fold:0 epoch:4 step:7 train loss:0.388597, train acc:82.300, train f1:82.861, train precision:80.312, train recall:85.577, train auc:90.284
fold:0 epoch:4 step:8 train loss:0.387494, train acc:82.535, train f1:83.066, train precision:80.389, train recall:85.926, train auc:90.351
fold:0 epoch:4 step:9 train loss:0.376221, train acc:83.061, train f1:83.769, train precision:80.564, train recall:87.239, train auc:90.897
fold:0 epoch:4        valid loss:0.746322, valid acc:61.909, valid f1:41.340, valid precision:89.873, valid recall:26.843, valid auc:85.878
None
====================================================================================================
fold:0 epoch:5 step:0 train loss:0.380224, train acc:82.907, train f1:83.594, train precision:80.721, train recall:86.678, train auc:90.645
fold:0 epoch:5 step:1 train loss:0.374164, train acc:83.237, train f1:84.069, train precision:80.379, train recall:88.115, train auc:90.993
fold:0 epoch:5 step:2 train loss:0.371291, train acc:83.630, train f1:84.450, train precision:80.604, train recall:88.682, train auc:91.169
fold:0 epoch:5 step:3 train loss:0.374588, train acc:83.295, train f1:84.032, train precision:79.764, train recall:88.781, train auc:90.995
fold:0 epoch:5 step:4 train loss:0.369195, train acc:83.246, train f1:83.941, train precision:80.219, train recall:88.025, train auc:91.169
fold:0 epoch:5 step:5 train loss:0.369581, train acc:83.313, train f1:84.060, train precision:81.301, train recall:87.013, train auc:91.191
fold:0 epoch:5 step:6 train loss:0.366813, train acc:83.658, train f1:84.416, train precision:81.579, train recall:87.458, train auc:91.399
fold:0 epoch:5 step:7 train loss:0.369333, train acc:83.508, train f1:84.265, train precision:79.971, train recall:89.046, train auc:91.245
fold:0 epoch:5 step:8 train loss:0.360594, train acc:83.884, train f1:84.485, train precision:80.311, train recall:89.116, train auc:91.668
fold:0 epoch:5 step:9 train loss:0.368304, train acc:83.720, train f1:84.527, train precision:81.299, train recall:88.022, train auc:91.263
fold:0 epoch:5        valid loss:0.533406, valid acc:73.569, valid f1:68.443, valid precision:84.911, valid recall:57.325, valid auc:86.705
None
====================================================================================================
fold:0 epoch:6 step:0 train loss:0.346790, train acc:84.705, train f1:85.467, train precision:82.128, train recall:89.088, train auc:92.300
fold:0 epoch:6 step:1 train loss:0.347459, train acc:84.668, train f1:85.241, train precision:81.611, train recall:89.209, train auc:92.290
fold:0 epoch:6 step:2 train loss:0.349426, train acc:84.485, train f1:85.074, train precision:81.794, train recall:88.629, train auc:92.168
fold:0 epoch:6 step:3 train loss:0.344298, train acc:84.915, train f1:85.499, train precision:81.943, train recall:89.377, train auc:92.454
fold:0 epoch:6 step:4 train loss:0.338841, train acc:85.037, train f1:85.688, train precision:82.340, train recall:89.320, train auc:92.705
fold:0 epoch:6 step:5 train loss:0.335796, train acc:85.205, train f1:85.782, train precision:82.075, train recall:89.840, train auc:92.806
fold:0 epoch:6 step:6 train loss:0.335516, train acc:85.086, train f1:85.665, train precision:82.242, train recall:89.385, train auc:92.804
fold:0 epoch:6 step:7 train loss:0.330789, train acc:85.443, train f1:86.007, train precision:83.513, train recall:88.654, train auc:93.061
fold:0 epoch:6 step:8 train loss:0.335021, train acc:85.178, train f1:85.638, train precision:83.062, train recall:88.380, train auc:92.859
fold:0 epoch:6 step:9 train loss:0.325069, train acc:85.831, train f1:86.277, train precision:84.078, train recall:88.593, train auc:93.340
fold:0 epoch:6        valid loss:0.526314, valid acc:74.267, valid f1:68.315, valid precision:88.868, valid recall:55.484, valid auc:88.648
None
====================================================================================================
fold:0 epoch:7 step:0 train loss:0.319494, train acc:86.111, train f1:86.579, train precision:84.101, train recall:89.207, train auc:93.565
fold:0 epoch:7 step:1 train loss:0.315781, train acc:86.288, train f1:86.782, train precision:84.054, train recall:89.692, train auc:93.626
fold:0 epoch:7 step:2 train loss:0.313361, train acc:86.505, train f1:86.835, train precision:83.966, train recall:89.908, train auc:93.764
fold:0 epoch:7 step:3 train loss:0.312372, train acc:86.572, train f1:86.935, train precision:84.089, train recall:89.981, train auc:93.796
fold:0 epoch:7 step:4 train loss:0.302560, train acc:87.198, train f1:87.512, train precision:85.291, train recall:89.853, train auc:94.205
fold:0 epoch:7 step:5 train loss:0.306485, train acc:86.667, train f1:87.024, train precision:84.962, train recall:89.188, train auc:94.028
fold:0 epoch:7 step:6 train loss:0.308081, train acc:86.642, train f1:87.056, train precision:84.885, train recall:89.341, train auc:94.002
fold:0 epoch:7 step:7 train loss:0.299752, train acc:87.128, train f1:87.523, train precision:84.925, train recall:90.284, train auc:94.307
fold:0 epoch:7 step:8 train loss:0.297545, train acc:87.161, train f1:87.603, train precision:84.903, train recall:90.480, train auc:94.398
fold:0 epoch:7 step:9 train loss:0.300040, train acc:87.309, train f1:87.634, train precision:85.345, train recall:90.049, train auc:94.285
fold:0 epoch:7        valid loss:0.413467, valid acc:80.517, valid f1:77.839, valid precision:90.245, valid recall:68.432, valid auc:92.454
None
====================================================================================================
fold:0 epoch:8 step:0 train loss:0.287567, train acc:87.531, train f1:87.820, train precision:85.003, train recall:90.831, train auc:94.801
fold:0 epoch:8 step:1 train loss:0.289538, train acc:87.659, train f1:87.949, train precision:85.518, train recall:90.523, train auc:94.679
fold:0 epoch:8 step:2 train loss:0.286324, train acc:87.726, train f1:87.954, train precision:85.956, train recall:90.047, train auc:94.827
fold:0 epoch:8 step:3 train loss:0.288367, train acc:87.689, train f1:88.017, train precision:85.914, train recall:90.225, train auc:94.730
fold:0 epoch:8 step:4 train loss:0.284836, train acc:87.698, train f1:88.052, train precision:85.574, train recall:90.678, train auc:94.853
fold:0 epoch:8 step:5 train loss:0.283184, train acc:88.049, train f1:88.457, train precision:86.240, train recall:90.790, train auc:94.892
fold:0 epoch:8 step:6 train loss:0.283742, train acc:87.827, train f1:88.252, train precision:85.740, train recall:90.916, train auc:94.920
fold:0 epoch:8 step:7 train loss:0.278949, train acc:88.217, train f1:88.640, train precision:85.957, train recall:91.497, train auc:95.040
fold:0 epoch:8 step:8 train loss:0.274740, train acc:88.120, train f1:88.459, train precision:86.008, train recall:91.053, train auc:95.247
fold:0 epoch:8 step:9 train loss:0.273769, train acc:88.329, train f1:88.640, train precision:85.968, train recall:91.483, train auc:95.200
fold:0 epoch:8        valid loss:0.325466, valid acc:85.262, valid f1:84.105, valid precision:91.269, valid recall:77.984, valid auc:95.024
None
====================================================================================================
fold:0 epoch:9 step:0 train loss:0.280360, train acc:87.918, train f1:88.193, train precision:86.306, train recall:90.164, train auc:95.051
fold:0 epoch:9 step:1 train loss:0.268273, train acc:88.483, train f1:88.782, train precision:86.504, train recall:91.183, train auc:95.435
fold:0 epoch:9 step:2 train loss:0.269126, train acc:88.306, train f1:88.677, train precision:86.236, train recall:91.261, train auc:95.387
fold:0 epoch:9 step:3 train loss:0.269622, train acc:88.458, train f1:88.783, train precision:85.609, train recall:92.201, train auc:95.397
fold:0 epoch:9 step:4 train loss:0.265991, train acc:88.571, train f1:88.937, train precision:86.443, train recall:91.580, train auc:95.514
fold:0 epoch:9 step:5 train loss:0.267548, train acc:88.559, train f1:88.975, train precision:86.484, train recall:91.612, train auc:95.472
fold:0 epoch:9 step:6 train loss:0.272000, train acc:88.263, train f1:88.613, train precision:85.725, train recall:91.703, train auc:95.313
fold:0 epoch:9 step:7 train loss:0.264625, train acc:88.693, train f1:88.940, train precision:86.145, train recall:91.923, train auc:95.543
fold:0 epoch:9 step:8 train loss:0.264906, train acc:88.638, train f1:88.929, train precision:86.931, train recall:91.021, train auc:95.550
fold:0 epoch:9 step:9 train loss:0.249085, train acc:89.402, train f1:89.779, train precision:88.141, train recall:91.478, train auc:96.068
fold:0 epoch:9        valid loss:0.320333, valid acc:85.629, valid f1:84.554, valid precision:91.391, valid recall:78.668, valid auc:95.345
None
====================================================================================================
fold:0 epoch:10 step:0 train loss:0.263190, train acc:88.806, train f1:89.134, train precision:86.222, train recall:92.249, train auc:95.605
fold:0 epoch:10 step:1 train loss:0.257702, train acc:89.023, train f1:89.482, train precision:87.061, train recall:92.042, train auc:95.786
fold:0 epoch:10 step:2 train loss:0.256792, train acc:88.943, train f1:89.303, train precision:85.990, train recall:92.882, train auc:95.791
fold:0 epoch:10 step:3 train loss:0.256698, train acc:88.889, train f1:89.235, train precision:86.680, train recall:91.945, train auc:95.800
fold:0 epoch:10 step:4 train loss:0.257699, train acc:88.858, train f1:89.193, train precision:87.087, train recall:91.403, train auc:95.780
fold:0 epoch:10 step:5 train loss:0.253166, train acc:88.950, train f1:89.227, train precision:86.692, train recall:91.915, train auc:95.941
fold:0 epoch:10 step:6 train loss:0.252930, train acc:89.127, train f1:89.353, train precision:86.352, train recall:92.570, train auc:95.947
fold:0 epoch:10 step:7 train loss:0.252598, train acc:89.157, train f1:89.402, train precision:87.153, train recall:91.770, train auc:95.940
fold:0 epoch:10 step:8 train loss:0.254144, train acc:89.365, train f1:89.687, train precision:87.650, train recall:91.820, train auc:95.904
fold:0 epoch:10 step:9 train loss:0.259306, train acc:88.646, train f1:89.031, train precision:86.782, train recall:91.399, train auc:95.666
fold:0 epoch:10        valid loss:0.280454, valid acc:87.695, valid f1:87.265, valid precision:90.423, valid recall:84.320, valid auc:95.795
None
====================================================================================================
fold:0 epoch:11 step:0 train loss:0.245741, train acc:89.420, train f1:89.811, train precision:87.100, train recall:92.696, train auc:96.163
fold:0 epoch:11 step:1 train loss:0.251961, train acc:89.111, train f1:89.497, train precision:86.527, train recall:92.678, train auc:95.969
fold:0 epoch:11 step:2 train loss:0.243333, train acc:89.691, train f1:89.906, train precision:87.246, train recall:92.732, train auc:96.261
fold:0 epoch:11 step:3 train loss:0.248225, train acc:89.203, train f1:89.444, train precision:87.182, train recall:91.828, train auc:96.081
fold:0 epoch:11 step:4 train loss:0.247172, train acc:89.319, train f1:89.555, train precision:87.283, train recall:91.948, train auc:96.148
fold:0 epoch:11 step:5 train loss:0.239396, train acc:89.584, train f1:89.837, train precision:87.760, train recall:92.015, train auc:96.356
fold:0 epoch:11 step:6 train loss:0.241060, train acc:89.468, train f1:89.735, train precision:87.322, train recall:92.285, train auc:96.254
fold:0 epoch:11 step:7 train loss:0.244862, train acc:89.337, train f1:89.728, train precision:87.535, train recall:92.033, train auc:96.167
fold:0 epoch:11 step:8 train loss:0.243533, train acc:89.365, train f1:89.674, train precision:86.881, train recall:92.652, train auc:96.240
fold:0 epoch:11 step:9 train loss:0.246609, train acc:89.147, train f1:89.535, train precision:87.199, train recall:92.001, train auc:96.143
fold:0 epoch:11        valid loss:0.256522, valid acc:88.669, valid f1:88.364, valid precision:90.810, valid recall:86.047, valid auc:96.356
None
====================================================================================================
fold:0 epoch:12 step:0 train loss:0.236822, train acc:89.737, train f1:90.081, train precision:87.423, train recall:92.906, train auc:96.442
fold:0 epoch:12 step:1 train loss:0.232184, train acc:89.975, train f1:90.258, train precision:87.470, train recall:93.230, train auc:96.598
fold:0 epoch:12 step:2 train loss:0.235910, train acc:89.655, train f1:89.933, train precision:87.410, train recall:92.606, train auc:96.486
fold:0 epoch:12 step:3 train loss:0.239867, train acc:89.734, train f1:90.059, train precision:87.599, train recall:92.660, train auc:96.333
fold:0 epoch:12 step:4 train loss:0.230943, train acc:90.097, train f1:90.391, train precision:87.824, train recall:93.112, train auc:96.579
fold:0 epoch:12 step:5 train loss:0.233828, train acc:89.795, train f1:89.996, train precision:87.800, train recall:92.304, train auc:96.532
fold:0 epoch:12 step:6 train loss:0.238198, train acc:89.673, train f1:89.928, train precision:88.474, train recall:91.430, train auc:96.420
fold:0 epoch:12 step:7 train loss:0.235463, train acc:89.700, train f1:89.961, train precision:87.628, train recall:92.421, train auc:96.486
fold:0 epoch:12 step:8 train loss:0.237038, train acc:89.612, train f1:89.928, train precision:87.358, train recall:92.653, train auc:96.408
fold:0 epoch:12 step:9 train loss:0.232868, train acc:90.009, train f1:90.257, train precision:87.033, train recall:93.730, train auc:96.578
fold:0 epoch:12        valid loss:0.242740, valid acc:89.573, valid f1:89.468, valid precision:90.379, valid recall:88.575, valid auc:96.690
None
====================================================================================================
fold:0 epoch:13 step:0 train loss:0.225418, train acc:90.262, train f1:90.545, train precision:87.775, train recall:93.495, train auc:96.779
fold:0 epoch:13 step:1 train loss:0.230874, train acc:90.027, train f1:90.239, train precision:87.505, train recall:93.149, train auc:96.605
fold:0 epoch:13 step:2 train loss:0.226965, train acc:90.152, train f1:90.336, train precision:88.251, train recall:92.522, train auc:96.726
fold:0 epoch:13 step:3 train loss:0.224926, train acc:90.222, train f1:90.446, train precision:88.143, train recall:92.872, train auc:96.782
fold:0 epoch:13 step:4 train loss:0.227462, train acc:90.027, train f1:90.322, train precision:88.263, train recall:92.480, train auc:96.742
fold:0 epoch:13 step:5 train loss:0.228836, train acc:89.890, train f1:90.184, train precision:87.880, train recall:92.612, train auc:96.683
fold:0 epoch:13 step:6 train loss:0.224843, train acc:90.302, train f1:90.569, train precision:88.147, train recall:93.128, train auc:96.812
fold:0 epoch:13 step:7 train loss:0.222346, train acc:90.518, train f1:90.806, train precision:88.909, train recall:92.786, train auc:96.897
fold:0 epoch:13 step:8 train loss:0.220084, train acc:90.396, train f1:90.663, train precision:88.343, train recall:93.108, train auc:96.909
fold:0 epoch:13 step:9 train loss:0.212491, train acc:90.668, train f1:90.968, train precision:88.285, train recall:93.819, train auc:97.166
fold:0 epoch:13        valid loss:0.231296, valid acc:90.030, valid f1:89.934, valid precision:90.808, valid recall:89.077, valid auc:96.947
None
====================================================================================================
fold:0 epoch:14 step:0 train loss:0.219095, train acc:90.442, train f1:90.800, train precision:88.113, train recall:93.655, train auc:96.965
fold:0 epoch:14 step:1 train loss:0.214583, train acc:90.674, train f1:90.982, train precision:87.985, train recall:94.189, train auc:97.106
fold:0 epoch:14 step:2 train loss:0.215234, train acc:90.668, train f1:90.959, train precision:88.540, train recall:93.513, train auc:97.066
fold:0 epoch:14 step:3 train loss:0.216365, train acc:90.402, train f1:90.563, train precision:89.006, train recall:92.175, train auc:97.034
fold:0 epoch:14 step:4 train loss:0.216604, train acc:90.540, train f1:90.687, train precision:89.261, train recall:92.160, train auc:97.022
fold:0 epoch:14 step:5 train loss:0.220031, train acc:90.585, train f1:90.741, train precision:89.186, train recall:92.351, train auc:96.933
fold:0 epoch:14 step:6 train loss:0.214968, train acc:90.762, train f1:91.004, train precision:89.106, train recall:92.986, train auc:97.075
fold:0 epoch:14 step:7 train loss:0.210709, train acc:90.762, train f1:91.016, train precision:88.431, train recall:93.757, train auc:97.172
fold:0 epoch:14 step:8 train loss:0.209779, train acc:90.897, train f1:91.122, train precision:88.308, train recall:94.122, train auc:97.243
fold:0 epoch:14 step:9 train loss:0.210098, train acc:91.020, train f1:91.173, train precision:88.681, train recall:93.809, train auc:97.234
fold:0 epoch:14        valid loss:0.205632, valid acc:91.363, valid f1:91.528, valid precision:89.815, valid recall:93.308, valid auc:97.403
None
====================================================================================================
fold:0 epoch:15 step:0 train loss:0.205383, train acc:91.214, train f1:91.391, train precision:89.509, train recall:93.353, train auc:97.342
fold:0 epoch:15 step:1 train loss:0.209264, train acc:90.933, train f1:91.116, train precision:89.397, train recall:92.902, train auc:97.224
fold:0 epoch:15 step:2 train loss:0.210224, train acc:91.006, train f1:91.232, train precision:89.201, train recall:93.356, train auc:97.203
fold:0 epoch:15 step:3 train loss:0.205977, train acc:90.988, train f1:91.170, train precision:89.387, train recall:93.025, train auc:97.317
fold:0 epoch:15 step:4 train loss:0.203450, train acc:91.107, train f1:91.333, train precision:88.911, train recall:93.891, train auc:97.394
fold:0 epoch:15 step:5 train loss:0.202454, train acc:91.382, train f1:91.644, train precision:89.427, train recall:93.974, train auc:97.387
fold:0 epoch:15 step:6 train loss:0.206402, train acc:91.022, train f1:91.305, train precision:89.248, train recall:93.460, train auc:97.305
fold:0 epoch:15 step:7 train loss:0.208619, train acc:90.955, train f1:91.109, train precision:88.152, train recall:94.270, train auc:97.271
fold:0 epoch:15 step:8 train loss:0.203069, train acc:91.260, train f1:91.470, train precision:89.351, train recall:93.691, train auc:97.400
fold:0 epoch:15 step:9 train loss:0.209295, train acc:91.117, train f1:91.274, train precision:89.983, train recall:92.602, train auc:97.244
fold:0 epoch:15        valid loss:0.207845, valid acc:91.264, valid f1:91.173, valid precision:92.130, valid recall:90.236, valid auc:97.542
None
====================================================================================================
fold:0 epoch:16 step:0 train loss:0.202176, train acc:91.467, train f1:91.669, train precision:89.995, train recall:93.405, train auc:97.401
fold:0 epoch:16 step:1 train loss:0.193861, train acc:91.675, train f1:91.883, train precision:90.013, train recall:93.832, train auc:97.619
fold:0 epoch:16 step:2 train loss:0.198173, train acc:91.376, train f1:91.570, train precision:89.145, train recall:94.131, train auc:97.523
fold:0 epoch:16 step:3 train loss:0.198610, train acc:91.489, train f1:91.654, train precision:89.357, train recall:94.072, train auc:97.535
fold:0 epoch:16 step:4 train loss:0.198957, train acc:91.403, train f1:91.571, train precision:89.727, train recall:93.493, train auc:97.504
fold:0 epoch:16 step:5 train loss:0.196359, train acc:91.559, train f1:91.734, train precision:90.203, train recall:93.318, train auc:97.595
fold:0 epoch:16 step:6 train loss:0.195132, train acc:91.623, train f1:91.744, train precision:90.624, train recall:92.892, train auc:97.630
fold:0 epoch:16 step:7 train loss:0.199156, train acc:91.574, train f1:91.763, train precision:90.067, train recall:93.523, train auc:97.514
fold:0 epoch:16 step:8 train loss:0.197533, train acc:91.617, train f1:91.830, train precision:89.104, train recall:94.729, train auc:97.552
fold:0 epoch:16 step:9 train loss:0.198576, train acc:91.750, train f1:91.987, train precision:89.021, train recall:95.157, train auc:97.503
fold:0 epoch:16        valid loss:0.198618, valid acc:91.698, valid f1:91.735, valid precision:91.321, valid recall:92.154, valid auc:97.626
None
====================================================================================================
fold:0 epoch:17 step:0 train loss:0.190528, train acc:91.895, train f1:92.049, train precision:90.065, train recall:94.123, train auc:97.721
fold:0 epoch:17 step:1 train loss:0.190200, train acc:92.010, train f1:92.184, train precision:90.860, train recall:93.547, train auc:97.733
fold:0 epoch:17 step:2 train loss:0.190293, train acc:91.913, train f1:92.136, train precision:90.281, train recall:94.067, train auc:97.713
fold:0 epoch:17 step:3 train loss:0.190781, train acc:91.858, train f1:92.048, train precision:90.009, train recall:94.182, train auc:97.697
fold:0 epoch:17 step:4 train loss:0.189518, train acc:91.986, train f1:92.085, train precision:90.214, train recall:94.035, train auc:97.744
fold:0 epoch:17 step:5 train loss:0.184380, train acc:92.389, train f1:92.508, train precision:91.020, train recall:94.045, train auc:97.856
fold:0 epoch:17 step:6 train loss:0.194283, train acc:91.830, train f1:92.011, train precision:90.784, train recall:93.272, train auc:97.633
fold:0 epoch:17 step:7 train loss:0.186770, train acc:92.120, train f1:92.248, train precision:90.248, train recall:94.338, train auc:97.794
fold:0 epoch:17 step:8 train loss:0.191179, train acc:91.940, train f1:92.100, train precision:89.787, train recall:94.535, train auc:97.685
fold:0 epoch:17 step:9 train loss:0.197195, train acc:91.759, train f1:91.912, train precision:90.329, train recall:93.551, train auc:97.571
fold:0 epoch:17        valid loss:0.183743, valid acc:92.449, valid f1:92.471, valid precision:92.202, valid recall:92.741, valid auc:97.967
None
====================================================================================================
fold:0 epoch:18 step:0 train loss:0.181632, train acc:92.422, train f1:92.617, train precision:90.610, train recall:94.715, train auc:97.916
fold:0 epoch:18 step:1 train loss:0.176711, train acc:92.593, train f1:92.779, train precision:90.709, train recall:94.945, train auc:98.021
fold:0 epoch:18 step:2 train loss:0.182930, train acc:92.221, train f1:92.374, train precision:90.445, train recall:94.387, train auc:97.897
fold:0 epoch:18 step:3 train loss:0.186971, train acc:92.050, train f1:92.167, train precision:90.761, train recall:93.616, train auc:97.790
fold:0 epoch:18 step:4 train loss:0.187911, train acc:91.956, train f1:92.091, train precision:90.229, train recall:94.032, train auc:97.793
fold:0 epoch:18 step:5 train loss:0.183239, train acc:92.178, train f1:92.361, train precision:90.245, train recall:94.580, train auc:97.848
fold:0 epoch:18 step:6 train loss:0.181827, train acc:92.444, train f1:92.609, train precision:90.412, train recall:94.915, train auc:97.903
fold:0 epoch:18 step:7 train loss:0.186847, train acc:92.197, train f1:92.370, train precision:91.052, train recall:93.727, train auc:97.824
fold:0 epoch:18 step:8 train loss:0.180061, train acc:92.392, train f1:92.496, train precision:91.030, train recall:94.010, train auc:97.947
fold:0 epoch:18 step:9 train loss:0.186223, train acc:92.296, train f1:92.427, train precision:90.289, train recall:94.670, train auc:97.788
fold:0 epoch:18        valid loss:0.177075, valid acc:92.782, valid f1:92.823, valid precision:92.297, valid recall:93.355, valid auc:98.096
None
====================================================================================================
fold:0 epoch:19 step:0 train loss:0.175694, train acc:92.636, train f1:92.774, train precision:90.818, train recall:94.815, train auc:98.044
fold:0 epoch:19 step:1 train loss:0.177851, train acc:92.648, train f1:92.753, train precision:90.698, train recall:94.903, train auc:98.001
fold:0 epoch:19 step:2 train loss:0.174478, train acc:92.523, train f1:92.636, train precision:90.850, train recall:94.494, train auc:98.080
fold:0 epoch:19 step:3 train loss:0.177502, train acc:92.770, train f1:92.934, train precision:91.464, train recall:94.453, train auc:98.005
fold:0 epoch:19 step:4 train loss:0.179503, train acc:92.645, train f1:92.834, train precision:90.751, train recall:95.015, train auc:97.956
fold:0 epoch:19 step:5 train loss:0.178650, train acc:92.538, train f1:92.708, train precision:91.017, train recall:94.463, train auc:97.976
fold:0 epoch:19 step:6 train loss:0.172349, train acc:92.773, train f1:92.898, train precision:91.192, train recall:94.670, train auc:98.112
fold:0 epoch:19 step:7 train loss:0.178595, train acc:92.566, train f1:92.766, train precision:91.265, train recall:94.318, train auc:97.962
fold:0 epoch:19 step:8 train loss:0.175968, train acc:92.599, train f1:92.748, train precision:90.636, train recall:94.960, train auc:98.035
fold:0 epoch:19 step:9 train loss:0.179619, train acc:92.436, train f1:92.561, train precision:90.158, train recall:95.094, train auc:97.966
fold:0 epoch:19        valid loss:0.170634, valid acc:93.035, valid f1:93.178, valid precision:91.304, valid recall:95.131, valid auc:98.165
None
====================================================================================================
fold:0 epoch:20 step:0 train loss:0.171135, train acc:92.908, train f1:93.087, train precision:91.428, train recall:94.807, train auc:98.128
fold:0 epoch:20 step:1 train loss:0.175159, train acc:92.810, train f1:92.924, train precision:91.430, train recall:94.468, train auc:98.083
fold:0 epoch:20 step:2 train loss:0.168444, train acc:92.993, train f1:93.100, train precision:91.310, train recall:94.960, train auc:98.212
fold:0 epoch:20 step:3 train loss:0.175834, train acc:92.731, train f1:92.907, train precision:91.095, train recall:94.793, train auc:98.044
fold:0 epoch:20 step:4 train loss:0.172595, train acc:92.862, train f1:93.018, train precision:91.416, train recall:94.677, train auc:98.120
fold:0 epoch:20 step:5 train loss:0.172327, train acc:92.874, train f1:92.981, train precision:90.944, train recall:95.111, train auc:98.105
fold:0 epoch:20 step:6 train loss:0.175559, train acc:92.755, train f1:92.908, train precision:91.352, train recall:94.517, train auc:98.048
fold:0 epoch:20 step:7 train loss:0.168043, train acc:93.085, train f1:93.270, train precision:91.175, train recall:95.465, train auc:98.206
fold:0 epoch:20 step:8 train loss:0.175053, train acc:92.700, train f1:92.849, train precision:90.305, train recall:95.539, train auc:98.097
fold:0 epoch:20 step:9 train loss:0.164947, train acc:93.008, train f1:93.076, train precision:91.209, train recall:95.020, train auc:98.259
fold:0 epoch:20        valid loss:0.162254, valid acc:93.509, valid f1:93.611, valid precision:92.161, valid recall:95.108, valid auc:98.333
None
====================================================================================================
fold:0 epoch:21 step:0 train loss:0.163631, train acc:93.448, train f1:93.473, train precision:92.381, train recall:94.592, train auc:98.301
fold:0 epoch:21 step:1 train loss:0.171326, train acc:92.865, train f1:92.964, train precision:91.891, train recall:94.062, train auc:98.141
fold:0 epoch:21 step:2 train loss:0.169457, train acc:93.048, train f1:93.182, train precision:91.403, train recall:95.031, train auc:98.166
fold:0 epoch:21 step:3 train loss:0.170376, train acc:92.935, train f1:93.057, train precision:90.709, train recall:95.529, train auc:98.182
fold:0 epoch:21 step:4 train loss:0.171850, train acc:92.996, train f1:93.152, train precision:91.915, train recall:94.423, train auc:98.118
fold:0 epoch:21 step:5 train loss:0.164620, train acc:93.222, train f1:93.313, train precision:91.693, train recall:94.992, train auc:98.275
fold:0 epoch:21 step:6 train loss:0.164678, train acc:93.219, train f1:93.361, train precision:91.588, train recall:95.204, train auc:98.286
fold:0 epoch:21 step:7 train loss:0.165861, train acc:93.225, train f1:93.393, train precision:91.562, train recall:95.299, train auc:98.237
fold:0 epoch:21 step:8 train loss:0.161704, train acc:93.365, train f1:93.516, train precision:91.315, train recall:95.825, train auc:98.351
fold:0 epoch:21 step:9 train loss:0.166430, train acc:93.105, train f1:93.306, train precision:91.925, train recall:94.730, train auc:98.245
fold:0 epoch:21        valid loss:0.158917, valid acc:93.661, valid f1:93.749, valid precision:92.465, valid recall:95.069, valid auc:98.404
None
====================================================================================================
fold:0 epoch:22 step:0 train loss:0.160601, train acc:93.460, train f1:93.622, train precision:92.063, train recall:95.235, train auc:98.356
fold:0 epoch:22 step:1 train loss:0.164483, train acc:93.219, train f1:93.373, train precision:91.384, train recall:95.451, train auc:98.292
fold:0 epoch:22 step:2 train loss:0.159134, train acc:93.466, train f1:93.612, train precision:91.823, train recall:95.472, train auc:98.375
fold:0 epoch:22 step:3 train loss:0.160916, train acc:93.402, train f1:93.537, train precision:91.792, train recall:95.350, train auc:98.340
fold:0 epoch:22 step:4 train loss:0.159498, train acc:93.356, train f1:93.485, train precision:91.720, train recall:95.319, train auc:98.388
fold:0 epoch:22 step:5 train loss:0.161427, train acc:93.231, train f1:93.301, train precision:91.662, train recall:94.999, train auc:98.350
fold:0 epoch:22 step:6 train loss:0.165465, train acc:93.246, train f1:93.334, train precision:91.685, train recall:95.043, train auc:98.273
fold:0 epoch:22 step:7 train loss:0.162686, train acc:93.234, train f1:93.359, train precision:91.783, train recall:94.989, train auc:98.299
fold:0 epoch:22 step:8 train loss:0.161026, train acc:93.393, train f1:93.496, train precision:91.968, train recall:95.075, train auc:98.360
fold:0 epoch:22 step:9 train loss:0.165759, train acc:93.245, train f1:93.396, train precision:91.155, train recall:95.751, train auc:98.224
fold:0 epoch:22        valid loss:0.156168, valid acc:93.814, valid f1:93.918, valid precision:92.356, valid recall:95.533, valid auc:98.446
None
====================================================================================================
fold:0 epoch:23 step:0 train loss:0.159937, train acc:93.454, train f1:93.586, train precision:91.376, train recall:95.906, train auc:98.381
fold:0 epoch:23 step:1 train loss:0.153261, train acc:93.716, train f1:93.881, train precision:92.526, train recall:95.277, train auc:98.499
fold:0 epoch:23 step:2 train loss:0.156692, train acc:93.677, train f1:93.806, train precision:92.763, train recall:94.872, train auc:98.438
fold:0 epoch:23 step:3 train loss:0.159046, train acc:93.494, train f1:93.590, train precision:91.785, train recall:95.467, train auc:98.399
fold:0 epoch:23 step:4 train loss:0.158601, train acc:93.521, train f1:93.622, train precision:91.513, train recall:95.830, train auc:98.401
fold:0 epoch:23 step:5 train loss:0.157841, train acc:93.500, train f1:93.656, train precision:91.743, train recall:95.650, train auc:98.410
fold:0 epoch:23 step:6 train loss:0.160719, train acc:93.365, train f1:93.451, train precision:92.196, train recall:94.741, train auc:98.358
fold:0 epoch:23 step:7 train loss:0.149877, train acc:93.909, train f1:94.007, train precision:92.671, train recall:95.381, train auc:98.565
fold:0 epoch:23 step:8 train loss:0.154994, train acc:93.631, train f1:93.728, train precision:91.864, train recall:95.668, train auc:98.468
fold:0 epoch:23 step:9 train loss:0.158812, train acc:93.483, train f1:93.575, train precision:91.272, train recall:95.997, train auc:98.373
fold:0 epoch:23        valid loss:0.154109, valid acc:93.928, valid f1:93.959, valid precision:93.484, valid recall:94.439, valid auc:98.533
None
====================================================================================================
fold:0 epoch:24 step:0 train loss:0.154739, train acc:93.683, train f1:93.768, train precision:92.246, train recall:95.341, train auc:98.477
fold:0 epoch:24 step:1 train loss:0.153371, train acc:93.784, train f1:93.877, train precision:92.600, train recall:95.190, train auc:98.489
fold:0 epoch:24 step:2 train loss:0.153293, train acc:93.674, train f1:93.829, train precision:92.402, train recall:95.302, train auc:98.496
fold:0 epoch:24 step:3 train loss:0.153643, train acc:93.668, train f1:93.841, train precision:91.651, train recall:96.138, train auc:98.496
fold:0 epoch:24 step:4 train loss:0.154389, train acc:93.848, train f1:93.973, train precision:92.063, train recall:95.964, train auc:98.479
fold:0 epoch:24 step:5 train loss:0.158867, train acc:93.423, train f1:93.519, train precision:91.837, train recall:95.264, train auc:98.386
fold:0 epoch:24 step:6 train loss:0.155284, train acc:93.707, train f1:93.806, train precision:92.599, train recall:95.045, train auc:98.467
fold:0 epoch:24 step:7 train loss:0.147993, train acc:94.019, train f1:94.137, train precision:92.466, train recall:95.869, train auc:98.605
fold:0 epoch:24 step:8 train loss:0.155467, train acc:93.558, train f1:93.659, train precision:91.528, train recall:95.891, train auc:98.463
fold:0 epoch:24 step:9 train loss:0.146695, train acc:94.151, train f1:94.214, train precision:92.279, train recall:96.232, train auc:98.612
fold:0 epoch:24        valid loss:0.147611, valid acc:94.165, valid f1:94.267, valid precision:92.641, valid recall:95.951, valid auc:98.603
None
====================================================================================================
fold:0 epoch:25 step:0 train loss:0.153733, train acc:93.680, train f1:93.776, train precision:92.561, train recall:95.024, train auc:98.512
fold:0 epoch:25 step:1 train loss:0.148560, train acc:93.866, train f1:93.972, train precision:92.556, train recall:95.432, train auc:98.594
fold:0 epoch:25 step:2 train loss:0.150872, train acc:93.875, train f1:94.007, train precision:92.589, train recall:95.470, train auc:98.549
fold:0 epoch:25 step:3 train loss:0.153961, train acc:93.719, train f1:93.821, train precision:92.532, train recall:95.146, train auc:98.483
fold:0 epoch:25 step:4 train loss:0.147463, train acc:94.028, train f1:94.130, train precision:92.420, train recall:95.905, train auc:98.587
fold:0 epoch:25 step:5 train loss:0.149860, train acc:94.144, train f1:94.235, train precision:92.575, train recall:95.956, train auc:98.563
fold:0 epoch:25 step:6 train loss:0.146601, train acc:93.967, train f1:94.057, train precision:92.290, train recall:95.893, train auc:98.621
fold:0 epoch:25 step:7 train loss:0.148687, train acc:93.887, train f1:93.988, train precision:92.132, train recall:95.920, train auc:98.587
fold:0 epoch:25 step:8 train loss:0.143958, train acc:94.147, train f1:94.247, train precision:92.673, train recall:95.875, train auc:98.673
fold:0 epoch:25 step:9 train loss:0.150996, train acc:93.624, train f1:93.710, train precision:92.120, train recall:95.357, train auc:98.530
fold:0 epoch:25        valid loss:0.143560, valid acc:94.355, valid f1:94.429, valid precision:93.215, valid recall:95.675, valid auc:98.679
None
====================================================================================================
fold:0 epoch:26 step:0 train loss:0.142480, train acc:94.308, train f1:94.389, train precision:92.982, train recall:95.839, train auc:98.695
fold:0 epoch:26 step:1 train loss:0.146430, train acc:94.095, train f1:94.176, train precision:92.722, train recall:95.676, train auc:98.615
fold:0 epoch:26 step:2 train loss:0.148416, train acc:93.982, train f1:94.042, train precision:92.488, train recall:95.649, train auc:98.595
fold:0 epoch:26 step:3 train loss:0.145771, train acc:94.122, train f1:94.208, train precision:93.005, train recall:95.442, train auc:98.625
fold:0 epoch:26 step:4 train loss:0.140627, train acc:94.223, train f1:94.312, train precision:92.480, train recall:96.217, train auc:98.719
fold:0 epoch:26 step:5 train loss:0.145596, train acc:94.153, train f1:94.255, train precision:92.165, train recall:96.441, train auc:98.651
fold:0 epoch:26 step:6 train loss:0.141160, train acc:94.281, train f1:94.409, train precision:92.928, train recall:95.937, train auc:98.713
fold:0 epoch:26 step:7 train loss:0.145477, train acc:94.174, train f1:94.281, train precision:92.532, train recall:96.098, train auc:98.640
fold:0 epoch:26 step:8 train loss:0.145014, train acc:94.171, train f1:94.290, train precision:93.198, train recall:95.408, train auc:98.639
fold:0 epoch:26 step:9 train loss:0.151387, train acc:93.852, train f1:93.976, train precision:92.974, train recall:94.999, train auc:98.553
fold:0 epoch:26        valid loss:0.145245, valid acc:94.302, valid f1:94.343, valid precision:93.662, valid recall:95.035, valid auc:98.686
None
====================================================================================================
fold:0 epoch:27 step:0 train loss:0.139269, train acc:94.525, train f1:94.609, train precision:92.763, train recall:96.529, train auc:98.767
fold:0 epoch:27 step:1 train loss:0.143526, train acc:94.055, train f1:94.172, train precision:92.305, train recall:96.116, train auc:98.684
fold:0 epoch:27 step:2 train loss:0.136231, train acc:94.534, train f1:94.601, train precision:93.000, train recall:96.258, train auc:98.823
fold:0 epoch:27 step:3 train loss:0.142316, train acc:94.235, train f1:94.340, train precision:92.731, train recall:96.006, train auc:98.705
fold:0 epoch:27 step:4 train loss:0.141896, train acc:94.110, train f1:94.204, train precision:92.761, train recall:95.693, train auc:98.697
fold:0 epoch:27 step:5 train loss:0.142056, train acc:94.226, train f1:94.320, train precision:93.195, train recall:95.472, train auc:98.708
fold:0 epoch:27 step:6 train loss:0.145211, train acc:94.089, train f1:94.191, train precision:92.880, train recall:95.541, train auc:98.633
fold:0 epoch:27 step:7 train loss:0.142480, train acc:94.189, train f1:94.269, train precision:92.378, train recall:96.239, train auc:98.709
fold:0 epoch:27 step:8 train loss:0.141581, train acc:94.376, train f1:94.487, train precision:92.993, train recall:96.030, train auc:98.707
fold:0 epoch:27 step:9 train loss:0.139991, train acc:94.169, train f1:94.335, train precision:92.898, train recall:95.817, train auc:98.782
fold:0 epoch:27        valid loss:0.143387, valid acc:94.482, valid f1:94.572, valid precision:93.057, valid recall:96.137, valid auc:98.677
None
====================================================================================================
fold:0 epoch:28 step:0 train loss:0.138382, train acc:94.418, train f1:94.527, train precision:92.808, train recall:96.311, train auc:98.776
fold:0 epoch:28 step:1 train loss:0.141912, train acc:94.299, train f1:94.432, train precision:93.386, train recall:95.503, train auc:98.728
fold:0 epoch:28 step:2 train loss:0.141470, train acc:94.305, train f1:94.347, train precision:92.923, train recall:95.816, train auc:98.720
fold:0 epoch:28 step:3 train loss:0.139635, train acc:94.461, train f1:94.558, train precision:93.087, train recall:96.076, train auc:98.748
fold:0 epoch:28 step:4 train loss:0.141443, train acc:94.336, train f1:94.463, train precision:92.699, train recall:96.296, train auc:98.707
fold:0 epoch:28 step:5 train loss:0.138779, train acc:94.449, train f1:94.514, train precision:92.799, train recall:96.294, train auc:98.767
fold:0 epoch:28 step:6 train loss:0.140121, train acc:94.550, train f1:94.612, train precision:93.417, train recall:95.838, train auc:98.732
fold:0 epoch:28 step:7 train loss:0.136216, train acc:94.559, train f1:94.595, train precision:93.425, train recall:95.794, train auc:98.797
fold:0 epoch:28 step:8 train loss:0.138938, train acc:94.458, train f1:94.520, train precision:93.082, train recall:96.003, train auc:98.746
fold:0 epoch:28 step:9 train loss:0.133905, train acc:94.503, train f1:94.697, train precision:93.452, train recall:95.975, train auc:98.863
fold:0 epoch:28        valid loss:0.134275, valid acc:94.840, valid f1:94.894, valid precision:93.908, valid recall:95.902, valid auc:98.836
None
====================================================================================================
fold:0 epoch:29 step:0 train loss:0.129378, train acc:94.751, train f1:94.854, train precision:93.012, train recall:96.771, train auc:98.914
fold:0 epoch:29 step:1 train loss:0.138424, train acc:94.452, train f1:94.546, train precision:93.358, train recall:95.764, train auc:98.754
fold:0 epoch:29 step:2 train loss:0.137449, train acc:94.482, train f1:94.557, train precision:93.044, train recall:96.119, train auc:98.775
fold:0 epoch:29 step:3 train loss:0.129438, train acc:94.797, train f1:94.906, train precision:93.265, train recall:96.606, train auc:98.930
fold:0 epoch:29 step:4 train loss:0.136018, train acc:94.525, train f1:94.602, train precision:92.891, train recall:96.376, train auc:98.806
fold:0 epoch:29 step:5 train loss:0.135081, train acc:94.638, train f1:94.721, train precision:93.610, train recall:95.859, train auc:98.832
fold:0 epoch:29 step:6 train loss:0.133301, train acc:94.543, train f1:94.610, train precision:93.572, train recall:95.672, train auc:98.871
fold:0 epoch:29 step:7 train loss:0.135276, train acc:94.473, train f1:94.541, train precision:93.008, train recall:96.126, train auc:98.823
fold:0 epoch:29 step:8 train loss:0.135326, train acc:94.522, train f1:94.594, train precision:93.166, train recall:96.067, train auc:98.829
fold:0 epoch:29 step:9 train loss:0.150419, train acc:93.931, train f1:94.032, train precision:92.844, train recall:95.251, train auc:98.572
fold:0 epoch:29        valid loss:0.134023, valid acc:94.881, valid f1:94.974, valid precision:93.262, valid recall:96.751, valid auc:98.845
None
====================================================================================================
fold:0 epoch:30 step:0 train loss:0.129530, train acc:94.879, train f1:94.905, train precision:93.480, train recall:96.374, train auc:98.918
fold:0 epoch:30 step:1 train loss:0.132404, train acc:94.675, train f1:94.779, train precision:93.722, train recall:95.860, train auc:98.884
fold:0 epoch:30 step:2 train loss:0.130735, train acc:94.717, train f1:94.775, train precision:92.915, train recall:96.710, train auc:98.907
fold:0 epoch:30 step:3 train loss:0.134531, train acc:94.681, train f1:94.812, train precision:93.255, train recall:96.422, train auc:98.822
fold:0 epoch:30 step:4 train loss:0.132268, train acc:94.760, train f1:94.836, train precision:93.130, train recall:96.605, train auc:98.867
fold:0 epoch:30 step:5 train loss:0.132894, train acc:94.708, train f1:94.828, train precision:93.358, train recall:96.345, train auc:98.851
fold:0 epoch:30 step:6 train loss:0.134142, train acc:94.531, train f1:94.588, train precision:93.526, train recall:95.674, train auc:98.845
fold:0 epoch:30 step:7 train loss:0.133624, train acc:94.684, train f1:94.755, train precision:93.923, train recall:95.601, train auc:98.858
fold:0 epoch:30 step:8 train loss:0.134353, train acc:94.666, train f1:94.722, train precision:93.285, train recall:96.203, train auc:98.830
fold:0 epoch:30 step:9 train loss:0.127045, train acc:94.855, train f1:94.945, train precision:93.579, train recall:96.352, train auc:98.958
fold:0 epoch:30        valid loss:0.132702, valid acc:94.911, valid f1:94.951, valid precision:94.198, valid recall:95.716, valid auc:98.885
None
====================================================================================================
fold:0 epoch:31 step:0 train loss:0.130150, train acc:94.827, train f1:94.955, train precision:93.362, train recall:96.602, train auc:98.921
fold:0 epoch:31 step:1 train loss:0.123769, train acc:95.038, train f1:95.088, train precision:93.534, train recall:96.695, train auc:99.016
fold:0 epoch:31 step:2 train loss:0.126836, train acc:94.910, train f1:94.966, train precision:93.593, train recall:96.380, train auc:98.955
fold:0 epoch:31 step:3 train loss:0.131947, train acc:94.696, train f1:94.711, train precision:93.859, train recall:95.577, train auc:98.881
fold:0 epoch:31 step:4 train loss:0.130304, train acc:94.675, train f1:94.735, train precision:93.463, train recall:96.042, train auc:98.915
fold:0 epoch:31 step:5 train loss:0.127318, train acc:94.846, train f1:94.913, train precision:93.497, train recall:96.373, train auc:98.949
fold:0 epoch:31 step:6 train loss:0.127660, train acc:94.965, train f1:95.050, train precision:93.733, train recall:96.403, train auc:98.945
fold:0 epoch:31 step:7 train loss:0.129001, train acc:94.827, train f1:94.924, train precision:93.753, train recall:96.124, train auc:98.925
fold:0 epoch:31 step:8 train loss:0.130239, train acc:94.751, train f1:94.845, train precision:93.528, train recall:96.200, train auc:98.920
fold:0 epoch:31 step:9 train loss:0.125621, train acc:95.022, train f1:95.125, train precision:93.198, train recall:97.133, train auc:98.990
fold:0 epoch:31        valid loss:0.126771, valid acc:95.223, valid f1:95.284, valid precision:94.083, valid recall:96.516, valid auc:98.959
None
====================================================================================================
fold:0 epoch:32 step:0 train loss:0.125223, train acc:95.029, train f1:95.132, train precision:93.916, train recall:96.379, train auc:98.987
fold:0 epoch:32 step:1 train loss:0.127635, train acc:94.885, train f1:94.921, train precision:93.582, train recall:96.298, train auc:98.959
fold:0 epoch:32 step:2 train loss:0.122168, train acc:95.166, train f1:95.211, train precision:94.287, train recall:96.153, train auc:99.046
fold:0 epoch:32 step:3 train loss:0.129631, train acc:94.830, train f1:94.889, train precision:94.370, train recall:95.413, train auc:98.931
fold:0 epoch:32 step:4 train loss:0.123050, train acc:95.114, train f1:95.180, train precision:93.527, train recall:96.892, train auc:99.025
fold:0 epoch:32 step:5 train loss:0.124886, train acc:94.968, train f1:95.045, train precision:93.128, train recall:97.042, train auc:99.001
fold:0 epoch:32 step:6 train loss:0.129195, train acc:94.894, train f1:94.977, train precision:93.675, train recall:96.316, train auc:98.919
fold:0 epoch:32 step:7 train loss:0.127554, train acc:94.797, train f1:94.884, train precision:94.002, train recall:95.784, train auc:98.971
fold:0 epoch:32 step:8 train loss:0.128040, train acc:95.010, train f1:95.099, train precision:93.356, train recall:96.909, train auc:98.935
fold:0 epoch:32 step:9 train loss:0.128397, train acc:94.925, train f1:94.930, train precision:93.412, train recall:96.499, train auc:98.937
fold:0 epoch:32        valid loss:0.124175, valid acc:95.368, valid f1:95.408, valid precision:94.582, valid recall:96.249, valid auc:99.014
None
====================================================================================================
fold:0 epoch:33 step:0 train loss:0.120114, train acc:95.139, train f1:95.197, train precision:94.099, train recall:96.321, train auc:99.068
fold:0 epoch:33 step:1 train loss:0.121633, train acc:95.096, train f1:95.120, train precision:94.095, train recall:96.169, train auc:99.040
fold:0 epoch:33 step:2 train loss:0.122522, train acc:95.203, train f1:95.273, train precision:94.399, train recall:96.164, train auc:99.049
fold:0 epoch:33 step:3 train loss:0.125473, train acc:95.035, train f1:95.089, train precision:93.874, train recall:96.337, train auc:98.988
fold:0 epoch:33 step:4 train loss:0.125184, train acc:95.020, train f1:95.106, train precision:93.480, train recall:96.789, train auc:98.993
fold:0 epoch:33 step:5 train loss:0.120586, train acc:95.251, train f1:95.319, train precision:94.034, train recall:96.639, train auc:99.053
fold:0 epoch:33 step:6 train loss:0.125595, train acc:95.041, train f1:95.105, train precision:93.936, train recall:96.303, train auc:98.993
fold:0 epoch:33 step:7 train loss:0.124216, train acc:94.971, train f1:95.043, train precision:93.646, train recall:96.482, train auc:98.999
fold:0 epoch:33 step:8 train loss:0.125839, train acc:94.928, train f1:94.989, train precision:93.986, train recall:96.014, train auc:98.979
fold:0 epoch:33 step:9 train loss:0.131569, train acc:94.776, train f1:94.840, train precision:93.781, train recall:95.923, train auc:98.891
fold:0 epoch:33        valid loss:0.121114, valid acc:95.426, valid f1:95.499, valid precision:93.999, valid recall:97.048, valid auc:99.049
None
====================================================================================================
fold:0 epoch:34 step:0 train loss:0.117301, train acc:95.389, train f1:95.479, train precision:94.341, train recall:96.644, train auc:99.118
fold:0 epoch:34 step:1 train loss:0.122525, train acc:95.126, train f1:95.207, train precision:93.703, train recall:96.761, train auc:99.038
fold:0 epoch:34 step:2 train loss:0.120952, train acc:95.200, train f1:95.295, train precision:93.683, train recall:96.962, train auc:99.051
fold:0 epoch:34 step:3 train loss:0.120601, train acc:95.230, train f1:95.302, train precision:94.038, train recall:96.600, train auc:99.061
fold:0 epoch:34 step:4 train loss:0.114207, train acc:95.615, train f1:95.642, train precision:94.851, train recall:96.447, train auc:99.156
fold:0 epoch:34 step:5 train loss:0.117091, train acc:95.352, train f1:95.429, train precision:94.305, train recall:96.580, train auc:99.113
fold:0 epoch:34 step:6 train loss:0.121205, train acc:95.172, train f1:95.247, train precision:93.406, train recall:97.162, train auc:99.053
fold:0 epoch:34 step:7 train loss:0.126630, train acc:94.946, train f1:94.994, train precision:93.396, train recall:96.648, train auc:98.981
fold:0 epoch:34 step:8 train loss:0.118586, train acc:95.248, train f1:95.291, train precision:94.669, train recall:95.920, train auc:99.113
fold:0 epoch:34 step:9 train loss:0.121681, train acc:95.119, train f1:95.107, train precision:93.841, train recall:96.408, train auc:99.047
fold:0 epoch:34        valid loss:0.119456, valid acc:95.511, valid f1:95.538, valid precision:94.974, valid recall:96.108, valid auc:99.097
None
====================================================================================================
fold:0 epoch:35 step:0 train loss:0.118725, train acc:95.294, train f1:95.289, train precision:94.400, train recall:96.194, train auc:99.081
fold:0 epoch:35 step:1 train loss:0.121087, train acc:95.209, train f1:95.271, train precision:94.372, train recall:96.186, train auc:99.046
fold:0 epoch:35 step:2 train loss:0.118080, train acc:95.181, train f1:95.229, train precision:94.365, train recall:96.109, train auc:99.102
fold:0 epoch:35 step:3 train loss:0.117972, train acc:95.428, train f1:95.508, train precision:94.247, train recall:96.802, train auc:99.089
fold:0 epoch:35 step:4 train loss:0.123684, train acc:95.026, train f1:95.104, train precision:92.965, train recall:97.344, train auc:99.058
fold:0 epoch:35 step:5 train loss:0.121760, train acc:95.203, train f1:95.296, train precision:93.719, train recall:96.926, train auc:99.039
fold:0 epoch:35 step:6 train loss:0.119010, train acc:95.288, train f1:95.313, train precision:94.158, train recall:96.496, train auc:99.093
fold:0 epoch:35 step:7 train loss:0.118228, train acc:95.300, train f1:95.345, train precision:95.052, train recall:95.640, train auc:99.118
fold:0 epoch:35 step:8 train loss:0.120160, train acc:95.340, train f1:95.439, train precision:94.388, train recall:96.514, train auc:99.053
fold:0 epoch:35 step:9 train loss:0.129618, train acc:94.881, train f1:94.957, train precision:92.912, train recall:97.094, train auc:98.944
fold:0 epoch:35        valid loss:0.117231, valid acc:95.631, valid f1:95.676, valid precision:94.708, valid recall:96.664, valid auc:99.104
None
====================================================================================================
fold:0 epoch:36 step:0 train loss:0.114635, train acc:95.483, train f1:95.586, train precision:94.387, train recall:96.816, train auc:99.152
fold:0 epoch:36 step:1 train loss:0.114912, train acc:95.465, train f1:95.501, train precision:94.732, train recall:96.282, train auc:99.136
fold:0 epoch:36 step:2 train loss:0.121492, train acc:95.151, train f1:95.172, train precision:94.112, train recall:96.257, train auc:99.044
fold:0 epoch:36 step:3 train loss:0.116172, train acc:95.404, train f1:95.461, train precision:93.933, train recall:97.041, train auc:99.131
fold:0 epoch:36 step:4 train loss:0.114357, train acc:95.453, train f1:95.506, train precision:93.814, train recall:97.260, train auc:99.163
fold:0 epoch:36 step:5 train loss:0.117134, train acc:95.386, train f1:95.433, train precision:94.991, train recall:95.879, train auc:99.126
fold:0 epoch:36 step:6 train loss:0.118743, train acc:95.297, train f1:95.321, train precision:95.156, train recall:95.486, train auc:99.115
fold:0 epoch:36 step:7 train loss:0.114224, train acc:95.483, train f1:95.534, train precision:94.333, train recall:96.767, train auc:99.164
fold:0 epoch:36 step:8 train loss:0.117689, train acc:95.312, train f1:95.402, train precision:93.741, train recall:97.123, train auc:99.108
fold:0 epoch:36 step:9 train loss:0.125551, train acc:95.040, train f1:95.092, train precision:93.658, train recall:96.571, train auc:98.990
fold:0 epoch:36        valid loss:0.113953, valid acc:95.740, valid f1:95.809, valid precision:94.274, valid recall:97.396, valid auc:99.158
None
====================================================================================================
fold:0 epoch:37 step:0 train loss:0.114351, train acc:95.599, train f1:95.627, train precision:94.720, train recall:96.553, train auc:99.155
fold:0 epoch:37 step:1 train loss:0.115626, train acc:95.480, train f1:95.495, train precision:94.560, train recall:96.448, train auc:99.139
fold:0 epoch:37 step:2 train loss:0.111610, train acc:95.621, train f1:95.686, train precision:94.998, train recall:96.384, train auc:99.197
fold:0 epoch:37 step:3 train loss:0.115432, train acc:95.416, train f1:95.492, train precision:94.164, train recall:96.858, train auc:99.138
fold:0 epoch:37 step:4 train loss:0.111881, train acc:95.609, train f1:95.677, train precision:94.230, train recall:97.169, train auc:99.172
fold:0 epoch:37 step:5 train loss:0.110784, train acc:95.673, train f1:95.678, train precision:94.497, train recall:96.889, train auc:99.203
fold:0 epoch:37 step:6 train loss:0.112530, train acc:95.740, train f1:95.777, train precision:95.126, train recall:96.436, train auc:99.173
fold:0 epoch:37 step:7 train loss:0.116653, train acc:95.312, train f1:95.356, train precision:94.494, train recall:96.235, train auc:99.132
fold:0 epoch:37 step:8 train loss:0.110828, train acc:95.547, train f1:95.631, train precision:94.451, train recall:96.840, train auc:99.200
fold:0 epoch:37 step:9 train loss:0.110872, train acc:95.796, train f1:95.884, train precision:94.501, train recall:97.309, train auc:99.199
fold:0 epoch:37        valid loss:0.112969, valid acc:95.774, valid f1:95.834, valid precision:94.482, valid recall:97.226, valid auc:99.172
None
====================================================================================================
fold:0 epoch:38 step:0 train loss:0.108832, train acc:95.706, train f1:95.751, train precision:94.724, train recall:96.800, train auc:99.231
fold:0 epoch:38 step:1 train loss:0.109583, train acc:95.673, train f1:95.702, train precision:94.675, train recall:96.752, train auc:99.213
fold:0 epoch:38 step:2 train loss:0.112526, train acc:95.538, train f1:95.562, train precision:94.586, train recall:96.559, train auc:99.185
fold:0 epoch:38 step:3 train loss:0.107392, train acc:95.752, train f1:95.814, train precision:94.652, train recall:97.004, train auc:99.248
fold:0 epoch:38 step:4 train loss:0.116133, train acc:95.432, train f1:95.509, train precision:94.396, train recall:96.649, train auc:99.133
fold:0 epoch:38 step:5 train loss:0.110070, train acc:95.685, train f1:95.756, train precision:94.603, train recall:96.937, train auc:99.212
fold:0 epoch:38 step:6 train loss:0.113094, train acc:95.609, train f1:95.669, train precision:94.681, train recall:96.679, train auc:99.160
fold:0 epoch:38 step:7 train loss:0.114106, train acc:95.474, train f1:95.499, train precision:94.074, train recall:96.967, train auc:99.159
fold:0 epoch:38 step:8 train loss:0.111298, train acc:95.679, train f1:95.733, train precision:94.678, train recall:96.813, train auc:99.190
fold:0 epoch:38 step:9 train loss:0.107549, train acc:95.664, train f1:95.744, train precision:94.593, train recall:96.924, train auc:99.264
fold:0 epoch:38        valid loss:0.110513, valid acc:95.927, valid f1:95.982, valid precision:94.688, valid recall:97.312, valid auc:99.202
None
====================================================================================================
fold:0 epoch:39 step:0 train loss:0.105527, train acc:95.908, train f1:95.972, train precision:94.583, train recall:97.403, train auc:99.280
fold:0 epoch:39 step:1 train loss:0.104825, train acc:95.972, train f1:96.000, train precision:95.009, train recall:97.011, train auc:99.278
fold:0 epoch:39 step:2 train loss:0.107056, train acc:95.831, train f1:95.859, train precision:95.007, train recall:96.727, train auc:99.248
fold:0 epoch:39 step:3 train loss:0.111377, train acc:95.609, train f1:95.675, train precision:94.873, train recall:96.490, train auc:99.199
fold:0 epoch:39 step:4 train loss:0.111449, train acc:95.654, train f1:95.699, train precision:94.332, train recall:97.107, train auc:99.199
fold:0 epoch:39 step:5 train loss:0.108001, train acc:95.825, train f1:95.863, train precision:94.882, train recall:96.865, train auc:99.232
fold:0 epoch:39 step:6 train loss:0.111578, train acc:95.694, train f1:95.768, train precision:95.246, train recall:96.297, train auc:99.167
fold:0 epoch:39 step:7 train loss:0.114547, train acc:95.496, train f1:95.535, train precision:94.190, train recall:96.919, train auc:99.153
fold:0 epoch:39 step:8 train loss:0.108762, train acc:95.755, train f1:95.811, train precision:94.292, train recall:97.380, train auc:99.238
fold:0 epoch:39 step:9 train loss:0.103922, train acc:95.963, train f1:96.006, train precision:94.923, train recall:97.113, train auc:99.282
fold:0 epoch:39        valid loss:0.112171, valid acc:95.835, valid f1:95.920, valid precision:93.995, valid recall:97.926, valid auc:99.207
None
====================================================================================================
fold:0 epoch:40 step:0 train loss:0.111182, train acc:95.584, train f1:95.629, train precision:94.535, train recall:96.748, train auc:99.197
fold:0 epoch:40 step:1 train loss:0.109640, train acc:95.651, train f1:95.697, train precision:95.177, train recall:96.223, train auc:99.236
fold:0 epoch:40 step:2 train loss:0.105148, train acc:95.847, train f1:95.891, train precision:94.925, train recall:96.876, train auc:99.284
fold:0 epoch:40 step:3 train loss:0.106501, train acc:95.816, train f1:95.853, train precision:94.529, train recall:97.214, train auc:99.271
fold:0 epoch:40 step:4 train loss:0.105459, train acc:95.850, train f1:95.924, train precision:94.771, train recall:97.106, train auc:99.279
fold:0 epoch:40 step:5 train loss:0.109559, train acc:95.688, train f1:95.728, train precision:94.740, train recall:96.737, train auc:99.210
fold:0 epoch:40 step:6 train loss:0.105968, train acc:95.908, train f1:95.925, train precision:95.222, train recall:96.639, train auc:99.275
fold:0 epoch:40 step:7 train loss:0.107611, train acc:95.700, train f1:95.726, train precision:94.525, train recall:96.958, train auc:99.249
fold:0 epoch:40 step:8 train loss:0.107346, train acc:95.728, train f1:95.810, train precision:94.766, train recall:96.877, train auc:99.244
fold:0 epoch:40 step:9 train loss:0.103877, train acc:95.901, train f1:95.941, train precision:94.380, train recall:97.556, train auc:99.320
fold:0 epoch:40        valid loss:0.108403, valid acc:96.092, valid f1:96.131, valid precision:95.184, valid recall:97.098, valid auc:99.245
None
====================================================================================================
fold:0 epoch:41 step:0 train loss:0.107869, train acc:95.764, train f1:95.830, train precision:94.356, train recall:97.351, train auc:99.264
fold:0 epoch:41 step:1 train loss:0.103556, train acc:95.947, train f1:95.989, train precision:95.264, train recall:96.725, train auc:99.305
fold:0 epoch:41 step:2 train loss:0.106166, train acc:95.905, train f1:95.890, train precision:95.673, train recall:96.108, train auc:99.295
fold:0 epoch:41 step:3 train loss:0.106453, train acc:95.825, train f1:95.879, train precision:95.402, train recall:96.361, train auc:99.274
fold:0 epoch:41 step:4 train loss:0.104059, train acc:95.856, train f1:95.891, train precision:94.484, train recall:97.340, train auc:99.317
fold:0 epoch:41 step:5 train loss:0.099143, train acc:96.155, train f1:96.180, train precision:94.738, train recall:97.666, train auc:99.359
fold:0 epoch:41 step:6 train loss:0.108930, train acc:95.734, train f1:95.755, train precision:95.085, train recall:96.434, train auc:99.228
fold:0 epoch:41 step:7 train loss:0.105641, train acc:95.850, train f1:95.904, train precision:95.187, train recall:96.631, train auc:99.279
fold:0 epoch:41 step:8 train loss:0.106482, train acc:95.859, train f1:95.936, train precision:94.775, train recall:97.126, train auc:99.275
fold:0 epoch:41 step:9 train loss:0.111644, train acc:95.664, train f1:95.728, train precision:94.523, train recall:96.963, train auc:99.179
fold:0 epoch:41        valid loss:0.106429, valid acc:96.092, valid f1:96.116, valid precision:95.536, valid recall:96.704, valid auc:99.273
None
====================================================================================================
fold:0 epoch:42 step:0 train loss:0.101342, train acc:96.048, train f1:96.090, train precision:95.407, train recall:96.782, train auc:99.332
fold:0 epoch:42 step:1 train loss:0.104650, train acc:95.993, train f1:96.024, train precision:95.535, train recall:96.518, train auc:99.298
fold:0 epoch:42 step:2 train loss:0.104848, train acc:95.853, train f1:95.916, train precision:94.192, train recall:97.704, train auc:99.296
fold:0 epoch:42 step:3 train loss:0.107429, train acc:95.822, train f1:95.891, train precision:94.125, train recall:97.724, train auc:99.271
fold:0 epoch:42 step:4 train loss:0.104848, train acc:95.856, train f1:95.899, train precision:94.987, train recall:96.829, train auc:99.286
fold:0 epoch:42 step:5 train loss:0.105863, train acc:95.898, train f1:95.935, train precision:95.819, train recall:96.051, train auc:99.288
fold:0 epoch:42 step:6 train loss:0.103711, train acc:96.048, train f1:96.056, train precision:95.149, train recall:96.981, train auc:99.291
fold:0 epoch:42 step:7 train loss:0.107050, train acc:95.663, train f1:95.722, train precision:94.609, train recall:96.862, train auc:99.258
fold:0 epoch:42 step:8 train loss:0.101900, train acc:95.944, train f1:96.004, train precision:94.540, train recall:97.514, train auc:99.325
fold:0 epoch:42 step:9 train loss:0.107685, train acc:95.893, train f1:95.893, train precision:95.032, train recall:96.770, train auc:99.241
fold:0 epoch:42        valid loss:0.107674, valid acc:96.068, valid f1:96.118, valid precision:94.910, valid recall:97.357, valid auc:99.258
None
====================================================================================================
fold:0 epoch:43 step:0 train loss:0.099067, train acc:96.167, train f1:96.196, train precision:95.295, train recall:97.114, train auc:99.354
fold:0 epoch:43 step:1 train loss:0.098926, train acc:96.143, train f1:96.177, train precision:95.330, train recall:97.040, train auc:99.367
fold:0 epoch:43 step:2 train loss:0.105118, train acc:95.880, train f1:95.913, train precision:95.050, train recall:96.792, train auc:99.284
fold:0 epoch:43 step:3 train loss:0.099605, train acc:96.191, train f1:96.229, train precision:95.468, train recall:97.003, train auc:99.345
fold:0 epoch:43 step:4 train loss:0.101215, train acc:96.069, train f1:96.134, train precision:95.242, train recall:97.043, train auc:99.341
fold:0 epoch:43 step:5 train loss:0.100662, train acc:96.127, train f1:96.164, train precision:94.933, train recall:97.428, train auc:99.332
fold:0 epoch:43 step:6 train loss:0.102750, train acc:95.941, train f1:95.994, train precision:94.722, train recall:97.301, train auc:99.324
fold:0 epoch:43 step:7 train loss:0.104402, train acc:96.048, train f1:96.091, train precision:94.840, train recall:97.376, train auc:99.281
fold:0 epoch:43 step:8 train loss:0.099956, train acc:96.100, train f1:96.128, train precision:95.143, train recall:97.135, train auc:99.351
fold:0 epoch:43 step:9 train loss:0.101539, train acc:95.981, train f1:96.048, train precision:95.200, train recall:96.911, train auc:99.338
fold:0 epoch:43        valid loss:0.103084, valid acc:96.222, valid f1:96.260, valid precision:95.284, valid recall:97.257, valid auc:99.299
None
====================================================================================================
fold:0 epoch:44 step:0 train loss:0.096464, train acc:96.188, train f1:96.217, train precision:95.497, train recall:96.948, train auc:99.392
fold:0 epoch:44 step:1 train loss:0.098890, train acc:96.259, train f1:96.284, train precision:95.262, train recall:97.328, train auc:99.357
fold:0 epoch:44 step:2 train loss:0.096229, train acc:96.268, train f1:96.307, train precision:95.622, train recall:97.001, train auc:99.399
fold:0 epoch:44 step:3 train loss:0.099586, train acc:96.063, train f1:96.121, train precision:95.001, train recall:97.267, train auc:99.357
fold:0 epoch:44 step:4 train loss:0.096800, train acc:96.121, train f1:96.151, train precision:94.669, train recall:97.681, train auc:99.414
fold:0 epoch:44 step:5 train loss:0.098334, train acc:96.255, train f1:96.291, train precision:95.406, train recall:97.193, train auc:99.370
fold:0 epoch:44 step:6 train loss:0.103195, train acc:95.865, train f1:95.911, train precision:95.419, train recall:96.408, train auc:99.329
fold:0 epoch:44 step:7 train loss:0.100215, train acc:96.201, train f1:96.244, train precision:95.424, train recall:97.079, train auc:99.341
fold:0 epoch:44 step:8 train loss:0.100782, train acc:96.075, train f1:96.130, train precision:94.767, train recall:97.533, train auc:99.343
fold:0 epoch:44 step:9 train loss:0.107066, train acc:95.849, train f1:95.855, train precision:94.904, train recall:96.824, train auc:99.256
fold:0 epoch:44        valid loss:0.100905, valid acc:96.324, valid f1:96.370, valid precision:95.162, valid recall:97.610, valid auc:99.340
None
====================================================================================================
fold:0 epoch:45 step:0 train loss:0.097797, train acc:96.259, train f1:96.292, train precision:96.019, train recall:96.566, train auc:99.379
fold:0 epoch:45 step:1 train loss:0.095324, train acc:96.365, train f1:96.359, train precision:95.411, train recall:97.326, train auc:99.415
fold:0 epoch:45 step:2 train loss:0.095568, train acc:96.295, train f1:96.326, train precision:95.420, train recall:97.250, train auc:99.414
fold:0 epoch:45 step:3 train loss:0.098167, train acc:96.164, train f1:96.209, train precision:95.043, train recall:97.405, train auc:99.368
fold:0 epoch:45 step:4 train loss:0.097053, train acc:96.164, train f1:96.180, train precision:95.608, train recall:96.759, train auc:99.390
fold:0 epoch:45 step:5 train loss:0.097961, train acc:96.329, train f1:96.344, train precision:95.690, train recall:97.008, train auc:99.367
fold:0 epoch:45 step:6 train loss:0.102603, train acc:95.984, train f1:96.042, train precision:95.132, train recall:96.969, train auc:99.308
fold:0 epoch:45 step:7 train loss:0.098337, train acc:96.133, train f1:96.203, train precision:95.078, train recall:97.356, train auc:99.363
fold:0 epoch:45 step:8 train loss:0.099415, train acc:96.033, train f1:96.078, train precision:94.831, train recall:97.359, train auc:99.365
fold:0 epoch:45 step:9 train loss:0.094415, train acc:96.306, train f1:96.343, train precision:95.942, train recall:96.748, train auc:99.418
fold:0 epoch:45        valid loss:0.102394, valid acc:96.304, valid f1:96.334, valid precision:95.547, valid recall:97.135, valid auc:99.323
None
====================================================================================================
fold:0 epoch:46 step:0 train loss:0.093154, train acc:96.420, train f1:96.428, train precision:95.737, train recall:97.129, train auc:99.430
fold:0 epoch:46 step:1 train loss:0.096429, train acc:96.237, train f1:96.289, train precision:95.362, train recall:97.234, train auc:99.389
fold:0 epoch:46 step:2 train loss:0.096272, train acc:96.246, train f1:96.282, train precision:95.080, train recall:97.514, train auc:99.400
fold:0 epoch:46 step:3 train loss:0.091676, train acc:96.469, train f1:96.508, train precision:95.897, train recall:97.127, train auc:99.457
fold:0 epoch:46 step:4 train loss:0.095889, train acc:96.332, train f1:96.363, train precision:95.588, train recall:97.151, train auc:99.390
fold:0 epoch:46 step:5 train loss:0.097173, train acc:96.225, train f1:96.284, train precision:95.461, train recall:97.121, train auc:99.382
fold:0 epoch:46 step:6 train loss:0.098690, train acc:96.152, train f1:96.202, train precision:94.919, train recall:97.521, train auc:99.366
fold:0 epoch:46 step:7 train loss:0.096908, train acc:96.158, train f1:96.205, train precision:95.346, train recall:97.080, train auc:99.395
fold:0 epoch:46 step:8 train loss:0.096172, train acc:96.329, train f1:96.331, train precision:95.507, train recall:97.170, train auc:99.395
fold:0 epoch:46 step:9 train loss:0.099050, train acc:96.025, train f1:96.030, train precision:95.095, train recall:96.984, train auc:99.371
fold:0 epoch:46        valid loss:0.097658, valid acc:96.475, valid f1:96.518, valid precision:95.356, valid recall:97.709, valid auc:99.375
None
====================================================================================================
fold:0 epoch:47 step:0 train loss:0.091263, train acc:96.472, train f1:96.502, train precision:95.468, train recall:97.559, train auc:99.457
fold:0 epoch:47 step:1 train loss:0.094031, train acc:96.329, train f1:96.363, train precision:95.545, train recall:97.194, train auc:99.421
fold:0 epoch:47 step:2 train loss:0.091867, train acc:96.475, train f1:96.534, train precision:95.709, train recall:97.372, train auc:99.444
fold:0 epoch:47 step:3 train loss:0.093544, train acc:96.399, train f1:96.444, train precision:95.444, train recall:97.467, train auc:99.425
fold:0 epoch:47 step:4 train loss:0.094187, train acc:96.350, train f1:96.364, train precision:95.522, train recall:97.221, train auc:99.422
fold:0 epoch:47 step:5 train loss:0.095345, train acc:96.317, train f1:96.341, train precision:95.705, train recall:96.984, train auc:99.406
fold:0 epoch:47 step:6 train loss:0.094202, train acc:96.286, train f1:96.329, train precision:95.445, train recall:97.229, train auc:99.425
fold:0 epoch:47 step:7 train loss:0.095865, train acc:96.332, train f1:96.375, train precision:95.221, train recall:97.558, train auc:99.398
fold:0 epoch:47 step:8 train loss:0.091676, train acc:96.466, train f1:96.501, train precision:95.634, train recall:97.384, train auc:99.455
fold:0 epoch:47 step:9 train loss:0.093411, train acc:96.394, train f1:96.356, train precision:95.557, train recall:97.167, train auc:99.437
fold:0 epoch:47        valid loss:0.099029, valid acc:96.476, valid f1:96.520, valid precision:95.340, valid recall:97.730, valid auc:99.375
None
====================================================================================================
fold:0 epoch:48 step:0 train loss:0.089815, train acc:96.600, train f1:96.600, train precision:96.547, train recall:96.653, train auc:99.480
fold:0 epoch:48 step:1 train loss:0.094081, train acc:96.454, train f1:96.477, train precision:95.591, train recall:97.381, train auc:99.421
fold:0 epoch:48 step:2 train loss:0.089956, train acc:96.411, train f1:96.454, train precision:95.038, train recall:97.912, train auc:99.482
fold:0 epoch:48 step:3 train loss:0.094074, train acc:96.417, train f1:96.465, train precision:95.289, train recall:97.671, train auc:99.418
fold:0 epoch:48 step:4 train loss:0.090874, train acc:96.475, train f1:96.494, train precision:96.030, train recall:96.962, train auc:99.465
fold:0 epoch:48 step:5 train loss:0.096640, train acc:96.234, train f1:96.255, train precision:96.162, train recall:96.349, train auc:99.410
fold:0 epoch:48 step:6 train loss:0.091592, train acc:96.463, train f1:96.508, train precision:95.487, train recall:97.551, train auc:99.457
fold:0 epoch:48 step:7 train loss:0.097731, train acc:96.136, train f1:96.191, train precision:94.838, train recall:97.583, train auc:99.386
fold:0 epoch:48 step:8 train loss:0.095199, train acc:96.368, train f1:96.400, train precision:95.601, train recall:97.211, train auc:99.407
fold:0 epoch:48 step:9 train loss:0.094128, train acc:96.385, train f1:96.378, train precision:96.031, train recall:96.727, train auc:99.410
fold:0 epoch:48        valid loss:0.096430, valid acc:96.471, valid f1:96.515, valid precision:95.337, valid recall:97.722, valid auc:99.398
None
====================================================================================================
fold:0 epoch:49 step:0 train loss:0.087585, train acc:96.658, train f1:96.678, train precision:95.901, train recall:97.467, train auc:99.499
fold:0 epoch:49 step:1 train loss:0.091479, train acc:96.497, train f1:96.541, train precision:95.528, train recall:97.576, train auc:99.456
fold:0 epoch:49 step:2 train loss:0.087632, train acc:96.603, train f1:96.609, train precision:95.431, train recall:97.816, train auc:99.499
fold:0 epoch:49 step:3 train loss:0.093603, train acc:96.429, train f1:96.465, train precision:95.949, train recall:96.987, train auc:99.424
fold:0 epoch:49 step:4 train loss:0.092436, train acc:96.469, train f1:96.473, train precision:95.799, train recall:97.157, train auc:99.437
fold:0 epoch:49 step:5 train loss:0.089685, train acc:96.521, train f1:96.551, train precision:95.965, train recall:97.145, train auc:99.470
fold:0 epoch:49 step:6 train loss:0.095324, train acc:96.295, train f1:96.332, train precision:95.535, train recall:97.142, train auc:99.411
fold:0 epoch:49 step:7 train loss:0.092726, train acc:96.442, train f1:96.491, train precision:95.588, train recall:97.412, train auc:99.445
fold:0 epoch:49 step:8 train loss:0.093122, train acc:96.317, train f1:96.360, train precision:95.527, train recall:97.207, train auc:99.437
fold:0 epoch:49 step:9 train loss:0.087784, train acc:96.517, train f1:96.558, train precision:95.512, train recall:97.627, train auc:99.502
fold:0 epoch:49        valid loss:0.096717, valid acc:96.525, valid f1:96.550, valid precision:95.848, valid recall:97.263, valid auc:99.386
None
====================================================================================================
fold:0 epoch:50 step:0 train loss:0.091648, train acc:96.344, train f1:96.368, train precision:95.322, train recall:97.438, train auc:99.454
fold:0 epoch:50 step:1 train loss:0.088829, train acc:96.548, train f1:96.574, train precision:95.869, train recall:97.291, train auc:99.485
fold:0 epoch:50 step:2 train loss:0.088008, train acc:96.619, train f1:96.625, train precision:96.051, train recall:97.205, train auc:99.491
fold:0 epoch:50 step:3 train loss:0.090528, train acc:96.423, train f1:96.457, train precision:95.964, train recall:96.955, train auc:99.466
fold:0 epoch:50 step:4 train loss:0.088181, train acc:96.616, train f1:96.648, train precision:95.650, train recall:97.666, train auc:99.495
fold:0 epoch:50 step:5 train loss:0.086593, train acc:96.680, train f1:96.738, train precision:95.779, train recall:97.716, train auc:99.508
fold:0 epoch:50 step:6 train loss:0.088610, train acc:96.646, train f1:96.660, train precision:96.091, train recall:97.236, train auc:99.484
fold:0 epoch:50 step:7 train loss:0.089716, train acc:96.548, train f1:96.567, train precision:95.918, train recall:97.225, train auc:99.476
fold:0 epoch:50 step:8 train loss:0.092078, train acc:96.451, train f1:96.492, train precision:95.727, train recall:97.269, train auc:99.442
fold:0 epoch:50 step:9 train loss:0.084864, train acc:96.887, train f1:96.895, train precision:95.802, train recall:98.012, train auc:99.525
fold:0 epoch:50        valid loss:0.093510, valid acc:96.668, valid f1:96.700, valid precision:95.785, valid recall:97.634, valid auc:99.424
None
====================================================================================================
fold:0 epoch:51 step:0 train loss:0.086993, train acc:96.735, train f1:96.780, train precision:96.143, train recall:97.425, train auc:99.505
fold:0 epoch:51 step:1 train loss:0.085625, train acc:96.683, train f1:96.702, train precision:95.822, train recall:97.600, train auc:99.522
fold:0 epoch:51 step:2 train loss:0.089013, train acc:96.628, train f1:96.643, train precision:95.905, train recall:97.391, train auc:99.475
fold:0 epoch:51 step:3 train loss:0.085475, train acc:96.625, train f1:96.654, train precision:95.951, train recall:97.366, train auc:99.528
fold:0 epoch:51 step:4 train loss:0.084736, train acc:96.722, train f1:96.753, train precision:96.097, train recall:97.419, train auc:99.532
fold:0 epoch:51 step:5 train loss:0.088889, train acc:96.579, train f1:96.614, train precision:95.481, train recall:97.775, train auc:99.492
fold:0 epoch:51 step:6 train loss:0.087393, train acc:96.613, train f1:96.624, train precision:95.745, train recall:97.520, train auc:99.495
fold:0 epoch:51 step:7 train loss:0.093883, train acc:96.405, train f1:96.408, train precision:96.208, train recall:96.608, train auc:99.439
fold:0 epoch:51 step:8 train loss:0.089430, train acc:96.515, train f1:96.559, train precision:95.740, train recall:97.393, train auc:99.475
fold:0 epoch:51 step:9 train loss:0.090535, train acc:96.473, train f1:96.526, train precision:95.068, train recall:98.029, train auc:99.476
fold:0 epoch:51        valid loss:0.092723, valid acc:96.663, valid f1:96.689, valid precision:95.949, valid recall:97.440, valid auc:99.439
[96.66318401462713, 96.68891336745934, 95.94907407407408, 97.44025075094684, 99.43942584408104]
====================================================================================================
fold:0 epoch:52 step:0 train loss:0.084912, train acc:96.786, train f1:96.840, train precision:95.803, train recall:97.901, train auc:99.524
fold:0 epoch:52 step:1 train loss:0.082704, train acc:96.820, train f1:96.851, train precision:96.443, train recall:97.262, train auc:99.557
fold:0 epoch:52 step:2 train loss:0.087973, train acc:96.597, train f1:96.614, train precision:96.219, train recall:97.012, train auc:99.491
fold:0 epoch:52 step:3 train loss:0.085201, train acc:96.707, train f1:96.709, train precision:95.771, train recall:97.665, train auc:99.524
fold:0 epoch:52 step:4 train loss:0.089898, train acc:96.548, train f1:96.591, train precision:95.517, train recall:97.689, train auc:99.466
fold:0 epoch:52 step:5 train loss:0.084870, train acc:96.643, train f1:96.668, train precision:95.838, train recall:97.513, train auc:99.531
fold:0 epoch:52 step:6 train loss:0.086170, train acc:96.716, train f1:96.742, train precision:96.223, train recall:97.266, train auc:99.516
fold:0 epoch:52 step:7 train loss:0.088843, train acc:96.545, train f1:96.552, train precision:96.090, train recall:97.019, train auc:99.477
fold:0 epoch:52 step:8 train loss:0.088631, train acc:96.609, train f1:96.652, train precision:95.760, train recall:97.560, train auc:99.486
fold:0 epoch:52 step:9 train loss:0.086195, train acc:96.447, train f1:96.439, train precision:95.081, train recall:97.836, train auc:99.536
fold:0 epoch:52        valid loss:0.092819, valid acc:96.705, valid f1:96.743, valid precision:95.650, valid recall:97.861, valid auc:99.443
[1;31mTest score increased (96.663184 --> 96.704976).[0m
[96.70497583910148, 96.74262474985476, 95.64973193770743, 97.86078098471987, 99.44299857568652]
====================================================================================================
fold:0 epoch:53 step:0 train loss:0.084356, train acc:96.780, train f1:96.798, train precision:96.315, train recall:97.285, train auc:99.533
fold:0 epoch:53 step:1 train loss:0.086951, train acc:96.579, train f1:96.589, train precision:96.592, train recall:96.586, train auc:99.525
fold:0 epoch:53 step:2 train loss:0.084082, train acc:96.808, train f1:96.809, train precision:96.192, train recall:97.433, train auc:99.547
fold:0 epoch:53 step:3 train loss:0.083857, train acc:96.799, train f1:96.855, train precision:95.790, train recall:97.945, train auc:99.540
fold:0 epoch:53 step:4 train loss:0.084523, train acc:96.777, train f1:96.804, train precision:95.761, train recall:97.871, train auc:99.531
fold:0 epoch:53 step:5 train loss:0.083802, train acc:96.710, train f1:96.711, train precision:95.944, train recall:97.490, train auc:99.552
fold:0 epoch:53 step:6 train loss:0.090600, train acc:96.436, train f1:96.449, train precision:95.831, train recall:97.075, train auc:99.468
fold:0 epoch:53 step:7 train loss:0.088986, train acc:96.576, train f1:96.617, train precision:96.182, train recall:97.056, train auc:99.481
fold:0 epoch:53 step:8 train loss:0.083863, train acc:96.869, train f1:96.900, train precision:96.023, train recall:97.792, train auc:99.531
fold:0 epoch:53 step:9 train loss:0.078370, train acc:96.957, train f1:96.991, train precision:96.188, train recall:97.807, train auc:99.596
fold:0 epoch:53        valid loss:0.094200, valid acc:96.629, valid f1:96.673, valid precision:95.439, valid recall:97.939, valid auc:99.445
[1;31mEarlyStopping counter: 1 out of 50[0m
[96.70497583910148, 96.74262474985476, 95.64973193770743, 97.86078098471987, 99.44299857568652]
====================================================================================================
fold:0 epoch:54 step:0 train loss:0.080394, train acc:96.872, train f1:96.889, train precision:96.098, train recall:97.692, train auc:99.577
fold:0 epoch:54 step:1 train loss:0.082383, train acc:96.954, train f1:96.982, train precision:96.842, train recall:97.123, train auc:99.552
fold:0 epoch:54 step:2 train loss:0.081469, train acc:96.912, train f1:96.945, train precision:96.086, train recall:97.819, train auc:99.562
fold:0 epoch:54 step:3 train loss:0.083184, train acc:96.780, train f1:96.802, train precision:95.554, train recall:98.084, train auc:99.553
fold:0 epoch:54 step:4 train loss:0.085262, train acc:96.725, train f1:96.753, train precision:95.995, train recall:97.523, train auc:99.533
fold:0 epoch:54 step:5 train loss:0.082440, train acc:96.759, train f1:96.782, train precision:96.583, train recall:96.982, train auc:99.564
fold:0 epoch:54 step:6 train loss:0.084445, train acc:96.799, train f1:96.798, train precision:96.144, train recall:97.462, train auc:99.529
fold:0 epoch:54 step:7 train loss:0.084677, train acc:96.735, train f1:96.779, train precision:95.867, train recall:97.708, train auc:99.527
fold:0 epoch:54 step:8 train loss:0.085224, train acc:96.701, train f1:96.736, train precision:95.430, train recall:98.077, train auc:99.527
fold:0 epoch:54 step:9 train loss:0.081740, train acc:96.746, train f1:96.773, train precision:95.970, train recall:97.590, train auc:99.554
fold:0 epoch:54        valid loss:0.090104, valid acc:96.800, valid f1:96.840, valid precision:95.665, valid recall:98.044, valid auc:99.475
[1;31mTest score increased (96.704976 --> 96.800313).[0m
[96.80031343868356, 96.83960682128944, 95.66480617784234, 98.04362021679509, 99.47486204462864]
====================================================================================================
fold:0 epoch:55 step:0 train loss:0.082521, train acc:96.765, train f1:96.761, train precision:96.879, train recall:96.643, train auc:99.581
fold:0 epoch:55 step:1 train loss:0.080502, train acc:96.768, train f1:96.769, train precision:96.495, train recall:97.044, train auc:99.589
fold:0 epoch:55 step:2 train loss:0.082282, train acc:96.664, train f1:96.709, train precision:95.624, train recall:97.819, train auc:99.566
fold:0 epoch:55 step:3 train loss:0.082193, train acc:96.829, train f1:96.872, train precision:95.347, train recall:98.446, train auc:99.584
fold:0 epoch:55 step:4 train loss:0.083337, train acc:96.872, train f1:96.888, train precision:96.033, train recall:97.757, train auc:99.537
fold:0 epoch:55 step:5 train loss:0.082155, train acc:96.811, train f1:96.811, train precision:96.879, train recall:96.743, train auc:99.568
fold:0 epoch:55 step:6 train loss:0.082670, train acc:96.881, train f1:96.889, train precision:96.285, train recall:97.500, train auc:99.545
fold:0 epoch:55 step:7 train loss:0.088759, train acc:96.603, train f1:96.662, train precision:95.911, train recall:97.424, train auc:99.479
fold:0 epoch:55 step:8 train loss:0.084896, train acc:96.747, train f1:96.778, train precision:95.989, train recall:97.580, train auc:99.530
fold:0 epoch:55 step:9 train loss:0.080944, train acc:96.825, train f1:96.840, train precision:96.125, train recall:97.566, train auc:99.573
fold:0 epoch:55        valid loss:0.090566, valid acc:96.773, valid f1:96.821, valid precision:95.392, valid recall:98.294, valid auc:99.491
[1;31mEarlyStopping counter: 1 out of 50[0m
[96.80031343868356, 96.83960682128944, 95.66480617784234, 98.04362021679509, 99.47486204462864]
====================================================================================================
fold:0 epoch:56 step:0 train loss:0.081052, train acc:96.918, train f1:96.917, train precision:96.294, train recall:97.549, train auc:99.573
fold:0 epoch:56 step:1 train loss:0.081106, train acc:96.896, train f1:96.926, train precision:96.225, train recall:97.637, train auc:99.570
fold:0 epoch:56 step:2 train loss:0.080273, train acc:96.857, train f1:96.902, train precision:95.659, train recall:98.178, train auc:99.580
fold:0 epoch:56 step:3 train loss:0.080746, train acc:96.844, train f1:96.901, train precision:96.255, train recall:97.556, train auc:99.571
fold:0 epoch:56 step:4 train loss:0.082960, train acc:96.777, train f1:96.769, train precision:95.878, train recall:97.678, train auc:99.555
fold:0 epoch:56 step:5 train loss:0.082667, train acc:96.805, train f1:96.829, train precision:96.644, train recall:97.014, train auc:99.559
fold:0 epoch:56 step:6 train loss:0.078545, train acc:97.101, train f1:97.108, train precision:96.555, train recall:97.667, train auc:99.591
fold:0 epoch:56 step:7 train loss:0.077538, train acc:97.101, train f1:97.138, train precision:96.430, train recall:97.858, train auc:99.598
fold:0 epoch:56 step:8 train loss:0.078157, train acc:96.960, train f1:96.978, train precision:96.016, train recall:97.959, train auc:99.607
fold:0 epoch:56 step:9 train loss:0.078765, train acc:96.746, train f1:96.773, train precision:95.886, train recall:97.676, train auc:99.605
fold:0 epoch:56        valid loss:0.087444, valid acc:96.964, valid f1:96.997, valid precision:95.931, valid recall:98.088, valid auc:99.498
[1;31mTest score increased (96.800313 --> 96.963563).[0m
[96.96356275303644, 96.99732665211607, 95.93061870944669, 98.08802403029907, 99.49760743067046]
====================================================================================================
fold:0 epoch:57 step:0 train loss:0.082085, train acc:96.829, train f1:96.821, train precision:96.154, train recall:97.498, train auc:99.559
fold:0 epoch:57 step:1 train loss:0.078706, train acc:97.092, train f1:97.106, train precision:96.973, train recall:97.238, train auc:99.601
fold:0 epoch:57 step:2 train loss:0.075710, train acc:97.128, train f1:97.150, train precision:96.580, train recall:97.727, train auc:99.619
fold:0 epoch:57 step:3 train loss:0.076945, train acc:97.025, train f1:97.097, train precision:96.218, train recall:97.993, train auc:99.614
fold:0 epoch:57 step:4 train loss:0.077444, train acc:96.970, train f1:96.975, train precision:95.863, train recall:98.114, train auc:99.608
fold:0 epoch:57 step:5 train loss:0.075907, train acc:97.165, train f1:97.183, train precision:96.799, train recall:97.571, train auc:99.617
fold:0 epoch:57 step:6 train loss:0.079288, train acc:96.979, train f1:96.978, train precision:96.754, train recall:97.204, train auc:99.597
fold:0 epoch:57 step:7 train loss:0.080499, train acc:96.912, train f1:96.935, train precision:96.097, train recall:97.788, train auc:99.574
fold:0 epoch:57 step:8 train loss:0.082334, train acc:96.838, train f1:96.874, train precision:95.770, train recall:98.004, train auc:99.564
fold:0 epoch:57 step:9 train loss:0.080427, train acc:96.843, train f1:96.861, train precision:96.263, train recall:97.466, train auc:99.587
fold:0 epoch:57        valid loss:0.087110, valid acc:96.957, valid f1:96.974, valid precision:96.431, valid recall:97.524, valid auc:99.507
[1;31mEarlyStopping counter: 1 out of 50[0m
[96.96356275303644, 96.99732665211607, 95.93061870944669, 98.08802403029907, 99.49760743067046]
====================================================================================================
fold:0 epoch:58 step:0 train loss:0.079549, train acc:96.930, train f1:96.925, train precision:96.618, train recall:97.234, train auc:99.589
fold:0 epoch:58 step:1 train loss:0.077439, train acc:97.055, train f1:97.061, train precision:96.453, train recall:97.677, train auc:99.605
fold:0 epoch:58 step:2 train loss:0.077029, train acc:96.991, train f1:97.001, train precision:96.357, train recall:97.655, train auc:99.615
fold:0 epoch:58 step:3 train loss:0.075068, train acc:97.128, train f1:97.148, train precision:96.414, train recall:97.892, train auc:99.630
fold:0 epoch:58 step:4 train loss:0.077880, train acc:96.954, train f1:96.984, train precision:96.384, train recall:97.591, train auc:99.609
fold:0 epoch:58 step:5 train loss:0.077916, train acc:97.122, train f1:97.123, train precision:96.789, train recall:97.459, train auc:99.592
fold:0 epoch:58 step:6 train loss:0.077204, train acc:97.083, train f1:97.102, train precision:96.412, train recall:97.802, train auc:99.610
fold:0 epoch:58 step:7 train loss:0.079283, train acc:96.906, train f1:96.960, train precision:96.308, train recall:97.621, train auc:99.591
fold:0 epoch:58 step:8 train loss:0.079564, train acc:96.854, train f1:96.900, train precision:95.859, train recall:97.963, train auc:99.590
fold:0 epoch:58 step:9 train loss:0.077378, train acc:97.150, train f1:97.165, train precision:96.339, train recall:98.005, train auc:99.606
fold:0 epoch:58        valid loss:0.085969, valid acc:96.978, valid f1:97.001, valid precision:96.253, valid recall:97.762, valid auc:99.508
[1;31mTest score increased (96.963563 --> 96.977929).[0m
[96.97792869269949, 97.0014254243877, 96.25305387681625, 97.76152540159332, 99.50830877394475]
====================================================================================================
fold:0 epoch:59 step:0 train loss:0.076228, train acc:97.119, train f1:97.122, train precision:96.610, train recall:97.640, train auc:99.611
fold:0 epoch:59 step:1 train loss:0.075824, train acc:97.134, train f1:97.150, train precision:96.807, train recall:97.496, train auc:99.623
fold:0 epoch:59 step:2 train loss:0.073762, train acc:97.131, train f1:97.157, train precision:96.394, train recall:97.933, train auc:99.645
fold:0 epoch:59 step:3 train loss:0.073772, train acc:97.290, train f1:97.332, train precision:96.417, train recall:98.265, train auc:99.630
fold:0 epoch:59 step:4 train loss:0.074022, train acc:97.165, train f1:97.197, train precision:96.698, train recall:97.701, train auc:99.642
fold:0 epoch:59 step:5 train loss:0.075428, train acc:97.202, train f1:97.187, train precision:96.585, train recall:97.796, train auc:99.625
fold:0 epoch:59 step:6 train loss:0.079528, train acc:96.921, train f1:96.935, train precision:96.516, train recall:97.358, train auc:99.591
fold:0 epoch:59 step:7 train loss:0.079018, train acc:97.000, train f1:97.018, train precision:96.395, train recall:97.649, train auc:99.593
fold:0 epoch:59 step:8 train loss:0.082897, train acc:96.909, train f1:96.932, train precision:96.328, train recall:97.544, train auc:99.548
fold:0 epoch:59 step:9 train loss:0.072299, train acc:97.370, train f1:97.382, train precision:96.662, train recall:98.112, train auc:99.636
fold:0 epoch:59        valid loss:0.084779, valid acc:97.047, valid f1:97.076, valid precision:96.135, valid recall:98.036, valid auc:99.533
[1;31mTest score increased (96.977929 --> 97.047146).[0m
[97.0471464019851, 97.07605364232415, 96.13493161211004, 98.03578424970615, 99.53256963895521]
====================================================================================================
fold:0 epoch:60 step:0 train loss:0.077165, train acc:96.988, train f1:97.010, train precision:96.359, train recall:97.670, train auc:99.609
fold:0 epoch:60 step:1 train loss:0.075498, train acc:97.101, train f1:97.107, train precision:96.548, train recall:97.672, train auc:99.628
fold:0 epoch:60 step:2 train loss:0.071953, train acc:97.290, train f1:97.324, train precision:96.452, train recall:98.212, train auc:99.657
fold:0 epoch:60 step:3 train loss:0.075955, train acc:97.107, train f1:97.124, train precision:96.029, train recall:98.245, train auc:99.632
fold:0 epoch:60 step:4 train loss:0.075339, train acc:97.107, train f1:97.115, train precision:96.749, train recall:97.482, train auc:99.633
fold:0 epoch:60 step:5 train loss:0.078796, train acc:96.988, train f1:96.975, train precision:97.037, train recall:96.912, train auc:99.614
fold:0 epoch:60 step:6 train loss:0.076634, train acc:97.098, train f1:97.123, train precision:96.288, train recall:97.974, train auc:99.612
fold:0 epoch:60 step:7 train loss:0.081161, train acc:96.881, train f1:96.909, train precision:95.722, train recall:98.126, train auc:99.591
fold:0 epoch:60 step:8 train loss:0.073961, train acc:97.107, train f1:97.146, train precision:96.692, train recall:97.605, train auc:99.636
fold:0 epoch:60 step:9 train loss:0.079319, train acc:96.790, train f1:96.829, train precision:96.719, train recall:96.938, train auc:99.588
fold:0 epoch:60        valid loss:0.083376, valid acc:97.111, valid f1:97.137, valid precision:96.275, valid recall:98.015, valid auc:99.543
[1;31mTest score increased (97.047146 --> 97.111140).[0m
[97.11114013321144, 97.13701431492842, 96.27472612053262, 98.01488833746899, 99.54260892812098]
====================================================================================================
fold:0 epoch:61 step:0 train loss:0.069889, train acc:97.379, train f1:97.398, train precision:96.611, train recall:98.198, train auc:99.674
fold:0 epoch:61 step:1 train loss:0.072752, train acc:97.198, train f1:97.225, train precision:96.179, train recall:98.295, train auc:99.659
fold:0 epoch:61 step:2 train loss:0.072591, train acc:97.214, train f1:97.258, train precision:96.611, train recall:97.914, train auc:99.654
fold:0 epoch:61 step:3 train loss:0.075728, train acc:97.165, train f1:97.168, train precision:96.841, train recall:97.498, train auc:99.626
fold:0 epoch:61 step:4 train loss:0.076236, train acc:97.070, train f1:97.074, train precision:96.649, train recall:97.502, train auc:99.622
fold:0 epoch:61 step:5 train loss:0.076282, train acc:97.052, train f1:97.078, train precision:96.512, train recall:97.651, train auc:99.627
fold:0 epoch:61 step:6 train loss:0.073307, train acc:97.256, train f1:97.271, train precision:96.530, train recall:98.024, train auc:99.649
fold:0 epoch:61 step:7 train loss:0.077881, train acc:97.076, train f1:97.090, train precision:96.532, train recall:97.653, train auc:99.595
fold:0 epoch:61 step:8 train loss:0.073962, train acc:97.156, train f1:97.171, train precision:96.679, train recall:97.669, train auc:99.641
fold:0 epoch:61 step:9 train loss:0.072895, train acc:97.300, train f1:97.309, train precision:96.572, train recall:98.057, train auc:99.636
fold:0 epoch:61        valid loss:0.083104, valid acc:97.162, valid f1:97.185, valid precision:96.421, valid recall:97.960, valid auc:99.551
[1;31mTest score increased (97.111140 --> 97.162074).[0m
[97.16207391928954, 97.18454023658673, 96.42122583299054, 97.96003656784642, 99.55067566582227]
====================================================================================================
fold:0 epoch:62 step:0 train loss:0.073136, train acc:97.144, train f1:97.166, train precision:96.407, train recall:97.937, train auc:99.645
fold:0 epoch:62 step:1 train loss:0.068422, train acc:97.372, train f1:97.418, train precision:96.748, train recall:98.098, train auc:99.696
fold:0 epoch:62 step:2 train loss:0.073987, train acc:97.223, train f1:97.255, train precision:96.596, train recall:97.922, train auc:99.639
fold:0 epoch:62 step:3 train loss:0.075502, train acc:97.113, train f1:97.138, train precision:96.845, train recall:97.433, train auc:99.624
fold:0 epoch:62 step:4 train loss:0.075807, train acc:97.086, train f1:97.106, train precision:96.298, train recall:97.928, train auc:99.628
fold:0 epoch:62 step:5 train loss:0.072859, train acc:97.113, train f1:97.108, train precision:96.429, train recall:97.795, train auc:99.663
fold:0 epoch:62 step:6 train loss:0.071406, train acc:97.375, train f1:97.372, train precision:97.306, train recall:97.437, train auc:99.665
fold:0 epoch:62 step:7 train loss:0.071286, train acc:97.281, train f1:97.294, train precision:96.768, train recall:97.826, train auc:99.660
fold:0 epoch:62 step:8 train loss:0.076103, train acc:97.018, train f1:97.038, train precision:95.832, train recall:98.274, train auc:99.629
fold:0 epoch:62 step:9 train loss:0.075255, train acc:97.150, train f1:97.155, train precision:96.360, train recall:97.964, train auc:99.646
fold:0 epoch:62        valid loss:0.084583, valid acc:97.082, valid f1:97.113, valid precision:96.105, valid recall:98.143, valid auc:99.538
[1;31mEarlyStopping counter: 1 out of 50[0m
[97.16207391928954, 97.18454023658673, 96.42122583299054, 97.96003656784642, 99.55067566582227]
====================================================================================================
fold:0 epoch:63 step:0 train loss:0.072316, train acc:97.150, train f1:97.134, train precision:97.158, train recall:97.110, train auc:99.680
fold:0 epoch:63 step:1 train loss:0.071449, train acc:97.321, train f1:97.327, train precision:97.091, train recall:97.565, train auc:99.668
fold:0 epoch:63 step:2 train loss:0.071097, train acc:97.327, train f1:97.348, train precision:96.663, train recall:98.042, train auc:99.660
fold:0 epoch:63 step:3 train loss:0.071042, train acc:97.369, train f1:97.411, train precision:96.691, train recall:98.142, train auc:99.656
fold:0 epoch:63 step:4 train loss:0.071939, train acc:97.302, train f1:97.312, train precision:96.747, train recall:97.884, train auc:99.657
fold:0 epoch:63 step:5 train loss:0.073435, train acc:97.241, train f1:97.247, train precision:96.940, train recall:97.556, train auc:99.651
fold:0 epoch:63 step:6 train loss:0.073018, train acc:97.253, train f1:97.267, train precision:96.528, train recall:98.017, train auc:99.654
fold:0 epoch:63 step:7 train loss:0.073802, train acc:97.125, train f1:97.158, train precision:96.315, train recall:98.015, train auc:99.645
fold:0 epoch:63 step:8 train loss:0.072101, train acc:97.232, train f1:97.253, train precision:96.642, train recall:97.873, train auc:99.660
fold:0 epoch:63 step:9 train loss:0.072773, train acc:97.238, train f1:97.216, train precision:97.354, train recall:97.079, train auc:99.670
fold:0 epoch:63        valid loss:0.082547, valid acc:97.229, valid f1:97.253, valid precision:96.424, valid recall:98.096, valid auc:99.555
[1;31mTest score increased (97.162074 --> 97.228680).[0m
[97.22867963954552, 97.2525053732812, 96.42352820354823, 98.09585999738802, 99.55454667654587]
====================================================================================================
fold:0 epoch:64 step:0 train loss:0.067263, train acc:97.485, train f1:97.490, train precision:96.987, train recall:97.997, train auc:99.695
fold:0 epoch:64 step:1 train loss:0.070198, train acc:97.321, train f1:97.339, train precision:96.572, train recall:98.118, train auc:99.675
fold:0 epoch:64 step:2 train loss:0.066819, train acc:97.397, train f1:97.440, train precision:96.821, train recall:98.067, train auc:99.708
fold:0 epoch:64 step:3 train loss:0.070023, train acc:97.342, train f1:97.356, train precision:96.695, train recall:98.025, train auc:99.678
fold:0 epoch:64 step:4 train loss:0.071577, train acc:97.247, train f1:97.252, train precision:96.838, train recall:97.668, train auc:99.669
fold:0 epoch:64 step:5 train loss:0.069098, train acc:97.333, train f1:97.344, train precision:96.950, train recall:97.742, train auc:99.685
fold:0 epoch:64 step:6 train loss:0.073126, train acc:97.198, train f1:97.200, train precision:96.458, train recall:97.953, train auc:99.650
fold:0 epoch:64 step:7 train loss:0.071651, train acc:97.256, train f1:97.285, train precision:96.432, train recall:98.154, train auc:99.662
fold:0 epoch:64 step:8 train loss:0.071922, train acc:97.162, train f1:97.195, train precision:96.880, train recall:97.513, train auc:99.665
fold:0 epoch:64 step:9 train loss:0.068931, train acc:97.335, train f1:97.313, train precision:96.840, train recall:97.790, train auc:99.692
fold:0 epoch:64        valid loss:0.082437, valid acc:97.229, valid f1:97.256, valid precision:96.312, valid recall:98.219, valid auc:99.570
[1;31mEarlyStopping counter: 1 out of 50[0m
[97.22867963954552, 97.2525053732812, 96.42352820354823, 98.09585999738802, 99.55454667654587]
====================================================================================================
fold:0 epoch:65 step:0 train loss:0.070039, train acc:97.302, train f1:97.322, train precision:96.981, train recall:97.665, train auc:99.678
fold:0 epoch:65 step:1 train loss:0.070513, train acc:97.324, train f1:97.335, train precision:96.325, train recall:98.366, train auc:99.682
fold:0 epoch:65 step:2 train loss:0.066773, train acc:97.464, train f1:97.477, train precision:96.910, train recall:98.052, train auc:99.704
fold:0 epoch:65 step:3 train loss:0.067952, train acc:97.336, train f1:97.362, train precision:96.984, train recall:97.743, train auc:99.696
fold:0 epoch:65 step:4 train loss:0.066215, train acc:97.394, train f1:97.406, train precision:96.836, train recall:97.984, train auc:99.712
fold:0 epoch:65 step:5 train loss:0.067863, train acc:97.473, train f1:97.479, train precision:96.743, train recall:98.227, train auc:99.696
fold:0 epoch:65 step:6 train loss:0.069028, train acc:97.421, train f1:97.421, train precision:96.833, train recall:98.016, train auc:99.684
fold:0 epoch:65 step:7 train loss:0.072443, train acc:97.186, train f1:97.188, train precision:96.881, train recall:97.497, train auc:99.659
fold:0 epoch:65 step:8 train loss:0.071763, train acc:97.263, train f1:97.277, train precision:96.945, train recall:97.612, train auc:99.659
fold:0 epoch:65 step:9 train loss:0.070472, train acc:97.546, train f1:97.625, train precision:97.285, train recall:97.967, train auc:99.670
fold:0 epoch:65        valid loss:0.080458, valid acc:97.297, valid f1:97.317, valid precision:96.598, valid recall:98.046, valid auc:99.572
[1;31mTest score increased (97.228680 --> 97.296591).[0m
[97.2965913543163, 97.31670641916416, 96.59795671530405, 98.04623220582474, 99.57217444709585]
====================================================================================================
fold:0 epoch:66 step:0 train loss:0.068707, train acc:97.369, train f1:97.424, train precision:96.176, train recall:98.704, train auc:99.709
fold:0 epoch:66 step:1 train loss:0.063620, train acc:97.620, train f1:97.646, train precision:96.892, train recall:98.413, train auc:99.736
fold:0 epoch:66 step:2 train loss:0.069386, train acc:97.360, train f1:97.342, train precision:97.076, train recall:97.609, train auc:99.691
fold:0 epoch:66 step:3 train loss:0.064274, train acc:97.540, train f1:97.529, train precision:97.302, train recall:97.757, train auc:99.736
fold:0 epoch:66 step:4 train loss:0.069837, train acc:97.363, train f1:97.393, train precision:96.669, train recall:98.127, train auc:99.682
fold:0 epoch:66 step:5 train loss:0.070429, train acc:97.311, train f1:97.333, train precision:96.611, train recall:98.067, train auc:99.677
fold:0 epoch:66 step:6 train loss:0.068340, train acc:97.421, train f1:97.424, train precision:97.101, train recall:97.749, train auc:99.689
fold:0 epoch:66 step:7 train loss:0.071370, train acc:97.287, train f1:97.291, train precision:97.294, train recall:97.288, train auc:99.672
fold:0 epoch:66 step:8 train loss:0.071408, train acc:97.269, train f1:97.296, train precision:96.506, train recall:98.099, train auc:99.668
fold:0 epoch:66 step:9 train loss:0.069311, train acc:97.335, train f1:97.359, train precision:96.425, train recall:98.310, train auc:99.704
fold:0 epoch:66        valid loss:0.080767, valid acc:97.317, valid f1:97.332, valid precision:96.823, valid recall:97.845, valid auc:99.573
[1;31mTest score increased (97.296591 --> 97.317487).[0m
[97.31748726655348, 97.33156650297504, 96.82338649228463, 97.84510905054199, 99.57335763685026]
====================================================================================================
fold:0 epoch:67 step:0 train loss:0.067768, train acc:97.363, train f1:97.376, train precision:96.981, train recall:97.774, train auc:99.703
fold:0 epoch:67 step:1 train loss:0.068646, train acc:97.308, train f1:97.315, train precision:97.392, train recall:97.238, train auc:99.710
fold:0 epoch:67 step:2 train loss:0.064716, train acc:97.577, train f1:97.599, train precision:97.193, train recall:98.008, train auc:99.720
fold:0 epoch:67 step:3 train loss:0.064770, train acc:97.510, train f1:97.515, train precision:96.458, train recall:98.596, train auc:99.740
fold:0 epoch:67 step:4 train loss:0.068062, train acc:97.366, train f1:97.383, train precision:96.728, train recall:98.046, train auc:99.696
fold:0 epoch:67 step:5 train loss:0.073063, train acc:97.214, train f1:97.225, train precision:97.004, train recall:97.447, train auc:99.657
fold:0 epoch:67 step:6 train loss:0.067865, train acc:97.473, train f1:97.495, train precision:97.177, train recall:97.814, train auc:99.696
fold:0 epoch:67 step:7 train loss:0.068821, train acc:97.379, train f1:97.381, train precision:96.677, train recall:98.096, train auc:99.688
fold:0 epoch:67 step:8 train loss:0.071840, train acc:97.241, train f1:97.255, train precision:96.849, train recall:97.664, train auc:99.661
fold:0 epoch:67 step:9 train loss:0.066522, train acc:97.590, train f1:97.588, train precision:97.297, train recall:97.881, train auc:99.710
fold:0 epoch:67        valid loss:0.079093, valid acc:97.289, valid f1:97.316, valid precision:96.340, valid recall:98.313, valid auc:99.594
[1;31mEarlyStopping counter: 1 out of 50[0m
[97.31748726655348, 97.33156650297504, 96.82338649228463, 97.84510905054199, 99.57335763685026]
====================================================================================================
fold:0 epoch:68 step:0 train loss:0.064577, train acc:97.543, train f1:97.550, train precision:97.346, train recall:97.755, train auc:99.723
fold:0 epoch:68 step:1 train loss:0.069355, train acc:97.397, train f1:97.396, train precision:96.422, train recall:98.390, train auc:99.686
fold:0 epoch:68 step:2 train loss:0.067276, train acc:97.464, train f1:97.489, train precision:96.715, train recall:98.276, train auc:99.705
fold:0 epoch:68 step:3 train loss:0.065917, train acc:97.464, train f1:97.484, train precision:96.725, train recall:98.254, train auc:99.716
fold:0 epoch:68 step:4 train loss:0.067877, train acc:97.433, train f1:97.448, train precision:97.498, train recall:97.397, train auc:99.709
fold:0 epoch:68 step:5 train loss:0.066572, train acc:97.473, train f1:97.463, train precision:97.100, train recall:97.829, train auc:99.705
fold:0 epoch:68 step:6 train loss:0.066660, train acc:97.437, train f1:97.437, train precision:96.735, train recall:98.150, train auc:99.703
fold:0 epoch:68 step:7 train loss:0.066272, train acc:97.556, train f1:97.571, train precision:97.314, train recall:97.829, train auc:99.705
fold:0 epoch:68 step:8 train loss:0.065369, train acc:97.543, train f1:97.569, train precision:97.009, train recall:98.135, train auc:99.717
fold:0 epoch:68 step:9 train loss:0.063877, train acc:97.704, train f1:97.773, train precision:97.119, train recall:98.437, train auc:99.712
fold:0 epoch:68        valid loss:0.079721, valid acc:97.214, valid f1:97.242, valid precision:96.299, valid recall:98.203, valid auc:99.594
[1;31mEarlyStopping counter: 2 out of 50[0m
[97.31748726655348, 97.33156650297504, 96.82338649228463, 97.84510905054199, 99.57335763685026]
====================================================================================================
fold:0 epoch:69 step:0 train loss:0.062732, train acc:97.614, train f1:97.636, train precision:96.469, train recall:98.831, train auc:99.760
fold:0 epoch:69 step:1 train loss:0.064663, train acc:97.577, train f1:97.608, train precision:97.279, train recall:97.938, train auc:99.727
fold:0 epoch:69 step:2 train loss:0.064709, train acc:97.510, train f1:97.499, train precision:97.279, train recall:97.721, train auc:99.729
fold:0 epoch:69 step:3 train loss:0.066854, train acc:97.433, train f1:97.425, train precision:96.994, train recall:97.860, train auc:99.711
fold:0 epoch:69 step:4 train loss:0.068093, train acc:97.498, train f1:97.515, train precision:96.898, train recall:98.140, train auc:99.695
fold:0 epoch:69 step:5 train loss:0.065451, train acc:97.610, train f1:97.614, train precision:96.967, train recall:98.270, train auc:99.714
fold:0 epoch:69 step:6 train loss:0.067351, train acc:97.403, train f1:97.425, train precision:97.134, train recall:97.718, train auc:99.701
fold:0 epoch:69 step:7 train loss:0.066320, train acc:97.488, train f1:97.503, train precision:97.376, train recall:97.630, train auc:99.722
fold:0 epoch:69 step:8 train loss:0.066229, train acc:97.534, train f1:97.563, train precision:96.932, train recall:98.203, train auc:99.711
fold:0 epoch:69 step:9 train loss:0.066278, train acc:97.423, train f1:97.421, train precision:96.579, train recall:98.277, train auc:99.716
fold:0 epoch:69        valid loss:0.079698, valid acc:97.375, valid f1:97.400, valid precision:96.477, valid recall:98.341, valid auc:99.603
[1;31mTest score increased (97.317487 --> 97.374951).[0m
[97.37495102520569, 97.4000776096236, 96.47661755285074, 98.34138696617474, 99.60315911152735]
====================================================================================================
fold:0 epoch:70 step:0 train loss:0.063749, train acc:97.647, train f1:97.639, train precision:96.817, train recall:98.474, train auc:99.729
fold:0 epoch:70 step:1 train loss:0.066833, train acc:97.507, train f1:97.506, train precision:97.378, train recall:97.634, train auc:99.715
fold:0 epoch:70 step:2 train loss:0.063194, train acc:97.626, train f1:97.626, train precision:97.093, train recall:98.166, train auc:99.736
fold:0 epoch:70 step:3 train loss:0.066248, train acc:97.482, train f1:97.526, train precision:97.092, train recall:97.963, train auc:99.724
fold:0 epoch:70 step:4 train loss:0.063000, train acc:97.592, train f1:97.613, train precision:96.738, train recall:98.504, train auc:99.750
fold:0 epoch:70 step:5 train loss:0.064745, train acc:97.592, train f1:97.598, train precision:97.145, train recall:98.055, train auc:99.711
fold:0 epoch:70 step:6 train loss:0.065322, train acc:97.455, train f1:97.467, train precision:97.497, train recall:97.438, train auc:99.723
fold:0 epoch:70 step:7 train loss:0.064822, train acc:97.644, train f1:97.654, train precision:97.364, train recall:97.946, train auc:99.721
fold:0 epoch:70 step:8 train loss:0.068991, train acc:97.379, train f1:97.408, train precision:96.473, train recall:98.361, train auc:99.700
fold:0 epoch:70 step:9 train loss:0.062667, train acc:97.748, train f1:97.765, train precision:97.222, train recall:98.315, train auc:99.730
fold:0 epoch:70        valid loss:0.078218, valid acc:97.367, valid f1:97.389, valid precision:96.588, valid recall:98.203, valid auc:99.616
[1;31mEarlyStopping counter: 1 out of 50[0m
[97.37495102520569, 97.4000776096236, 96.47661755285074, 98.34138696617474, 99.60315911152735]
====================================================================================================
fold:0 epoch:71 step:0 train loss:0.061779, train acc:97.568, train f1:97.583, train precision:97.403, train recall:97.764, train auc:99.750
fold:0 epoch:71 step:1 train loss:0.062471, train acc:97.610, train f1:97.619, train precision:97.244, train recall:97.998, train auc:99.742
fold:0 epoch:71 step:2 train loss:0.064504, train acc:97.543, train f1:97.567, train precision:96.954, train recall:98.187, train auc:99.731
fold:0 epoch:71 step:3 train loss:0.063897, train acc:97.623, train f1:97.636, train precision:97.008, train recall:98.271, train auc:99.730
fold:0 epoch:71 step:4 train loss:0.063889, train acc:97.556, train f1:97.562, train precision:97.530, train recall:97.595, train auc:99.735
fold:0 epoch:71 step:5 train loss:0.064841, train acc:97.522, train f1:97.541, train precision:97.282, train recall:97.802, train auc:99.727
fold:0 epoch:71 step:6 train loss:0.066315, train acc:97.464, train f1:97.488, train precision:96.413, train recall:98.588, train auc:99.725
fold:0 epoch:71 step:7 train loss:0.063647, train acc:97.525, train f1:97.539, train precision:96.529, train recall:98.571, train auc:99.738
fold:0 epoch:71 step:8 train loss:0.065231, train acc:97.510, train f1:97.499, train precision:97.397, train recall:97.600, train auc:99.729
fold:0 epoch:71 step:9 train loss:0.067039, train acc:97.397, train f1:97.385, train precision:97.196, train recall:97.574, train auc:99.709
fold:0 epoch:71        valid loss:0.077391, valid acc:97.427, valid f1:97.433, valid precision:97.200, valid recall:97.667, valid auc:99.605
[1;31mTest score increased (97.374951 --> 97.427191).[0m
[97.42719080579862, 97.43335852203143, 97.20034313343211, 97.66749379652605, 99.60532617208743]
====================================================================================================
fold:0 epoch:72 step:0 train loss:0.066010, train acc:97.482, train f1:97.516, train precision:96.675, train recall:98.372, train auc:99.724
fold:0 epoch:72 step:1 train loss:0.069807, train acc:97.314, train f1:97.321, train precision:96.610, train recall:98.044, train auc:99.691
fold:0 epoch:72 step:2 train loss:0.062673, train acc:97.626, train f1:97.639, train precision:97.746, train recall:97.532, train auc:99.755
fold:0 epoch:72 step:3 train loss:0.063902, train acc:97.571, train f1:97.573, train precision:97.950, train recall:97.200, train auc:99.753
fold:0 epoch:72 step:4 train loss:0.059117, train acc:97.745, train f1:97.749, train precision:97.002, train recall:98.508, train auc:99.776
fold:0 epoch:72 step:5 train loss:0.062650, train acc:97.617, train f1:97.641, train precision:96.340, train recall:98.977, train auc:99.765
fold:0 epoch:72 step:6 train loss:0.064320, train acc:97.528, train f1:97.558, train precision:96.956, train recall:98.168, train auc:99.726
fold:0 epoch:72 step:7 train loss:0.063288, train acc:97.662, train f1:97.657, train precision:97.604, train recall:97.711, train auc:99.738
fold:0 epoch:72 step:8 train loss:0.062979, train acc:97.607, train f1:97.614, train precision:97.365, train recall:97.864, train auc:99.740
fold:0 epoch:72 step:9 train loss:0.067918, train acc:97.300, train f1:97.268, train precision:96.657, train recall:97.886, train auc:99.703
fold:0 epoch:72        valid loss:0.075286, valid acc:97.487, valid f1:97.501, valid precision:96.955, valid recall:98.054, valid auc:99.626
[1;31mTest score increased (97.427191 --> 97.487267).[0m
[97.48726655348048, 97.50142849722093, 96.95498334151192, 98.05406817291367, 99.62574464670595]
====================================================================================================
fold:0 epoch:73 step:0 train loss:0.059802, train acc:97.662, train f1:97.664, train precision:97.154, train recall:98.179, train auc:99.768
fold:0 epoch:73 step:1 train loss:0.062511, train acc:97.540, train f1:97.540, train precision:97.267, train recall:97.815, train auc:99.745
fold:0 epoch:73 step:2 train loss:0.063158, train acc:97.501, train f1:97.516, train precision:97.065, train recall:97.971, train auc:99.740
fold:0 epoch:73 step:3 train loss:0.062561, train acc:97.595, train f1:97.632, train precision:96.708, train recall:98.574, train auc:99.750
fold:0 epoch:73 step:4 train loss:0.062843, train acc:97.580, train f1:97.594, train precision:97.066, train recall:98.127, train auc:99.747
fold:0 epoch:73 step:5 train loss:0.064332, train acc:97.574, train f1:97.605, train precision:97.315, train recall:97.897, train auc:99.728
fold:0 epoch:73 step:6 train loss:0.060249, train acc:97.794, train f1:97.799, train precision:97.452, train recall:98.149, train auc:99.764
fold:0 epoch:73 step:7 train loss:0.062890, train acc:97.662, train f1:97.660, train precision:97.185, train recall:98.140, train auc:99.734
fold:0 epoch:73 step:8 train loss:0.062195, train acc:97.549, train f1:97.555, train precision:97.032, train recall:98.084, train auc:99.743
fold:0 epoch:73 step:9 train loss:0.059489, train acc:97.704, train f1:97.725, train precision:96.873, train recall:98.593, train auc:99.763
fold:0 epoch:73        valid loss:0.076402, valid acc:97.464, valid f1:97.487, valid precision:96.607, valid recall:98.383, valid auc:99.624
[1;31mEarlyStopping counter: 1 out of 50[0m
[97.48726655348048, 97.50142849722093, 96.95498334151192, 98.05406817291367, 99.62574464670595]
====================================================================================================
fold:0 epoch:74 step:0 train loss:0.056813, train acc:97.849, train f1:97.857, train precision:97.599, train recall:98.117, train auc:99.786
fold:0 epoch:74 step:1 train loss:0.061882, train acc:97.614, train f1:97.619, train precision:97.152, train recall:98.091, train auc:99.751
fold:0 epoch:74 step:2 train loss:0.060387, train acc:97.717, train f1:97.731, train precision:97.148, train recall:98.322, train auc:99.760
fold:0 epoch:74 step:3 train loss:0.058319, train acc:97.772, train f1:97.792, train precision:97.427, train recall:98.160, train auc:99.774
fold:0 epoch:74 step:4 train loss:0.059652, train acc:97.684, train f1:97.697, train precision:97.452, train recall:97.944, train auc:99.774
fold:0 epoch:74 step:5 train loss:0.064511, train acc:97.537, train f1:97.536, train precision:97.082, train recall:97.994, train auc:99.720
fold:0 epoch:74 step:6 train loss:0.059629, train acc:97.672, train f1:97.673, train precision:97.119, train recall:98.233, train auc:99.769
fold:0 epoch:74 step:7 train loss:0.058361, train acc:97.824, train f1:97.831, train precision:97.301, train recall:98.367, train auc:99.774
fold:0 epoch:74 step:8 train loss:0.059365, train acc:97.754, train f1:97.773, train precision:97.425, train recall:98.123, train auc:99.767
fold:0 epoch:74 step:9 train loss:0.064473, train acc:97.397, train f1:97.406, train precision:97.082, train recall:97.731, train auc:99.725
fold:0 epoch:74        valid loss:0.075015, valid acc:97.529, valid f1:97.544, valid precision:96.974, valid recall:98.119, valid auc:99.638
[1;31mTest score increased (97.487267 --> 97.529058).[0m
[97.52905837795481, 97.54355898314768, 96.97446885406717, 98.11936789865483, 99.6380893905744]
====================================================================================================
fold:0 epoch:75 step:0 train loss:0.059104, train acc:97.794, train f1:97.812, train precision:97.467, train recall:98.160, train auc:99.766
fold:0 epoch:75 step:1 train loss:0.057022, train acc:97.791, train f1:97.801, train precision:97.434, train recall:98.171, train auc:99.792
fold:0 epoch:75 step:2 train loss:0.060610, train acc:97.705, train f1:97.726, train precision:97.273, train recall:98.183, train auc:99.758
fold:0 epoch:75 step:3 train loss:0.056837, train acc:97.839, train f1:97.854, train precision:97.405, train recall:98.307, train auc:99.786
fold:0 epoch:75 step:4 train loss:0.059257, train acc:97.772, train f1:97.774, train precision:97.205, train recall:98.350, train auc:99.766
fold:0 epoch:75 step:5 train loss:0.059625, train acc:97.794, train f1:97.795, train precision:97.264, train recall:98.332, train auc:99.766
fold:0 epoch:75 step:6 train loss:0.058466, train acc:97.855, train f1:97.860, train precision:97.317, train recall:98.408, train auc:99.773
fold:0 epoch:75 step:7 train loss:0.057915, train acc:97.885, train f1:97.882, train precision:97.438, train recall:98.330, train auc:99.778
fold:0 epoch:75 step:8 train loss:0.061025, train acc:97.739, train f1:97.765, train precision:97.433, train recall:98.099, train auc:99.756
fold:0 epoch:75 step:9 train loss:0.061587, train acc:97.740, train f1:97.746, train precision:97.175, train recall:98.324, train auc:99.749
fold:0 epoch:75        valid loss:0.074494, valid acc:97.515, valid f1:97.532, valid precision:96.841, valid recall:98.234, valid auc:99.642
[1;31mEarlyStopping counter: 1 out of 50[0m
[97.52905837795481, 97.54355898314768, 96.97446885406717, 98.11936789865483, 99.6380893905744]
====================================================================================================
fold:0 epoch:76 step:0 train loss:0.058294, train acc:97.806, train f1:97.823, train precision:97.454, train recall:98.194, train auc:99.777
fold:0 epoch:76 step:1 train loss:0.057449, train acc:97.739, train f1:97.749, train precision:97.439, train recall:98.062, train auc:99.786
fold:0 epoch:76 step:2 train loss:0.058982, train acc:97.757, train f1:97.771, train precision:97.255, train recall:98.293, train auc:99.771
fold:0 epoch:76 step:3 train loss:0.059398, train acc:97.690, train f1:97.679, train precision:97.146, train recall:98.218, train auc:99.768
fold:0 epoch:76 step:4 train loss:0.058220, train acc:97.839, train f1:97.821, train precision:97.653, train recall:97.990, train auc:99.773
fold:0 epoch:76 step:5 train loss:0.061640, train acc:97.678, train f1:97.685, train precision:97.843, train recall:97.528, train auc:99.757
fold:0 epoch:76 step:6 train loss:0.056371, train acc:97.800, train f1:97.837, train precision:96.973, train recall:98.717, train auc:99.794
fold:0 epoch:76 step:7 train loss:0.064606, train acc:97.513, train f1:97.557, train precision:96.473, train recall:98.666, train auc:99.750
fold:0 epoch:76 step:8 train loss:0.060761, train acc:97.720, train f1:97.713, train precision:97.287, train recall:98.143, train auc:99.756
fold:0 epoch:76 step:9 train loss:0.064084, train acc:97.661, train f1:97.661, train precision:97.954, train recall:97.370, train auc:99.738
fold:0 epoch:76        valid loss:0.073660, valid acc:97.500, valid f1:97.519, valid precision:96.796, valid recall:98.253, valid auc:99.648
[1;31mEarlyStopping counter: 2 out of 50[0m
[97.52905837795481, 97.54355898314768, 96.97446885406717, 98.11936789865483, 99.6380893905744]
====================================================================================================
fold:0 epoch:77 step:0 train loss:0.057181, train acc:97.800, train f1:97.819, train precision:97.610, train recall:98.030, train auc:99.793
fold:0 epoch:77 step:1 train loss:0.064004, train acc:97.507, train f1:97.522, train precision:96.385, train recall:98.686, train auc:99.753
fold:0 epoch:77 step:2 train loss:0.056356, train acc:97.882, train f1:97.893, train precision:97.396, train recall:98.395, train auc:99.792
fold:0 epoch:77 step:3 train loss:0.062609, train acc:97.632, train f1:97.634, train precision:97.993, train recall:97.278, train auc:99.762
fold:0 epoch:77 step:4 train loss:0.057134, train acc:97.806, train f1:97.800, train precision:97.755, train recall:97.845, train auc:99.788
fold:0 epoch:77 step:5 train loss:0.061797, train acc:97.684, train f1:97.713, train precision:96.622, train recall:98.830, train auc:99.757
fold:0 epoch:77 step:6 train loss:0.060366, train acc:97.693, train f1:97.727, train precision:96.669, train recall:98.808, train auc:99.767
fold:0 epoch:77 step:7 train loss:0.058355, train acc:97.778, train f1:97.781, train precision:97.437, train recall:98.128, train auc:99.781
fold:0 epoch:77 step:8 train loss:0.059927, train acc:97.784, train f1:97.766, train precision:98.098, train recall:97.437, train auc:99.775
fold:0 epoch:77 step:9 train loss:0.064961, train acc:97.449, train f1:97.443, train precision:97.460, train recall:97.425, train auc:99.728
fold:0 epoch:77        valid loss:0.072454, valid acc:97.610, valid f1:97.624, valid precision:97.037, valid recall:98.219, valid auc:99.659
[1;31mTest score increased (97.529058 --> 97.610030).[0m
[97.61003003787384, 97.62448725271302, 97.03749580655983, 98.21862348178138, 99.65879239700037]
====================================================================================================
fold:0 epoch:78 step:0 train loss:0.059061, train acc:97.717, train f1:97.722, train precision:96.896, train recall:98.563, train auc:99.778
fold:0 epoch:78 step:1 train loss:0.057483, train acc:97.791, train f1:97.803, train precision:96.967, train recall:98.653, train auc:99.790
fold:0 epoch:78 step:2 train loss:0.058970, train acc:97.833, train f1:97.853, train precision:97.464, train recall:98.245, train auc:99.764
fold:0 epoch:78 step:3 train loss:0.059567, train acc:97.775, train f1:97.771, train precision:97.744, train recall:97.798, train auc:99.768
fold:0 epoch:78 step:4 train loss:0.058113, train acc:97.839, train f1:97.848, train precision:97.593, train recall:98.104, train auc:99.775
fold:0 epoch:78 step:5 train loss:0.059327, train acc:97.757, train f1:97.764, train precision:96.959, train recall:98.583, train auc:99.777
fold:0 epoch:78 step:6 train loss:0.058939, train acc:97.757, train f1:97.781, train precision:97.320, train recall:98.247, train auc:99.770
fold:0 epoch:78 step:7 train loss:0.054718, train acc:97.974, train f1:97.976, train precision:97.696, train recall:98.258, train auc:99.805
fold:0 epoch:78 step:8 train loss:0.058358, train acc:97.794, train f1:97.798, train precision:97.617, train recall:97.980, train auc:99.777
fold:0 epoch:78 step:9 train loss:0.055337, train acc:97.880, train f1:97.918, train precision:97.657, train recall:98.181, train auc:99.796
fold:0 epoch:78        valid loss:0.072706, valid acc:97.558, valid f1:97.572, valid precision:97.013, valid recall:98.138, valid auc:99.662
[1;31mEarlyStopping counter: 1 out of 50[0m
[97.61003003787384, 97.62448725271302, 97.03749580655983, 98.21862348178138, 99.65879239700037]
====================================================================================================
fold:0 epoch:79 step:0 train loss:0.059405, train acc:97.699, train f1:97.710, train precision:96.950, train recall:98.481, train auc:99.779
fold:0 epoch:79 step:1 train loss:0.054281, train acc:97.961, train f1:97.969, train precision:97.265, train recall:98.683, train auc:99.810
fold:0 epoch:79 step:2 train loss:0.057202, train acc:97.864, train f1:97.882, train precision:97.793, train recall:97.971, train auc:99.783
fold:0 epoch:79 step:3 train loss:0.057036, train acc:97.861, train f1:97.863, train precision:97.723, train recall:98.004, train auc:99.792
fold:0 epoch:79 step:4 train loss:0.054793, train acc:97.931, train f1:97.946, train precision:97.292, train recall:98.609, train auc:99.806
fold:0 epoch:79 step:5 train loss:0.055099, train acc:97.928, train f1:97.933, train precision:97.461, train recall:98.409, train auc:99.800
fold:0 epoch:79 step:6 train loss:0.058384, train acc:97.739, train f1:97.764, train precision:97.520, train recall:98.009, train auc:99.781
fold:0 epoch:79 step:7 train loss:0.052664, train acc:98.007, train f1:98.025, train precision:97.603, train recall:98.451, train auc:99.816
fold:0 epoch:79 step:8 train loss:0.059405, train acc:97.757, train f1:97.756, train precision:97.160, train recall:98.360, train auc:99.772
fold:0 epoch:79 step:9 train loss:0.063266, train acc:97.819, train f1:97.790, train precision:97.478, train recall:98.105, train auc:99.737
fold:0 epoch:79        valid loss:0.072419, valid acc:97.611, valid f1:97.629, valid precision:96.924, valid recall:98.344, valid auc:99.665
[1;31mTest score increased (97.610030 --> 97.611336).[0m
[97.61133603238866, 97.62870959795671, 96.92375019307006, 98.3439989552044, 99.66510152568027]
====================================================================================================
fold:0 epoch:80 step:0 train loss:0.056639, train acc:97.855, train f1:97.863, train precision:97.771, train recall:97.956, train auc:99.796
fold:0 epoch:80 step:1 train loss:0.055288, train acc:97.940, train f1:97.948, train precision:97.707, train recall:98.190, train auc:99.804
fold:0 epoch:80 step:2 train loss:0.053805, train acc:97.968, train f1:97.987, train precision:97.345, train recall:98.637, train auc:99.819
fold:0 epoch:80 step:3 train loss:0.054163, train acc:97.937, train f1:97.943, train precision:97.374, train recall:98.518, train auc:99.812
fold:0 epoch:80 step:4 train loss:0.054828, train acc:97.934, train f1:97.937, train precision:97.737, train recall:98.137, train auc:99.805
fold:0 epoch:80 step:5 train loss:0.058863, train acc:97.775, train f1:97.771, train precision:97.732, train recall:97.810, train auc:99.774
fold:0 epoch:80 step:6 train loss:0.057076, train acc:97.958, train f1:97.953, train precision:97.555, train recall:98.353, train auc:99.782
fold:0 epoch:80 step:7 train loss:0.053630, train acc:97.922, train f1:97.948, train precision:97.604, train recall:98.295, train auc:99.810
fold:0 epoch:80 step:8 train loss:0.056274, train acc:97.818, train f1:97.831, train precision:96.999, train recall:98.678, train auc:99.802
fold:0 epoch:80 step:9 train loss:0.058074, train acc:97.748, train f1:97.746, train precision:97.591, train recall:97.901, train auc:99.782
fold:0 epoch:80        valid loss:0.074295, valid acc:97.492, valid f1:97.519, valid precision:96.511, valid recall:98.548, valid auc:99.666
[1;31mEarlyStopping counter: 1 out of 50[0m
[97.61133603238866, 97.62870959795671, 96.92375019307006, 98.3439989552044, 99.66510152568027]
====================================================================================================
fold:0 epoch:81 step:0 train loss:0.057218, train acc:97.812, train f1:97.825, train precision:98.102, train recall:97.550, train auc:99.794
fold:0 epoch:81 step:1 train loss:0.052894, train acc:97.931, train f1:97.948, train precision:97.317, train recall:98.586, train auc:99.821
fold:0 epoch:81 step:2 train loss:0.054728, train acc:97.983, train f1:97.994, train precision:97.083, train recall:98.921, train auc:99.813
fold:0 epoch:81 step:3 train loss:0.054626, train acc:97.906, train f1:97.915, train precision:97.470, train recall:98.363, train auc:99.801
fold:0 epoch:81 step:4 train loss:0.054970, train acc:97.910, train f1:97.919, train precision:97.970, train recall:97.869, train auc:99.806
fold:0 epoch:81 step:5 train loss:0.054120, train acc:97.919, train f1:97.907, train precision:97.697, train recall:98.118, train auc:99.810
fold:0 epoch:81 step:6 train loss:0.055422, train acc:97.910, train f1:97.910, train precision:97.373, train recall:98.454, train auc:99.800
fold:0 epoch:81 step:7 train loss:0.054922, train acc:97.986, train f1:97.999, train precision:97.496, train recall:98.507, train auc:99.799
fold:0 epoch:81 step:8 train loss:0.056735, train acc:97.787, train f1:97.794, train precision:97.371, train recall:98.222, train auc:99.790
fold:0 epoch:81 step:9 train loss:0.062967, train acc:97.731, train f1:97.748, train precision:97.459, train recall:98.039, train auc:99.723
fold:0 epoch:81        valid loss:0.071771, valid acc:97.635, valid f1:97.648, valid precision:97.117, valid recall:98.185, valid auc:99.667
[1;31mTest score increased (97.611336 --> 97.634844).[0m
[97.63484393365547, 97.6477770128976, 97.11672608897845, 98.18466762439598, 99.66657293138495]
====================================================================================================
fold:0 epoch:82 step:0 train loss:0.050866, train acc:98.032, train f1:98.024, train precision:97.787, train recall:98.262, train auc:99.832
fold:0 epoch:82 step:1 train loss:0.051698, train acc:98.071, train f1:98.093, train precision:97.833, train recall:98.354, train auc:99.826
fold:0 epoch:82 step:2 train loss:0.056161, train acc:97.931, train f1:97.937, train precision:97.209, train recall:98.676, train auc:99.798
fold:0 epoch:82 step:3 train loss:0.055794, train acc:97.943, train f1:97.940, train precision:97.582, train recall:98.300, train auc:99.786
fold:0 epoch:82 step:4 train loss:0.052232, train acc:98.016, train f1:98.011, train precision:98.185, train recall:97.837, train auc:99.827
fold:0 epoch:82 step:5 train loss:0.051937, train acc:98.080, train f1:98.082, train precision:97.793, train recall:98.373, train auc:99.822
fold:0 epoch:82 step:6 train loss:0.055504, train acc:97.931, train f1:97.972, train precision:97.315, train recall:98.639, train auc:99.796
fold:0 epoch:82 step:7 train loss:0.055274, train acc:97.910, train f1:97.922, train precision:97.124, train recall:98.734, train auc:99.806
fold:0 epoch:82 step:8 train loss:0.054859, train acc:97.940, train f1:97.936, train precision:97.730, train recall:98.143, train auc:99.796
fold:0 epoch:82 step:9 train loss:0.056001, train acc:97.854, train f1:97.868, train precision:98.090, train recall:97.646, train auc:99.786
fold:0 epoch:82        valid loss:0.069837, valid acc:97.780, valid f1:97.786, valid precision:97.524, valid recall:98.049, valid auc:99.684
[1;31mTest score increased (97.634844 --> 97.779809).[0m
[97.77980932480084, 97.78576638532876, 97.5240965420488, 98.04884419485438, 99.68353059729216]
====================================================================================================
fold:0 epoch:83 step:0 train loss:0.054242, train acc:97.931, train f1:97.948, train precision:97.418, train recall:98.485, train auc:99.807
fold:0 epoch:83 step:1 train loss:0.051074, train acc:98.148, train f1:98.147, train precision:97.171, train recall:99.143, train auc:99.843
fold:0 epoch:83 step:2 train loss:0.052508, train acc:97.949, train f1:97.961, train precision:97.688, train recall:98.235, train auc:99.816
fold:0 epoch:83 step:3 train loss:0.058044, train acc:97.824, train f1:97.814, train precision:98.160, train recall:97.470, train auc:99.795
fold:0 epoch:83 step:4 train loss:0.056915, train acc:97.861, train f1:97.864, train precision:97.498, train recall:98.232, train auc:99.786
fold:0 epoch:83 step:5 train loss:0.056120, train acc:97.891, train f1:97.910, train precision:97.102, train recall:98.731, train auc:99.802
fold:0 epoch:83 step:6 train loss:0.052491, train acc:97.955, train f1:97.965, train precision:97.645, train recall:98.288, train auc:99.820
fold:0 epoch:83 step:7 train loss:0.055686, train acc:97.830, train f1:97.840, train precision:97.760, train recall:97.920, train auc:99.800
fold:0 epoch:83 step:8 train loss:0.056007, train acc:97.806, train f1:97.816, train precision:97.795, train recall:97.837, train auc:99.799
fold:0 epoch:83 step:9 train loss:0.057828, train acc:97.652, train f1:97.658, train precision:96.733, train recall:98.601, train auc:99.800
fold:0 epoch:83        valid loss:0.070395, valid acc:97.707, valid f1:97.721, valid precision:97.099, valid recall:98.352, valid auc:99.686
[1;31mEarlyStopping counter: 1 out of 50[0m
[97.77980932480084, 97.78576638532876, 97.5240965420488, 98.04884419485438, 99.68353059729216]
====================================================================================================
fold:0 epoch:84 step:0 train loss:0.051098, train acc:98.029, train f1:98.052, train precision:97.464, train recall:98.647, train auc:99.828
fold:0 epoch:84 step:1 train loss:0.053350, train acc:97.943, train f1:97.960, train precision:97.671, train recall:98.252, train auc:99.814
fold:0 epoch:84 step:2 train loss:0.048431, train acc:98.175, train f1:98.176, train precision:97.664, train recall:98.694, train auc:99.847
fold:0 epoch:84 step:3 train loss:0.054982, train acc:98.062, train f1:98.064, train precision:97.858, train recall:98.270, train auc:99.795
fold:0 epoch:84 step:4 train loss:0.050147, train acc:98.117, train f1:98.128, train precision:97.752, train recall:98.508, train auc:99.835
fold:0 epoch:84 step:5 train loss:0.050088, train acc:98.056, train f1:98.079, train precision:97.676, train recall:98.486, train auc:99.838
fold:0 epoch:84 step:6 train loss:0.051031, train acc:98.099, train f1:98.107, train precision:97.724, train recall:98.493, train auc:99.829
fold:0 epoch:84 step:7 train loss:0.057410, train acc:97.699, train f1:97.690, train precision:97.428, train recall:97.954, train auc:99.789
fold:0 epoch:84 step:8 train loss:0.054460, train acc:97.925, train f1:97.920, train precision:97.698, train recall:98.142, train auc:99.804
fold:0 epoch:84 step:9 train loss:0.055894, train acc:97.951, train f1:97.932, train precision:97.473, train recall:98.395, train auc:99.793
fold:0 epoch:84        valid loss:0.070972, valid acc:97.692, valid f1:97.706, valid precision:97.139, valid recall:98.279, valid auc:99.678
[1;31mEarlyStopping counter: 2 out of 50[0m
[97.77980932480084, 97.78576638532876, 97.5240965420488, 98.04884419485438, 99.68353059729216]
====================================================================================================
fold:0 epoch:85 step:0 train loss:0.051358, train acc:98.029, train f1:98.047, train precision:97.752, train recall:98.345, train auc:99.829
fold:0 epoch:85 step:1 train loss:0.050854, train acc:98.047, train f1:98.062, train precision:97.783, train recall:98.342, train auc:99.832
fold:0 epoch:85 step:2 train loss:0.049432, train acc:98.108, train f1:98.107, train precision:97.571, train recall:98.649, train auc:99.844
fold:0 epoch:85 step:3 train loss:0.051704, train acc:98.068, train f1:98.073, train precision:97.844, train recall:98.304, train auc:99.823
fold:0 epoch:85 step:4 train loss:0.051130, train acc:98.138, train f1:98.133, train precision:97.869, train recall:98.398, train auc:99.821
fold:0 epoch:85 step:5 train loss:0.051239, train acc:98.056, train f1:98.066, train precision:97.719, train recall:98.416, train auc:99.828
fold:0 epoch:85 step:6 train loss:0.052969, train acc:98.077, train f1:98.087, train precision:97.572, train recall:98.608, train auc:99.811
fold:0 epoch:85 step:7 train loss:0.054839, train acc:97.919, train f1:97.920, train precision:97.694, train recall:98.148, train auc:99.806
fold:0 epoch:85 step:8 train loss:0.055861, train acc:97.934, train f1:97.941, train precision:97.588, train recall:98.297, train auc:99.789
fold:0 epoch:85 step:9 train loss:0.051065, train acc:98.012, train f1:98.021, train precision:97.748, train recall:98.297, train auc:99.832
fold:0 epoch:85        valid loss:0.067646, valid acc:97.739, valid f1:97.755, valid precision:97.062, valid recall:98.459, valid auc:99.703
[1;31mEarlyStopping counter: 3 out of 50[0m
[97.77980932480084, 97.78576638532876, 97.5240965420488, 98.04884419485438, 99.68353059729216]
====================================================================================================
fold:0 epoch:86 step:0 train loss:0.050176, train acc:98.206, train f1:98.223, train precision:98.045, train recall:98.401, train auc:99.832
fold:0 epoch:86 step:1 train loss:0.052875, train acc:98.050, train f1:98.059, train precision:97.375, train recall:98.752, train auc:99.819
fold:0 epoch:86 step:2 train loss:0.049693, train acc:98.175, train f1:98.183, train precision:97.767, train recall:98.602, train auc:99.835
fold:0 epoch:86 step:3 train loss:0.051610, train acc:97.995, train f1:98.005, train precision:97.984, train recall:98.026, train auc:99.829
fold:0 epoch:86 step:4 train loss:0.050443, train acc:98.114, train f1:98.122, train precision:97.789, train recall:98.457, train auc:99.836
fold:0 epoch:86 step:5 train loss:0.050303, train acc:98.099, train f1:98.108, train precision:97.678, train recall:98.542, train auc:99.837
fold:0 epoch:86 step:6 train loss:0.052075, train acc:98.114, train f1:98.109, train precision:97.744, train recall:98.476, train auc:99.817
fold:0 epoch:86 step:7 train loss:0.051020, train acc:98.053, train f1:98.048, train precision:98.144, train recall:97.952, train auc:99.837
fold:0 epoch:86 step:8 train loss:0.052730, train acc:98.035, train f1:98.022, train precision:97.841, train recall:98.203, train auc:99.814
fold:0 epoch:86 step:9 train loss:0.052457, train acc:98.127, train f1:98.164, train precision:97.701, train recall:98.632, train auc:99.814
fold:0 epoch:86        valid loss:0.069732, valid acc:97.728, valid f1:97.744, valid precision:97.032, valid recall:98.467, valid auc:99.697
[1;31mEarlyStopping counter: 4 out of 50[0m
[97.77980932480084, 97.78576638532876, 97.5240965420488, 98.04884419485438, 99.68353059729216]
====================================================================================================
fold:0 epoch:87 step:0 train loss:0.049815, train acc:98.047, train f1:98.064, train precision:97.410, train recall:98.727, train auc:99.842
fold:0 epoch:87 step:1 train loss:0.049246, train acc:98.111, train f1:98.109, train precision:97.712, train recall:98.509, train auc:99.840
fold:0 epoch:87 step:2 train loss:0.049433, train acc:98.141, train f1:98.150, train precision:98.082, train recall:98.219, train auc:99.836
fold:0 epoch:87 step:3 train loss:0.051616, train acc:98.013, train f1:98.003, train precision:97.629, train recall:98.380, train auc:99.824
fold:0 epoch:87 step:4 train loss:0.049291, train acc:98.108, train f1:98.121, train precision:97.530, train recall:98.720, train auc:99.846
fold:0 epoch:87 step:5 train loss:0.048229, train acc:98.175, train f1:98.180, train precision:98.007, train recall:98.353, train auc:99.852
fold:0 epoch:87 step:6 train loss:0.053079, train acc:97.958, train f1:97.960, train precision:97.868, train recall:98.053, train auc:99.819
fold:0 epoch:87 step:7 train loss:0.049006, train acc:98.102, train f1:98.109, train precision:97.799, train recall:98.420, train auc:99.840
fold:0 epoch:87 step:8 train loss:0.049600, train acc:98.102, train f1:98.121, train precision:97.726, train recall:98.520, train auc:99.839
fold:0 epoch:87 step:9 train loss:0.045787, train acc:98.355, train f1:98.360, train precision:97.803, train recall:98.924, train auc:99.861
fold:0 epoch:87        valid loss:0.071115, valid acc:97.640, valid f1:97.659, valid precision:96.865, valid recall:98.467, valid auc:99.696
[1;31mEarlyStopping counter: 5 out of 50[0m
[97.77980932480084, 97.78576638532876, 97.5240965420488, 98.04884419485438, 99.68353059729216]
====================================================================================================
fold:0 epoch:88 step:0 train loss:0.045758, train acc:98.285, train f1:98.296, train precision:97.970, train recall:98.625, train auc:99.866
fold:0 epoch:88 step:1 train loss:0.047802, train acc:98.260, train f1:98.272, train precision:98.105, train recall:98.439, train auc:99.852
fold:0 epoch:88 step:2 train loss:0.050379, train acc:98.090, train f1:98.084, train precision:97.511, train recall:98.664, train auc:99.833
fold:0 epoch:88 step:3 train loss:0.051463, train acc:98.026, train f1:98.028, train precision:97.775, train recall:98.283, train auc:99.827
fold:0 epoch:88 step:4 train loss:0.051916, train acc:97.955, train f1:97.961, train precision:97.860, train recall:98.063, train auc:99.828
fold:0 epoch:88 step:5 train loss:0.050463, train acc:98.126, train f1:98.131, train precision:98.113, train recall:98.149, train auc:99.838
fold:0 epoch:88 step:6 train loss:0.050040, train acc:98.175, train f1:98.188, train precision:97.537, train recall:98.847, train auc:99.835
fold:0 epoch:88 step:7 train loss:0.051194, train acc:98.090, train f1:98.105, train precision:97.720, train recall:98.492, train auc:99.829
fold:0 epoch:88 step:8 train loss:0.049736, train acc:98.077, train f1:98.083, train precision:97.993, train recall:98.173, train auc:99.842
fold:0 epoch:88 step:9 train loss:0.049973, train acc:98.171, train f1:98.135, train precision:97.662, train recall:98.612, train auc:99.829
fold:0 epoch:88        valid loss:0.069628, valid acc:97.819, valid f1:97.834, valid precision:97.173, valid recall:98.503, valid auc:99.705
[1;31mTest score increased (97.779809 --> 97.818989).[0m
[97.81898916024552, 97.8338132669209, 97.1733360818367, 98.5033302860128, 99.70502228317963]
====================================================================================================
fold:0 epoch:89 step:0 train loss:0.045961, train acc:98.273, train f1:98.268, train precision:98.190, train recall:98.346, train auc:99.865
fold:0 epoch:89 step:1 train loss:0.046978, train acc:98.199, train f1:98.200, train precision:97.699, train recall:98.706, train auc:99.857
fold:0 epoch:89 step:2 train loss:0.050387, train acc:98.087, train f1:98.088, train precision:97.739, train recall:98.439, train auc:99.834
fold:0 epoch:89 step:3 train loss:0.046846, train acc:98.242, train f1:98.242, train precision:98.122, train recall:98.362, train auc:99.856
fold:0 epoch:89 step:4 train loss:0.050427, train acc:98.181, train f1:98.185, train precision:98.018, train recall:98.353, train auc:99.830
fold:0 epoch:89 step:5 train loss:0.049643, train acc:98.074, train f1:98.078, train precision:97.795, train recall:98.363, train auc:99.839
fold:0 epoch:89 step:6 train loss:0.047203, train acc:98.276, train f1:98.283, train precision:97.935, train recall:98.634, train auc:99.850
fold:0 epoch:89 step:7 train loss:0.049534, train acc:98.074, train f1:98.079, train precision:98.082, train recall:98.076, train auc:99.846
fold:0 epoch:89 step:8 train loss:0.048766, train acc:98.114, train f1:98.136, train precision:97.947, train recall:98.326, train auc:99.847
fold:0 epoch:89 step:9 train loss:0.051272, train acc:98.030, train f1:98.039, train precision:97.578, train recall:98.505, train auc:99.823
fold:0 epoch:89        valid loss:0.068784, valid acc:97.777, valid f1:97.794, valid precision:97.055, valid recall:98.545, valid auc:99.708
[1;31mEarlyStopping counter: 1 out of 50[0m
[97.81898916024552, 97.8338132669209, 97.1733360818367, 98.5033302860128, 99.70502228317963]
====================================================================================================
fold:0 epoch:90 step:0 train loss:0.047491, train acc:98.178, train f1:98.192, train precision:97.710, train recall:98.679, train auc:99.850
fold:0 epoch:90 step:1 train loss:0.050126, train acc:98.062, train f1:98.079, train precision:97.910, train recall:98.248, train auc:99.840
fold:0 epoch:90 step:2 train loss:0.047718, train acc:98.203, train f1:98.196, train precision:97.899, train recall:98.495, train auc:99.848
fold:0 epoch:90 step:3 train loss:0.048532, train acc:98.108, train f1:98.096, train precision:97.664, train recall:98.532, train auc:99.844
fold:0 epoch:90 step:4 train loss:0.047016, train acc:98.199, train f1:98.206, train precision:97.820, train recall:98.596, train auc:99.854
fold:0 epoch:90 step:5 train loss:0.049668, train acc:98.199, train f1:98.191, train precision:97.885, train recall:98.499, train auc:99.835
fold:0 epoch:90 step:6 train loss:0.050070, train acc:98.065, train f1:98.093, train precision:97.817, train recall:98.371, train auc:99.839
fold:0 epoch:90 step:7 train loss:0.047498, train acc:98.251, train f1:98.255, train precision:97.717, train recall:98.800, train auc:99.851
fold:0 epoch:90 step:8 train loss:0.047447, train acc:98.215, train f1:98.230, train precision:97.978, train recall:98.483, train auc:99.851
fold:0 epoch:90 step:9 train loss:0.042965, train acc:98.399, train f1:98.406, train precision:98.165, train recall:98.648, train auc:99.877
fold:0 epoch:90        valid loss:0.069056, valid acc:97.799, valid f1:97.812, valid precision:97.255, valid recall:98.375, valid auc:99.704
[1;31mEarlyStopping counter: 2 out of 50[0m
[97.81898916024552, 97.8338132669209, 97.1733360818367, 98.5033302860128, 99.70502228317963]
====================================================================================================
fold:0 epoch:91 step:0 train loss:0.046065, train acc:98.230, train f1:98.228, train precision:98.019, train recall:98.439, train auc:99.859
fold:0 epoch:91 step:1 train loss:0.046804, train acc:98.215, train f1:98.220, train precision:97.914, train recall:98.529, train auc:99.857
fold:0 epoch:91 step:2 train loss:0.047578, train acc:98.215, train f1:98.228, train precision:97.994, train recall:98.464, train auc:99.855
fold:0 epoch:91 step:3 train loss:0.047620, train acc:98.184, train f1:98.196, train precision:97.831, train recall:98.564, train auc:99.848
fold:0 epoch:91 step:4 train loss:0.046549, train acc:98.245, train f1:98.257, train precision:97.999, train recall:98.517, train auc:99.860
fold:0 epoch:91 step:5 train loss:0.048377, train acc:98.148, train f1:98.168, train precision:97.893, train recall:98.444, train auc:99.842
fold:0 epoch:91 step:6 train loss:0.047067, train acc:98.190, train f1:98.176, train precision:97.717, train recall:98.640, train auc:99.855
fold:0 epoch:91 step:7 train loss:0.047379, train acc:98.151, train f1:98.153, train precision:97.819, train recall:98.489, train auc:99.852
fold:0 epoch:91 step:8 train loss:0.050341, train acc:98.148, train f1:98.150, train precision:98.022, train recall:98.279, train auc:99.836
fold:0 epoch:91 step:9 train loss:0.047143, train acc:98.223, train f1:98.219, train precision:97.667, train recall:98.776, train auc:99.860
fold:0 epoch:91        valid loss:0.067596, valid acc:97.871, valid f1:97.884, valid precision:97.318, valid recall:98.456, valid auc:99.719
[1;31mTest score increased (97.818989 --> 97.871229).[0m
[97.87122894083845, 97.88361162325691, 97.31753285312266, 98.45631448347916, 99.71899463358545]
====================================================================================================
fold:0 epoch:92 step:0 train loss:0.043777, train acc:98.322, train f1:98.332, train precision:98.011, train recall:98.655, train auc:99.873
fold:0 epoch:92 step:1 train loss:0.046157, train acc:98.273, train f1:98.278, train precision:98.122, train recall:98.433, train auc:99.858
fold:0 epoch:92 step:2 train loss:0.043993, train acc:98.331, train f1:98.339, train precision:98.199, train recall:98.480, train auc:99.877
fold:0 epoch:92 step:3 train loss:0.047914, train acc:98.117, train f1:98.120, train precision:97.600, train recall:98.646, train auc:99.848
fold:0 epoch:92 step:4 train loss:0.045903, train acc:98.248, train f1:98.238, train precision:98.135, train recall:98.340, train auc:99.862
fold:0 epoch:92 step:5 train loss:0.045287, train acc:98.337, train f1:98.328, train precision:98.139, train recall:98.519, train auc:99.863
fold:0 epoch:92 step:6 train loss:0.049865, train acc:98.093, train f1:98.095, train precision:97.919, train recall:98.272, train auc:99.838
fold:0 epoch:92 step:7 train loss:0.045494, train acc:98.294, train f1:98.299, train precision:97.820, train recall:98.783, train auc:99.862
fold:0 epoch:92 step:8 train loss:0.045839, train acc:98.227, train f1:98.251, train precision:98.200, train recall:98.301, train auc:99.863
fold:0 epoch:92 step:9 train loss:0.047791, train acc:98.232, train f1:98.236, train precision:97.713, train recall:98.765, train auc:99.852
fold:0 epoch:92        valid loss:0.066309, valid acc:97.883, valid f1:97.896, valid precision:97.308, valid recall:98.490, valid auc:99.722
[1;31mTest score increased (97.871229 --> 97.882983).[0m
[97.88298289147185, 97.89576166677485, 97.3083870967742, 98.49027034086457, 99.72215354726069]
====================================================================================================
fold:0 epoch:93 step:0 train loss:0.044235, train acc:98.297, train f1:98.309, train precision:98.118, train recall:98.500, train auc:99.871
fold:0 epoch:93 step:1 train loss:0.045851, train acc:98.291, train f1:98.288, train precision:97.989, train recall:98.590, train auc:99.864
fold:0 epoch:93 step:2 train loss:0.046031, train acc:98.300, train f1:98.297, train precision:97.964, train recall:98.632, train auc:99.860
fold:0 epoch:93 step:3 train loss:0.043840, train acc:98.294, train f1:98.314, train precision:98.139, train recall:98.489, train auc:99.874
fold:0 epoch:93 step:4 train loss:0.044296, train acc:98.322, train f1:98.322, train precision:97.726, train recall:98.926, train auc:99.875
fold:0 epoch:93 step:5 train loss:0.045478, train acc:98.148, train f1:98.159, train precision:98.001, train recall:98.317, train auc:99.870
fold:0 epoch:93 step:6 train loss:0.045741, train acc:98.254, train f1:98.254, train precision:97.913, train recall:98.597, train auc:99.856
fold:0 epoch:93 step:7 train loss:0.049847, train acc:98.129, train f1:98.127, train precision:98.070, train recall:98.184, train auc:99.834
fold:0 epoch:93 step:8 train loss:0.045910, train acc:98.285, train f1:98.294, train precision:97.695, train recall:98.901, train auc:99.866
fold:0 epoch:93 step:9 train loss:0.042037, train acc:98.461, train f1:98.479, train precision:98.061, train recall:98.900, train auc:99.877
fold:0 epoch:93        valid loss:0.067293, valid acc:97.908, valid f1:97.921, valid precision:97.315, valid recall:98.535, valid auc:99.718
[1;31mTest score increased (97.882983 --> 97.907797).[0m
[97.9077967872535, 97.92083062946139, 97.3145879014575, 98.53467415436855, 99.71834158515942]
====================================================================================================
fold:0 epoch:94 step:0 train loss:0.046376, train acc:98.276, train f1:98.257, train precision:98.382, train recall:98.133, train auc:99.863
fold:0 epoch:94 step:1 train loss:0.047894, train acc:98.285, train f1:98.283, train precision:98.325, train recall:98.241, train auc:99.848
fold:0 epoch:94 step:2 train loss:0.044042, train acc:98.407, train f1:98.418, train precision:98.032, train recall:98.807, train auc:99.869
fold:0 epoch:94 step:3 train loss:0.044933, train acc:98.230, train f1:98.247, train precision:97.615, train recall:98.886, train auc:99.875
fold:0 epoch:94 step:4 train loss:0.044465, train acc:98.340, train f1:98.344, train precision:98.052, train recall:98.639, train auc:99.874
fold:0 epoch:94 step:5 train loss:0.048073, train acc:98.209, train f1:98.210, train precision:98.267, train recall:98.154, train auc:99.853
fold:0 epoch:94 step:6 train loss:0.047157, train acc:98.239, train f1:98.234, train precision:97.824, train recall:98.648, train auc:99.850
fold:0 epoch:94 step:7 train loss:0.044803, train acc:98.315, train f1:98.334, train precision:98.003, train recall:98.667, train auc:99.867
fold:0 epoch:94 step:8 train loss:0.046997, train acc:98.303, train f1:98.314, train precision:98.034, train recall:98.595, train auc:99.851
fold:0 epoch:94 step:9 train loss:0.046177, train acc:98.320, train f1:98.319, train precision:98.068, train recall:98.570, train auc:99.854
fold:0 epoch:94        valid loss:0.065537, valid acc:97.937, valid f1:97.948, valid precision:97.411, valid recall:98.490, valid auc:99.733
[1;31mTest score increased (97.907797 --> 97.936529).[0m
[97.9365286665796, 97.94789204353586, 97.41145470045726, 98.49027034086457, 99.73299799219335]
====================================================================================================
fold:0 epoch:95 step:0 train loss:0.045136, train acc:98.178, train f1:98.168, train precision:98.087, train recall:98.249, train auc:99.869
fold:0 epoch:95 step:1 train loss:0.042338, train acc:98.441, train f1:98.449, train precision:98.250, train recall:98.650, train auc:99.887
fold:0 epoch:95 step:2 train loss:0.044688, train acc:98.251, train f1:98.260, train precision:97.482, train recall:99.051, train auc:99.868
fold:0 epoch:95 step:3 train loss:0.045389, train acc:98.245, train f1:98.253, train precision:97.994, train recall:98.514, train auc:99.869
fold:0 epoch:95 step:4 train loss:0.046018, train acc:98.251, train f1:98.248, train precision:97.988, train recall:98.510, train auc:99.858
fold:0 epoch:95 step:5 train loss:0.047273, train acc:98.105, train f1:98.112, train precision:97.966, train recall:98.258, train auc:99.859
fold:0 epoch:95 step:6 train loss:0.045693, train acc:98.306, train f1:98.305, train precision:98.033, train recall:98.579, train auc:99.860
fold:0 epoch:95 step:7 train loss:0.046764, train acc:98.309, train f1:98.317, train precision:98.020, train recall:98.617, train auc:99.852
fold:0 epoch:95 step:8 train loss:0.045868, train acc:98.322, train f1:98.343, train precision:98.101, train recall:98.587, train auc:99.861
fold:0 epoch:95 step:9 train loss:0.042859, train acc:98.434, train f1:98.430, train precision:97.980, train recall:98.883, train auc:99.874
fold:0 epoch:95        valid loss:0.066418, valid acc:97.900, valid f1:97.913, valid precision:97.304, valid recall:98.529, valid auc:99.727
[1;31mEarlyStopping counter: 1 out of 50[0m
[97.9365286665796, 97.94789204353586, 97.41145470045726, 98.49027034086457, 99.73299799219335]
====================================================================================================
fold:0 epoch:96 step:0 train loss:0.042989, train acc:98.410, train f1:98.414, train precision:98.489, train recall:98.340, train auc:99.882
fold:0 epoch:96 step:1 train loss:0.047338, train acc:98.221, train f1:98.225, train precision:97.752, train recall:98.703, train auc:99.855
fold:0 epoch:96 step:2 train loss:0.041075, train acc:98.425, train f1:98.434, train precision:98.124, train recall:98.746, train auc:99.889
fold:0 epoch:96 step:3 train loss:0.043243, train acc:98.376, train f1:98.374, train precision:98.146, train recall:98.603, train auc:99.876
fold:0 epoch:96 step:4 train loss:0.044959, train acc:98.383, train f1:98.380, train precision:98.440, train recall:98.319, train auc:99.870
fold:0 epoch:96 step:5 train loss:0.043218, train acc:98.392, train f1:98.393, train precision:98.235, train recall:98.553, train auc:99.875
fold:0 epoch:96 step:6 train loss:0.047666, train acc:98.196, train f1:98.203, train precision:97.458, train recall:98.958, train auc:99.857
fold:0 epoch:96 step:7 train loss:0.045479, train acc:98.383, train f1:98.388, train precision:98.083, train recall:98.694, train auc:99.861
fold:0 epoch:96 step:8 train loss:0.043924, train acc:98.370, train f1:98.385, train precision:98.313, train recall:98.456, train auc:99.877
fold:0 epoch:96 step:9 train loss:0.043774, train acc:98.303, train f1:98.302, train precision:97.914, train recall:98.693, train auc:99.866
fold:0 epoch:96        valid loss:0.066783, valid acc:97.926, valid f1:97.938, valid precision:97.386, valid recall:98.495, valid auc:99.726
[1;31mEarlyStopping counter: 2 out of 50[0m
[97.9365286665796, 97.94789204353586, 97.41145470045726, 98.49027034086457, 99.73299799219335]
====================================================================================================
fold:0 epoch:97 step:0 train loss:0.044535, train acc:98.312, train f1:98.315, train precision:97.919, train recall:98.715, train auc:99.871
fold:0 epoch:97 step:1 train loss:0.042370, train acc:98.370, train f1:98.371, train precision:98.311, train recall:98.431, train auc:99.881
fold:0 epoch:97 step:2 train loss:0.042375, train acc:98.459, train f1:98.474, train precision:98.222, train recall:98.728, train auc:99.884
fold:0 epoch:97 step:3 train loss:0.044422, train acc:98.404, train f1:98.418, train precision:97.994, train recall:98.845, train auc:99.869
fold:0 epoch:97 step:4 train loss:0.044704, train acc:98.309, train f1:98.299, train precision:97.878, train recall:98.723, train auc:99.871
fold:0 epoch:97 step:5 train loss:0.041760, train acc:98.465, train f1:98.464, train precision:98.485, train recall:98.443, train auc:99.882
fold:0 epoch:97 step:6 train loss:0.046482, train acc:98.209, train f1:98.211, train precision:98.064, train recall:98.358, train auc:99.857
fold:0 epoch:97 step:7 train loss:0.045785, train acc:98.181, train f1:98.187, train precision:97.883, train recall:98.493, train auc:99.865
fold:0 epoch:97 step:8 train loss:0.047241, train acc:98.230, train f1:98.232, train precision:97.784, train recall:98.683, train auc:99.851
fold:0 epoch:97 step:9 train loss:0.043327, train acc:98.391, train f1:98.412, train precision:98.455, train recall:98.369, train auc:99.881
fold:0 epoch:97        valid loss:0.066138, valid acc:97.903, valid f1:97.915, valid precision:97.346, valid recall:98.490, valid auc:99.736
[1;31mEarlyStopping counter: 3 out of 50[0m
[97.9365286665796, 97.94789204353586, 97.41145470045726, 98.49027034086457, 99.73299799219335]
====================================================================================================
fold:0 epoch:98 step:0 train loss:0.041373, train acc:98.480, train f1:98.481, train precision:98.331, train recall:98.632, train auc:99.882
fold:0 epoch:98 step:1 train loss:0.042618, train acc:98.410, train f1:98.423, train precision:97.970, train recall:98.881, train auc:99.879
fold:0 epoch:98 step:2 train loss:0.042833, train acc:98.434, train f1:98.443, train precision:98.243, train recall:98.644, train auc:99.883
fold:0 epoch:98 step:3 train loss:0.043520, train acc:98.456, train f1:98.463, train precision:98.457, train recall:98.469, train auc:99.877
fold:0 epoch:98 step:4 train loss:0.041103, train acc:98.465, train f1:98.467, train precision:98.236, train recall:98.698, train auc:99.888
fold:0 epoch:98 step:5 train loss:0.046472, train acc:98.203, train f1:98.219, train precision:97.885, train recall:98.556, train auc:99.866
fold:0 epoch:98 step:6 train loss:0.041627, train acc:98.434, train f1:98.440, train precision:98.240, train recall:98.641, train auc:99.887
fold:0 epoch:98 step:7 train loss:0.041094, train acc:98.492, train f1:98.496, train precision:98.185, train recall:98.809, train auc:99.889
fold:0 epoch:98 step:8 train loss:0.042197, train acc:98.441, train f1:98.428, train precision:97.979, train recall:98.881, train auc:99.878
fold:0 epoch:98 step:9 train loss:0.050122, train acc:98.153, train f1:98.138, train precision:97.930, train recall:98.348, train auc:99.841
fold:0 epoch:98        valid loss:0.064316, valid acc:97.972, valid f1:97.981, valid precision:97.561, valid recall:98.404, valid auc:99.738
[1;31mTest score increased (97.936529 --> 97.971791).[0m
[97.97179051847982, 97.98052040935748, 97.56059664387818, 98.40407470288625, 99.73814818505907]
====================================================================================================
fold:0 epoch:99 step:0 train loss:0.040284, train acc:98.499, train f1:98.507, train precision:98.280, train recall:98.734, train auc:99.891
fold:0 epoch:99 step:1 train loss:0.042820, train acc:98.361, train f1:98.362, train precision:97.995, train recall:98.733, train auc:99.881
fold:0 epoch:99 step:2 train loss:0.041974, train acc:98.508, train f1:98.501, train precision:98.305, train recall:98.698, train auc:99.883
fold:0 epoch:99 step:3 train loss:0.045657, train acc:98.218, train f1:98.203, train precision:98.361, train recall:98.046, train auc:99.869
fold:0 epoch:99 step:4 train loss:0.042178, train acc:98.462, train f1:98.468, train precision:98.360, train recall:98.576, train auc:99.884
fold:0 epoch:99 step:5 train loss:0.043151, train acc:98.300, train f1:98.310, train precision:97.661, train recall:98.968, train auc:99.885
fold:0 epoch:99 step:6 train loss:0.044824, train acc:98.340, train f1:98.339, train precision:97.897, train recall:98.786, train auc:99.869
fold:0 epoch:99 step:7 train loss:0.041970, train acc:98.410, train f1:98.421, train precision:98.406, train recall:98.436, train auc:99.889
fold:0 epoch:99 step:8 train loss:0.041868, train acc:98.474, train f1:98.485, train precision:98.468, train recall:98.503, train auc:99.884
fold:0 epoch:99 step:9 train loss:0.044958, train acc:98.303, train f1:98.317, train precision:97.933, train recall:98.705, train auc:99.866
fold:0 epoch:99        valid loss:0.064643, valid acc:97.991, valid f1:98.000, valid precision:97.584, valid recall:98.420, valid auc:99.744
[1;31mTest score increased (97.971791 --> 97.991380).[0m
[97.99138043620216, 97.99994798304247, 97.58371532903426, 98.41974663706412, 99.74369353635394]
====================================================================================================
fold:0 epoch:100 step:0 train loss:0.041132, train acc:98.483, train f1:98.478, train precision:98.012, train recall:98.947, train auc:99.891
fold:0 epoch:100 step:1 train loss:0.042071, train acc:98.340, train f1:98.333, train precision:98.405, train recall:98.261, train auc:99.887
fold:0 epoch:100 step:2 train loss:0.042212, train acc:98.398, train f1:98.397, train precision:98.610, train recall:98.184, train auc:99.889
fold:0 epoch:100 step:3 train loss:0.040923, train acc:98.459, train f1:98.476, train precision:97.842, train recall:99.119, train auc:99.895
fold:0 epoch:100 step:4 train loss:0.044436, train acc:98.331, train f1:98.343, train precision:97.671, train recall:99.024, train auc:99.878
fold:0 epoch:100 step:5 train loss:0.039755, train acc:98.447, train f1:98.450, train precision:98.381, train recall:98.519, train auc:99.898
fold:0 epoch:100 step:6 train loss:0.042578, train acc:98.392, train f1:98.382, train precision:98.433, train recall:98.330, train auc:99.881
fold:0 epoch:100 step:7 train loss:0.042708, train acc:98.447, train f1:98.455, train precision:98.119, train recall:98.794, train auc:99.875
fold:0 epoch:100 step:8 train loss:0.044230, train acc:98.370, train f1:98.385, train precision:97.929, train recall:98.845, train auc:99.872
fold:0 epoch:100 step:9 train loss:0.043746, train acc:98.373, train f1:98.391, train precision:98.194, train recall:98.588, train auc:99.875
fold:0 epoch:100        valid loss:0.064602, valid acc:98.004, valid f1:98.015, valid precision:97.496, valid recall:98.540, valid auc:99.743
[1;31mTest score increased (97.991380 --> 98.004440).[0m
[98.0044403813504, 98.0150688490517, 97.49580049101951, 98.53989813242784, 99.74294794089592]
====================================================================================================
fold:0 epoch:101 step:0 train loss:0.039921, train acc:98.532, train f1:98.545, train precision:98.334, train recall:98.757, train auc:99.896
fold:0 epoch:101 step:1 train loss:0.040873, train acc:98.474, train f1:98.479, train precision:98.127, train recall:98.834, train auc:99.889
fold:0 epoch:101 step:2 train loss:0.043709, train acc:98.380, train f1:98.375, train precision:97.988, train recall:98.765, train auc:99.870
fold:0 epoch:101 step:3 train loss:0.042373, train acc:98.428, train f1:98.424, train precision:98.446, train recall:98.403, train auc:99.883
fold:0 epoch:101 step:4 train loss:0.040986, train acc:98.517, train f1:98.519, train precision:98.429, train recall:98.609, train auc:99.888
fold:0 epoch:101 step:5 train loss:0.041166, train acc:98.447, train f1:98.461, train precision:97.930, train recall:98.997, train auc:99.891
fold:0 epoch:101 step:6 train loss:0.039787, train acc:98.523, train f1:98.531, train precision:98.179, train recall:98.885, train auc:99.897
fold:0 epoch:101 step:7 train loss:0.041716, train acc:98.395, train f1:98.396, train precision:98.450, train recall:98.342, train auc:99.888
fold:0 epoch:101 step:8 train loss:0.042767, train acc:98.349, train f1:98.357, train precision:97.902, train recall:98.816, train auc:99.883
fold:0 epoch:101 step:9 train loss:0.043310, train acc:98.373, train f1:98.357, train precision:97.862, train recall:98.858, train auc:99.876
fold:0 epoch:101        valid loss:0.064277, valid acc:97.960, valid f1:97.975, valid precision:97.276, valid recall:98.684, valid auc:99.748
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.0044403813504, 98.0150688490517, 97.49580049101951, 98.53989813242784, 99.74294794089592]
====================================================================================================
fold:0 epoch:102 step:0 train loss:0.040170, train acc:98.508, train f1:98.496, train precision:98.486, train recall:98.505, train auc:99.891
fold:0 epoch:102 step:1 train loss:0.039704, train acc:98.505, train f1:98.501, train precision:98.598, train recall:98.405, train auc:99.901
fold:0 epoch:102 step:2 train loss:0.042175, train acc:98.441, train f1:98.448, train precision:98.099, train recall:98.799, train auc:99.886
fold:0 epoch:102 step:3 train loss:0.042292, train acc:98.395, train f1:98.408, train precision:97.704, train recall:99.122, train auc:99.886
fold:0 epoch:102 step:4 train loss:0.038154, train acc:98.599, train f1:98.597, train precision:98.324, train recall:98.872, train auc:99.904
fold:0 epoch:102 step:5 train loss:0.043169, train acc:98.355, train f1:98.345, train precision:98.748, train recall:97.944, train auc:99.888
fold:0 epoch:102 step:6 train loss:0.043087, train acc:98.410, train f1:98.422, train precision:98.034, train recall:98.814, train auc:99.875
fold:0 epoch:102 step:7 train loss:0.042854, train acc:98.355, train f1:98.370, train precision:97.799, train recall:98.947, train auc:99.882
fold:0 epoch:102 step:8 train loss:0.041204, train acc:98.370, train f1:98.376, train precision:98.167, train recall:98.586, train auc:99.891
fold:0 epoch:102 step:9 train loss:0.044350, train acc:98.320, train f1:98.340, train precision:98.623, train recall:98.059, train auc:99.877
fold:0 epoch:102        valid loss:0.064687, valid acc:98.049, valid f1:98.060, valid precision:97.520, valid recall:98.605, valid auc:99.751
[1;31mTest score increased (98.004440 --> 98.048844).[0m
[98.04884419485438, 98.05963946179023, 97.52008473043837, 98.60519785816899, 99.75133137842936]
====================================================================================================
fold:0 epoch:103 step:0 train loss:0.038565, train acc:98.569, train f1:98.550, train precision:98.206, train recall:98.895, train auc:99.901
fold:0 epoch:103 step:1 train loss:0.038243, train acc:98.605, train f1:98.614, train precision:98.301, train recall:98.929, train auc:99.905
fold:0 epoch:103 step:2 train loss:0.039897, train acc:98.459, train f1:98.464, train precision:97.931, train recall:99.003, train auc:99.898
fold:0 epoch:103 step:3 train loss:0.039738, train acc:98.618, train f1:98.626, train precision:98.527, train recall:98.725, train auc:99.897
fold:0 epoch:103 step:4 train loss:0.042063, train acc:98.474, train f1:98.471, train precision:98.399, train recall:98.544, train auc:99.885
fold:0 epoch:103 step:5 train loss:0.040766, train acc:98.456, train f1:98.454, train precision:97.982, train recall:98.932, train auc:99.894
fold:0 epoch:103 step:6 train loss:0.040680, train acc:98.471, train f1:98.478, train precision:98.403, train recall:98.553, train auc:99.891
fold:0 epoch:103 step:7 train loss:0.038893, train acc:98.596, train f1:98.613, train precision:98.459, train recall:98.768, train auc:99.900
fold:0 epoch:103 step:8 train loss:0.041127, train acc:98.471, train f1:98.485, train precision:98.084, train recall:98.888, train auc:99.887
fold:0 epoch:103 step:9 train loss:0.041092, train acc:98.461, train f1:98.464, train precision:98.094, train recall:98.837, train auc:99.893
fold:0 epoch:103        valid loss:0.064310, valid acc:97.989, valid f1:98.000, valid precision:97.434, valid recall:98.574, valid auc:99.747
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.04884419485438, 98.05963946179023, 97.52008473043837, 98.60519785816899, 99.75133137842936]
====================================================================================================
fold:0 epoch:104 step:0 train loss:0.038928, train acc:98.544, train f1:98.546, train precision:98.405, train recall:98.687, train auc:99.904
fold:0 epoch:104 step:1 train loss:0.041891, train acc:98.370, train f1:98.367, train precision:98.204, train recall:98.529, train auc:99.886
fold:0 epoch:104 step:2 train loss:0.041018, train acc:98.511, train f1:98.522, train precision:98.403, train recall:98.642, train auc:99.889
fold:0 epoch:104 step:3 train loss:0.040674, train acc:98.444, train f1:98.450, train precision:98.152, train recall:98.750, train auc:99.891
fold:0 epoch:104 step:4 train loss:0.039026, train acc:98.520, train f1:98.506, train precision:98.284, train recall:98.728, train auc:99.899
fold:0 epoch:104 step:5 train loss:0.040189, train acc:98.499, train f1:98.513, train precision:98.555, train recall:98.471, train auc:99.895
fold:0 epoch:104 step:6 train loss:0.039083, train acc:98.560, train f1:98.565, train precision:98.200, train recall:98.932, train auc:99.901
fold:0 epoch:104 step:7 train loss:0.037491, train acc:98.584, train f1:98.585, train precision:98.082, train recall:99.093, train auc:99.907
fold:0 epoch:104 step:8 train loss:0.040472, train acc:98.541, train f1:98.545, train precision:98.461, train recall:98.629, train auc:99.894
fold:0 epoch:104 step:9 train loss:0.040644, train acc:98.452, train f1:98.462, train precision:98.255, train recall:98.669, train auc:99.893
fold:0 epoch:104        valid loss:0.065142, valid acc:98.002, valid f1:98.013, valid precision:97.486, valid recall:98.545, valid auc:99.742
[1;31mEarlyStopping counter: 2 out of 50[0m
[98.04884419485438, 98.05963946179023, 97.52008473043837, 98.60519785816899, 99.75133137842936]
====================================================================================================
fold:0 epoch:105 step:0 train loss:0.040103, train acc:98.453, train f1:98.444, train precision:97.984, train recall:98.908, train auc:99.899
fold:0 epoch:105 step:1 train loss:0.040986, train acc:98.477, train f1:98.489, train precision:98.551, train recall:98.426, train auc:99.892
fold:0 epoch:105 step:2 train loss:0.037016, train acc:98.645, train f1:98.640, train precision:98.303, train recall:98.980, train auc:99.912
fold:0 epoch:105 step:3 train loss:0.038233, train acc:98.608, train f1:98.604, train precision:98.261, train recall:98.949, train auc:99.903
fold:0 epoch:105 step:4 train loss:0.040488, train acc:98.465, train f1:98.461, train precision:98.248, train recall:98.675, train auc:99.889
fold:0 epoch:105 step:5 train loss:0.038817, train acc:98.535, train f1:98.541, train precision:98.385, train recall:98.697, train auc:99.900
fold:0 epoch:105 step:6 train loss:0.037225, train acc:98.611, train f1:98.629, train precision:98.590, train recall:98.668, train auc:99.910
fold:0 epoch:105 step:7 train loss:0.043561, train acc:98.398, train f1:98.414, train precision:98.044, train recall:98.787, train auc:99.874
fold:0 epoch:105 step:8 train loss:0.039111, train acc:98.563, train f1:98.568, train precision:98.284, train recall:98.854, train auc:99.900
fold:0 epoch:105 step:9 train loss:0.041502, train acc:98.566, train f1:98.563, train precision:98.398, train recall:98.728, train auc:99.885
fold:0 epoch:105        valid loss:0.064285, valid acc:97.987, valid f1:98.000, valid precision:97.392, valid recall:98.616, valid auc:99.748
[1;31mEarlyStopping counter: 3 out of 50[0m
[98.04884419485438, 98.05963946179023, 97.52008473043837, 98.60519785816899, 99.75133137842936]
====================================================================================================
fold:0 epoch:106 step:0 train loss:0.036584, train acc:98.636, train f1:98.643, train precision:98.598, train recall:98.688, train auc:99.911
fold:0 epoch:106 step:1 train loss:0.038352, train acc:98.535, train f1:98.540, train precision:98.098, train recall:98.985, train auc:99.904
fold:0 epoch:106 step:2 train loss:0.038445, train acc:98.578, train f1:98.576, train precision:98.396, train recall:98.757, train auc:99.902
fold:0 epoch:106 step:3 train loss:0.038193, train acc:98.560, train f1:98.557, train precision:98.630, train recall:98.485, train auc:99.904
fold:0 epoch:106 step:4 train loss:0.038651, train acc:98.517, train f1:98.531, train precision:98.252, train recall:98.812, train auc:99.900
fold:0 epoch:106 step:5 train loss:0.040475, train acc:98.410, train f1:98.411, train precision:97.925, train recall:98.903, train auc:99.902
fold:0 epoch:106 step:6 train loss:0.039826, train acc:98.529, train f1:98.535, train precision:98.428, train recall:98.643, train auc:99.891
fold:0 epoch:106 step:7 train loss:0.040233, train acc:98.431, train f1:98.420, train precision:98.420, train recall:98.420, train auc:99.894
fold:0 epoch:106 step:8 train loss:0.039532, train acc:98.538, train f1:98.542, train precision:98.353, train recall:98.731, train auc:99.893
fold:0 epoch:106 step:9 train loss:0.041993, train acc:98.434, train f1:98.448, train precision:98.020, train recall:98.879, train auc:99.874
fold:0 epoch:106        valid loss:0.065558, valid acc:98.054, valid f1:98.064, valid precision:97.574, valid recall:98.558, valid auc:99.745
[1;31mTest score increased (98.048844 --> 98.054068).[0m
[98.05406817291367, 98.0638286813244, 97.57440976442295, 98.55818205563537, 99.74534570384345]
====================================================================================================
fold:0 epoch:107 step:0 train loss:0.037895, train acc:98.520, train f1:98.530, train precision:98.236, train recall:98.827, train auc:99.907
fold:0 epoch:107 step:1 train loss:0.041083, train acc:98.514, train f1:98.514, train precision:98.571, train recall:98.457, train auc:99.890
fold:0 epoch:107 step:2 train loss:0.040861, train acc:98.575, train f1:98.580, train precision:98.445, train recall:98.715, train auc:99.882
fold:0 epoch:107 step:3 train loss:0.039228, train acc:98.523, train f1:98.525, train precision:98.035, train recall:99.020, train auc:99.906
fold:0 epoch:107 step:4 train loss:0.036799, train acc:98.636, train f1:98.640, train precision:98.385, train recall:98.895, train auc:99.911
fold:0 epoch:107 step:5 train loss:0.040405, train acc:98.499, train f1:98.508, train precision:98.706, train recall:98.311, train auc:99.897
fold:0 epoch:107 step:6 train loss:0.039563, train acc:98.541, train f1:98.541, train precision:98.086, train recall:99.000, train auc:99.901
fold:0 epoch:107 step:7 train loss:0.041553, train acc:98.367, train f1:98.366, train precision:98.016, train recall:98.719, train auc:99.890
fold:0 epoch:107 step:8 train loss:0.039043, train acc:98.544, train f1:98.542, train precision:98.666, train recall:98.419, train auc:99.901
fold:0 epoch:107 step:9 train loss:0.040288, train acc:98.558, train f1:98.550, train precision:98.393, train recall:98.707, train auc:99.882
fold:0 epoch:107        valid loss:0.064331, valid acc:98.104, valid f1:98.111, valid precision:97.759, valid recall:98.464, valid auc:99.748
[1;31mTest score increased (98.054068 --> 98.103696).[0m
[98.10369596447694, 98.1105067277412, 97.75939420658179, 98.46415045056811, 99.74776270620347]
====================================================================================================
fold:0 epoch:108 step:0 train loss:0.037847, train acc:98.627, train f1:98.619, train precision:98.287, train recall:98.953, train auc:99.904
fold:0 epoch:108 step:1 train loss:0.036282, train acc:98.651, train f1:98.653, train precision:98.204, train recall:99.106, train auc:99.913
fold:0 epoch:108 step:2 train loss:0.037923, train acc:98.642, train f1:98.651, train precision:98.475, train recall:98.828, train auc:99.904
fold:0 epoch:108 step:3 train loss:0.035205, train acc:98.727, train f1:98.731, train precision:98.566, train recall:98.896, train auc:99.917
fold:0 epoch:108 step:4 train loss:0.037802, train acc:98.526, train f1:98.539, train precision:98.257, train recall:98.823, train auc:99.909
fold:0 epoch:108 step:5 train loss:0.038456, train acc:98.532, train f1:98.541, train precision:98.365, train recall:98.718, train auc:99.905
fold:0 epoch:108 step:6 train loss:0.042236, train acc:98.358, train f1:98.359, train precision:97.941, train recall:98.781, train auc:99.885
fold:0 epoch:108 step:7 train loss:0.038694, train acc:98.541, train f1:98.540, train precision:98.636, train recall:98.443, train auc:99.904
fold:0 epoch:108 step:8 train loss:0.038838, train acc:98.492, train f1:98.494, train precision:98.410, train recall:98.578, train auc:99.900
fold:0 epoch:108 step:9 train loss:0.035360, train acc:98.610, train f1:98.613, train precision:98.251, train recall:98.978, train auc:99.917
fold:0 epoch:108        valid loss:0.064154, valid acc:98.040, valid f1:98.052, valid precision:97.441, valid recall:98.670, valid auc:99.753
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.10369596447694, 98.1105067277412, 97.75939420658179, 98.46415045056811, 99.74776270620347]
====================================================================================================
fold:0 epoch:109 step:0 train loss:0.036453, train acc:98.654, train f1:98.664, train precision:98.310, train recall:99.021, train auc:99.910
fold:0 epoch:109 step:1 train loss:0.037374, train acc:98.596, train f1:98.602, train precision:98.464, train recall:98.740, train auc:99.910
fold:0 epoch:109 step:2 train loss:0.038302, train acc:98.621, train f1:98.633, train precision:98.478, train recall:98.788, train auc:99.900
fold:0 epoch:109 step:3 train loss:0.039240, train acc:98.532, train f1:98.528, train precision:98.428, train recall:98.627, train auc:99.903
fold:0 epoch:109 step:4 train loss:0.039270, train acc:98.495, train f1:98.502, train precision:98.136, train recall:98.872, train auc:99.902
fold:0 epoch:109 step:5 train loss:0.040294, train acc:98.441, train f1:98.441, train precision:98.240, train recall:98.643, train auc:99.896
fold:0 epoch:109 step:6 train loss:0.036846, train acc:98.590, train f1:98.587, train precision:98.496, train recall:98.677, train auc:99.910
fold:0 epoch:109 step:7 train loss:0.038502, train acc:98.535, train f1:98.546, train precision:98.421, train recall:98.671, train auc:99.903
fold:0 epoch:109 step:8 train loss:0.040614, train acc:98.495, train f1:98.488, train precision:97.909, train recall:99.075, train auc:99.894
fold:0 epoch:109 step:9 train loss:0.044548, train acc:98.347, train f1:98.349, train precision:97.937, train recall:98.766, train auc:99.869
fold:0 epoch:109        valid loss:0.065849, valid acc:98.007, valid f1:98.024, valid precision:97.193, valid recall:98.869, valid auc:99.754
[1;31mEarlyStopping counter: 2 out of 50[0m
[98.10369596447694, 98.1105067277412, 97.75939420658179, 98.46415045056811, 99.74776270620347]
====================================================================================================
fold:0 epoch:110 step:0 train loss:0.039573, train acc:98.547, train f1:98.542, train precision:98.875, train recall:98.212, train auc:99.902
fold:0 epoch:110 step:1 train loss:0.038330, train acc:98.538, train f1:98.547, train precision:98.365, train recall:98.729, train auc:99.901
fold:0 epoch:110 step:2 train loss:0.040465, train acc:98.514, train f1:98.520, train precision:97.798, train recall:99.253, train auc:99.905
fold:0 epoch:110 step:3 train loss:0.038784, train acc:98.566, train f1:98.571, train precision:98.427, train recall:98.715, train auc:99.897
fold:0 epoch:110 step:4 train loss:0.038023, train acc:98.575, train f1:98.565, train precision:98.659, train recall:98.471, train auc:99.905
fold:0 epoch:110 step:5 train loss:0.038728, train acc:98.557, train f1:98.563, train precision:98.578, train recall:98.548, train auc:99.904
fold:0 epoch:110 step:6 train loss:0.040781, train acc:98.410, train f1:98.427, train precision:97.940, train recall:98.920, train auc:99.897
fold:0 epoch:110 step:7 train loss:0.039337, train acc:98.544, train f1:98.548, train precision:98.055, train recall:99.045, train auc:99.902
fold:0 epoch:110 step:8 train loss:0.037548, train acc:98.575, train f1:98.568, train precision:98.408, train recall:98.728, train auc:99.907
fold:0 epoch:110 step:9 train loss:0.040577, train acc:98.426, train f1:98.424, train precision:99.026, train recall:97.830, train auc:99.909
fold:0 epoch:110        valid loss:0.062838, valid acc:98.095, valid f1:98.103, valid precision:97.690, valid recall:98.519, valid auc:99.755
[1;31mEarlyStopping counter: 3 out of 50[0m
[98.10369596447694, 98.1105067277412, 97.75939420658179, 98.46415045056811, 99.74776270620347]
====================================================================================================
fold:0 epoch:111 step:0 train loss:0.039291, train acc:98.535, train f1:98.544, train precision:98.021, train recall:99.073, train auc:99.900
fold:0 epoch:111 step:1 train loss:0.036090, train acc:98.642, train f1:98.650, train precision:97.993, train recall:99.316, train auc:99.924
fold:0 epoch:111 step:2 train loss:0.035513, train acc:98.672, train f1:98.683, train precision:98.555, train recall:98.812, train auc:99.918
fold:0 epoch:111 step:3 train loss:0.037614, train acc:98.572, train f1:98.580, train precision:98.886, train recall:98.276, train auc:99.912
fold:0 epoch:111 step:4 train loss:0.039666, train acc:98.511, train f1:98.515, train precision:98.037, train recall:98.997, train auc:99.901
fold:0 epoch:111 step:5 train loss:0.038297, train acc:98.593, train f1:98.594, train precision:98.148, train recall:99.044, train auc:99.905
fold:0 epoch:111 step:6 train loss:0.037032, train acc:98.581, train f1:98.573, train precision:98.449, train recall:98.697, train auc:99.908
fold:0 epoch:111 step:7 train loss:0.040463, train acc:98.535, train f1:98.529, train precision:98.844, train recall:98.215, train auc:99.894
fold:0 epoch:111 step:8 train loss:0.036419, train acc:98.651, train f1:98.652, train precision:98.513, train recall:98.790, train auc:99.915
fold:0 epoch:111 step:9 train loss:0.039693, train acc:98.452, train f1:98.459, train precision:98.013, train recall:98.910, train auc:99.908
fold:0 epoch:111        valid loss:0.063157, valid acc:98.096, valid f1:98.107, valid precision:97.513, valid recall:98.710, valid auc:99.758
[1;31mEarlyStopping counter: 4 out of 50[0m
[98.10369596447694, 98.1105067277412, 97.75939420658179, 98.46415045056811, 99.74776270620347]
====================================================================================================
fold:0 epoch:112 step:0 train loss:0.035344, train acc:98.639, train f1:98.642, train precision:98.260, train recall:99.028, train auc:99.920
fold:0 epoch:112 step:1 train loss:0.038880, train acc:98.538, train f1:98.528, train precision:98.561, train recall:98.495, train auc:99.902
fold:0 epoch:112 step:2 train loss:0.038444, train acc:98.492, train f1:98.491, train precision:98.454, train recall:98.527, train auc:99.906
fold:0 epoch:112 step:3 train loss:0.037173, train acc:98.615, train f1:98.617, train precision:98.347, train recall:98.888, train auc:99.911
fold:0 epoch:112 step:4 train loss:0.037279, train acc:98.578, train f1:98.591, train precision:98.104, train recall:99.082, train auc:99.915
fold:0 epoch:112 step:5 train loss:0.038416, train acc:98.630, train f1:98.633, train precision:98.433, train recall:98.835, train auc:99.898
fold:0 epoch:112 step:6 train loss:0.039203, train acc:98.502, train f1:98.498, train precision:98.737, train recall:98.260, train auc:99.904
fold:0 epoch:112 step:7 train loss:0.037380, train acc:98.599, train f1:98.600, train precision:98.706, train recall:98.495, train auc:99.909
fold:0 epoch:112 step:8 train loss:0.037098, train acc:98.602, train f1:98.615, train precision:98.193, train recall:99.040, train auc:99.910
fold:0 epoch:112 step:9 train loss:0.039597, train acc:98.452, train f1:98.462, train precision:97.677, train recall:99.260, train auc:99.919
fold:0 epoch:112        valid loss:0.064250, valid acc:98.100, valid f1:98.112, valid precision:97.474, valid recall:98.759, valid auc:99.759
[1;31mEarlyStopping counter: 5 out of 50[0m
[98.10369596447694, 98.1105067277412, 97.75939420658179, 98.46415045056811, 99.74776270620347]
====================================================================================================
fold:0 epoch:113 step:0 train loss:0.035434, train acc:98.767, train f1:98.757, train precision:98.220, train recall:99.301, train auc:99.915
fold:0 epoch:113 step:1 train loss:0.040788, train acc:98.499, train f1:98.498, train precision:98.969, train recall:98.031, train auc:99.906
fold:0 epoch:113 step:2 train loss:0.036815, train acc:98.581, train f1:98.592, train precision:98.804, train recall:98.380, train auc:99.917
fold:0 epoch:113 step:3 train loss:0.038861, train acc:98.608, train f1:98.626, train precision:97.923, train recall:99.338, train auc:99.906
fold:0 epoch:113 step:4 train loss:0.037015, train acc:98.627, train f1:98.631, train precision:98.023, train recall:99.247, train auc:99.911
fold:0 epoch:113 step:5 train loss:0.038301, train acc:98.517, train f1:98.519, train precision:98.766, train recall:98.273, train auc:99.909
fold:0 epoch:113 step:6 train loss:0.037662, train acc:98.560, train f1:98.554, train precision:98.827, train recall:98.283, train auc:99.914
fold:0 epoch:113 step:7 train loss:0.037044, train acc:98.660, train f1:98.669, train precision:98.344, train recall:98.996, train auc:99.903
fold:0 epoch:113 step:8 train loss:0.039440, train acc:98.404, train f1:98.405, train precision:97.693, train recall:99.127, train auc:99.910
fold:0 epoch:113 step:9 train loss:0.042380, train acc:98.417, train f1:98.407, train precision:98.233, train recall:98.582, train auc:99.887
fold:0 epoch:113        valid loss:0.065341, valid acc:98.068, valid f1:98.083, valid precision:97.345, valid recall:98.832, valid auc:99.756
[1;31mEarlyStopping counter: 6 out of 50[0m
[98.10369596447694, 98.1105067277412, 97.75939420658179, 98.46415045056811, 99.74776270620347]
====================================================================================================
fold:0 epoch:114 step:0 train loss:0.036728, train acc:98.630, train f1:98.641, train precision:98.889, train recall:98.393, train auc:99.914
fold:0 epoch:114 step:1 train loss:0.038026, train acc:98.572, train f1:98.573, train precision:98.585, train recall:98.561, train auc:99.907
fold:0 epoch:114 step:2 train loss:0.038205, train acc:98.578, train f1:98.575, train precision:98.114, train recall:99.042, train auc:99.904
fold:0 epoch:114 step:3 train loss:0.037315, train acc:98.618, train f1:98.628, train precision:98.079, train recall:99.184, train auc:99.911
fold:0 epoch:114 step:4 train loss:0.036740, train acc:98.627, train f1:98.620, train precision:98.535, train recall:98.705, train auc:99.910
fold:0 epoch:114 step:5 train loss:0.038534, train acc:98.499, train f1:98.496, train precision:98.695, train recall:98.297, train auc:99.910
fold:0 epoch:114 step:6 train loss:0.036776, train acc:98.578, train f1:98.577, train precision:98.403, train recall:98.752, train auc:99.912
fold:0 epoch:114 step:7 train loss:0.037288, train acc:98.645, train f1:98.651, train precision:98.109, train recall:99.200, train auc:99.913
fold:0 epoch:114 step:8 train loss:0.035263, train acc:98.642, train f1:98.648, train precision:98.222, train recall:99.079, train auc:99.921
fold:0 epoch:114 step:9 train loss:0.038829, train acc:98.549, train f1:98.553, train precision:98.613, train recall:98.492, train auc:99.895
fold:0 epoch:114        valid loss:0.065748, valid acc:98.007, valid f1:98.024, valid precision:97.174, valid recall:98.890, valid auc:99.752
[1;31mEarlyStopping counter: 7 out of 50[0m
[98.10369596447694, 98.1105067277412, 97.75939420658179, 98.46415045056811, 99.74776270620347]
====================================================================================================
fold:0 epoch:115 step:0 train loss:0.037850, train acc:98.575, train f1:98.564, train precision:98.495, train recall:98.634, train auc:99.903
fold:0 epoch:115 step:1 train loss:0.035035, train acc:98.737, train f1:98.733, train precision:98.612, train recall:98.854, train auc:99.918
fold:0 epoch:115 step:2 train loss:0.037023, train acc:98.575, train f1:98.586, train precision:98.422, train recall:98.750, train auc:99.910
fold:0 epoch:115 step:3 train loss:0.037105, train acc:98.526, train f1:98.532, train precision:98.011, train recall:99.059, train auc:99.917
fold:0 epoch:115 step:4 train loss:0.034627, train acc:98.691, train f1:98.702, train precision:98.717, train recall:98.687, train auc:99.922
fold:0 epoch:115 step:5 train loss:0.034361, train acc:98.740, train f1:98.742, train precision:98.643, train recall:98.841, train auc:99.919
fold:0 epoch:115 step:6 train loss:0.038016, train acc:98.593, train f1:98.597, train precision:98.678, train recall:98.516, train auc:99.907
fold:0 epoch:115 step:7 train loss:0.034520, train acc:98.715, train f1:98.721, train precision:98.330, train recall:99.116, train auc:99.922
fold:0 epoch:115 step:8 train loss:0.036327, train acc:98.618, train f1:98.611, train precision:98.109, train recall:99.119, train auc:99.917
fold:0 epoch:115 step:9 train loss:0.034018, train acc:98.725, train f1:98.733, train precision:98.742, train recall:98.725, train auc:99.924
fold:0 epoch:115        valid loss:0.063190, valid acc:98.132, valid f1:98.144, valid precision:97.556, valid recall:98.738, valid auc:99.766
[1;31mTest score increased (98.103696 --> 98.132428).[0m
[98.13242784380306, 98.1436768180284, 97.55606596299259, 98.73840929868093, 99.7656078752919]
====================================================================================================
fold:0 epoch:116 step:0 train loss:0.033826, train acc:98.721, train f1:98.723, train precision:98.877, train recall:98.570, train auc:99.930
fold:0 epoch:116 step:1 train loss:0.032626, train acc:98.792, train f1:98.782, train precision:98.491, train recall:99.075, train auc:99.928
fold:0 epoch:116 step:2 train loss:0.033354, train acc:98.749, train f1:98.751, train precision:98.457, train recall:99.047, train auc:99.931
fold:0 epoch:116 step:3 train loss:0.034624, train acc:98.688, train f1:98.704, train precision:98.313, train recall:99.098, train auc:99.923
fold:0 epoch:116 step:4 train loss:0.034766, train acc:98.755, train f1:98.747, train precision:98.644, train recall:98.850, train auc:99.921
fold:0 epoch:116 step:5 train loss:0.033852, train acc:98.743, train f1:98.739, train precision:98.921, train recall:98.558, train auc:99.926
fold:0 epoch:116 step:6 train loss:0.035665, train acc:98.630, train f1:98.637, train precision:98.466, train recall:98.808, train auc:99.918
fold:0 epoch:116 step:7 train loss:0.035696, train acc:98.663, train f1:98.663, train precision:98.183, train recall:99.147, train auc:99.921
fold:0 epoch:116 step:8 train loss:0.037795, train acc:98.553, train f1:98.569, train precision:98.439, train recall:98.701, train auc:99.902
fold:0 epoch:116 step:9 train loss:0.035705, train acc:98.821, train f1:98.825, train precision:98.704, train recall:98.947, train auc:99.916
fold:0 epoch:116        valid loss:0.063135, valid acc:98.121, valid f1:98.132, valid precision:97.531, valid recall:98.741, valid auc:99.765
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.13242784380306, 98.1436768180284, 97.55606596299259, 98.73840929868093, 99.7656078752919]
====================================================================================================
fold:0 epoch:117 step:0 train loss:0.035737, train acc:98.636, train f1:98.638, train precision:98.678, train recall:98.599, train auc:99.919
fold:0 epoch:117 step:1 train loss:0.033459, train acc:98.795, train f1:98.798, train precision:98.418, train recall:99.181, train auc:99.927
fold:0 epoch:117 step:2 train loss:0.033821, train acc:98.734, train f1:98.746, train precision:98.440, train recall:99.055, train auc:99.926
fold:0 epoch:117 step:3 train loss:0.034695, train acc:98.666, train f1:98.671, train precision:98.513, train recall:98.831, train auc:99.921
fold:0 epoch:117 step:4 train loss:0.034231, train acc:98.679, train f1:98.685, train precision:98.838, train recall:98.532, train auc:99.924
fold:0 epoch:117 step:5 train loss:0.034348, train acc:98.746, train f1:98.742, train precision:98.600, train recall:98.884, train auc:99.919
fold:0 epoch:117 step:6 train loss:0.033821, train acc:98.798, train f1:98.795, train precision:98.644, train recall:98.946, train auc:99.922
fold:0 epoch:117 step:7 train loss:0.036177, train acc:98.672, train f1:98.668, train precision:98.202, train recall:99.139, train auc:99.914
fold:0 epoch:117 step:8 train loss:0.035746, train acc:98.666, train f1:98.668, train precision:98.401, train recall:98.936, train auc:99.916
fold:0 epoch:117 step:9 train loss:0.038510, train acc:98.566, train f1:98.572, train precision:98.477, train recall:98.667, train auc:99.902
fold:0 epoch:117        valid loss:0.063184, valid acc:98.135, valid f1:98.147, valid precision:97.524, valid recall:98.778, valid auc:99.768
[1;31mTest score increased (98.132428 --> 98.135040).[0m
[98.1350398328327, 98.14694661441436, 97.52430564509889, 98.77758913412563, 99.76756210837966]
====================================================================================================
fold:0 epoch:118 step:0 train loss:0.032936, train acc:98.730, train f1:98.726, train precision:98.557, train recall:98.895, train auc:99.923
fold:0 epoch:118 step:1 train loss:0.033847, train acc:98.746, train f1:98.744, train precision:98.549, train recall:98.941, train auc:99.923
fold:0 epoch:118 step:2 train loss:0.035554, train acc:98.636, train f1:98.645, train precision:98.427, train recall:98.864, train auc:99.916
fold:0 epoch:118 step:3 train loss:0.032981, train acc:98.724, train f1:98.729, train precision:98.465, train recall:98.994, train auc:99.928
fold:0 epoch:118 step:4 train loss:0.032840, train acc:98.749, train f1:98.753, train precision:98.579, train recall:98.928, train auc:99.928
fold:0 epoch:118 step:5 train loss:0.034682, train acc:98.657, train f1:98.670, train precision:98.843, train recall:98.497, train auc:99.925
fold:0 epoch:118 step:6 train loss:0.038710, train acc:98.550, train f1:98.560, train precision:98.040, train recall:99.086, train auc:99.903
fold:0 epoch:118 step:7 train loss:0.036765, train acc:98.669, train f1:98.674, train precision:98.524, train recall:98.824, train auc:99.909
fold:0 epoch:118 step:8 train loss:0.033038, train acc:98.666, train f1:98.654, train precision:98.433, train recall:98.877, train auc:99.934
fold:0 epoch:118 step:9 train loss:0.039075, train acc:98.487, train f1:98.475, train precision:98.808, train recall:98.145, train auc:99.909
fold:0 epoch:118        valid loss:0.061404, valid acc:98.173, valid f1:98.183, valid precision:97.656, valid recall:98.715, valid auc:99.776
[1;31mTest score increased (98.135040 --> 98.172914).[0m
[98.17291367376258, 98.1827628758849, 97.656330749354, 98.71490139741414, 99.77597484849346]
====================================================================================================
fold:0 epoch:119 step:0 train loss:0.033247, train acc:98.749, train f1:98.756, train precision:98.660, train recall:98.852, train auc:99.928
fold:0 epoch:119 step:1 train loss:0.033904, train acc:98.688, train f1:98.691, train precision:98.362, train recall:99.023, train auc:99.927
fold:0 epoch:119 step:2 train loss:0.033132, train acc:98.703, train f1:98.707, train precision:98.458, train recall:98.957, train auc:99.929
fold:0 epoch:119 step:3 train loss:0.033054, train acc:98.785, train f1:98.796, train precision:98.891, train recall:98.700, train auc:99.930
fold:0 epoch:119 step:4 train loss:0.031566, train acc:98.801, train f1:98.806, train precision:98.498, train recall:99.116, train auc:99.934
fold:0 epoch:119 step:5 train loss:0.032249, train acc:98.862, train f1:98.857, train precision:98.648, train recall:99.066, train auc:99.928
fold:0 epoch:119 step:6 train loss:0.032031, train acc:98.834, train f1:98.838, train precision:98.550, train recall:99.127, train auc:99.932
fold:0 epoch:119 step:7 train loss:0.036941, train acc:98.630, train f1:98.624, train precision:98.525, train recall:98.724, train auc:99.912
fold:0 epoch:119 step:8 train loss:0.034053, train acc:98.700, train f1:98.699, train precision:98.868, train recall:98.531, train auc:99.926
fold:0 epoch:119 step:9 train loss:0.037474, train acc:98.610, train f1:98.594, train precision:98.175, train recall:99.017, train auc:99.905
fold:0 epoch:119        valid loss:0.061797, valid acc:98.203, valid f1:98.212, valid precision:97.734, valid recall:98.694, valid auc:99.773
[1;31mTest score increased (98.172914 --> 98.202952).[0m
[98.2029515476035, 98.21173290359472, 97.73415069449834, 98.69400548517696, 99.77254924381349]
====================================================================================================
fold:0 epoch:120 step:0 train loss:0.032659, train acc:98.761, train f1:98.767, train precision:98.641, train recall:98.893, train auc:99.930
fold:0 epoch:120 step:1 train loss:0.033531, train acc:98.755, train f1:98.754, train precision:98.364, train recall:99.148, train auc:99.923
fold:0 epoch:120 step:2 train loss:0.035505, train acc:98.648, train f1:98.644, train precision:98.509, train recall:98.780, train auc:99.915
fold:0 epoch:120 step:3 train loss:0.033035, train acc:98.764, train f1:98.760, train precision:98.739, train recall:98.781, train auc:99.929
fold:0 epoch:120 step:4 train loss:0.034221, train acc:98.697, train f1:98.691, train precision:98.525, train recall:98.858, train auc:99.922
fold:0 epoch:120 step:5 train loss:0.033037, train acc:98.798, train f1:98.811, train precision:98.698, train recall:98.925, train auc:99.928
fold:0 epoch:120 step:6 train loss:0.033762, train acc:98.755, train f1:98.755, train precision:98.478, train recall:99.033, train auc:99.923
fold:0 epoch:120 step:7 train loss:0.032602, train acc:98.813, train f1:98.822, train precision:98.783, train recall:98.861, train auc:99.931
fold:0 epoch:120 step:8 train loss:0.034714, train acc:98.703, train f1:98.706, train precision:98.463, train recall:98.950, train auc:99.920
fold:0 epoch:120 step:9 train loss:0.031799, train acc:98.821, train f1:98.824, train precision:98.565, train recall:99.085, train auc:99.935
fold:0 epoch:120        valid loss:0.061829, valid acc:98.208, valid f1:98.218, valid precision:97.678, valid recall:98.765, valid auc:99.777
[1;31mTest score increased (98.202952 --> 98.208176).[0m
[98.2081755256628, 98.21808925138969, 97.67766267985843, 98.7645291889774, 99.77711242992432]
====================================================================================================
fold:0 epoch:121 step:0 train loss:0.032619, train acc:98.730, train f1:98.728, train precision:98.547, train recall:98.910, train auc:99.932
fold:0 epoch:121 step:1 train loss:0.033395, train acc:98.752, train f1:98.751, train precision:98.585, train recall:98.917, train auc:99.926
fold:0 epoch:121 step:2 train loss:0.031538, train acc:98.846, train f1:98.854, train precision:98.765, train recall:98.944, train auc:99.935
fold:0 epoch:121 step:3 train loss:0.033203, train acc:98.743, train f1:98.742, train precision:98.429, train recall:99.056, train auc:99.929
fold:0 epoch:121 step:4 train loss:0.033110, train acc:98.709, train f1:98.711, train precision:98.672, train recall:98.750, train auc:99.932
fold:0 epoch:121 step:5 train loss:0.034327, train acc:98.721, train f1:98.727, train precision:98.796, train recall:98.658, train auc:99.927
fold:0 epoch:121 step:6 train loss:0.032292, train acc:98.749, train f1:98.755, train precision:98.479, train recall:99.031, train auc:99.932
fold:0 epoch:121 step:7 train loss:0.033596, train acc:98.715, train f1:98.723, train precision:98.416, train recall:99.033, train auc:99.926
fold:0 epoch:121 step:8 train loss:0.033737, train acc:98.706, train f1:98.703, train precision:98.516, train recall:98.890, train auc:99.924
fold:0 epoch:121 step:9 train loss:0.035594, train acc:98.663, train f1:98.653, train precision:98.740, train recall:98.566, train auc:99.916
fold:0 epoch:121        valid loss:0.062374, valid acc:98.221, valid f1:98.228, valid precision:97.861, valid recall:98.597, valid auc:99.772
[1;31mTest score increased (98.208176 --> 98.221235).[0m
[98.22123547081102, 98.22790080408025, 97.86119824747881, 98.59736189108006, 99.77185850114844]
====================================================================================================
fold:0 epoch:122 step:0 train loss:0.031770, train acc:98.807, train f1:98.815, train precision:98.680, train recall:98.950, train auc:99.932
fold:0 epoch:122 step:1 train loss:0.034191, train acc:98.740, train f1:98.741, train precision:98.336, train recall:99.149, train auc:99.922
fold:0 epoch:122 step:2 train loss:0.032458, train acc:98.773, train f1:98.775, train precision:98.848, train recall:98.703, train auc:99.932
fold:0 epoch:122 step:3 train loss:0.031570, train acc:98.871, train f1:98.865, train precision:98.901, train recall:98.828, train auc:99.934
fold:0 epoch:122 step:4 train loss:0.031921, train acc:98.813, train f1:98.816, train precision:98.680, train recall:98.951, train auc:99.933
fold:0 epoch:122 step:5 train loss:0.033146, train acc:98.788, train f1:98.798, train precision:98.640, train recall:98.957, train auc:99.927
fold:0 epoch:122 step:6 train loss:0.031939, train acc:98.837, train f1:98.837, train precision:98.335, train recall:99.343, train auc:99.934
fold:0 epoch:122 step:7 train loss:0.030442, train acc:98.926, train f1:98.918, train precision:98.796, train recall:99.039, train auc:99.938
fold:0 epoch:122 step:8 train loss:0.030234, train acc:98.834, train f1:98.844, train precision:98.886, train recall:98.802, train auc:99.941
fold:0 epoch:122 step:9 train loss:0.037285, train acc:98.496, train f1:98.489, train precision:98.394, train recall:98.585, train auc:99.909
fold:0 epoch:122        valid loss:0.061011, valid acc:98.195, valid f1:98.201, valid precision:97.900, valid recall:98.503, valid auc:99.774
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.22123547081102, 98.22790080408025, 97.86119824747881, 98.59736189108006, 99.77185850114844]
====================================================================================================
fold:0 epoch:123 step:0 train loss:0.032164, train acc:98.767, train f1:98.769, train precision:98.607, train recall:98.932, train auc:99.934
fold:0 epoch:123 step:1 train loss:0.031161, train acc:98.837, train f1:98.838, train precision:98.535, train recall:99.144, train auc:99.938
fold:0 epoch:123 step:2 train loss:0.033201, train acc:98.737, train f1:98.741, train precision:98.717, train recall:98.765, train auc:99.928
fold:0 epoch:123 step:3 train loss:0.032295, train acc:98.752, train f1:98.756, train precision:98.789, train recall:98.723, train auc:99.937
fold:0 epoch:123 step:4 train loss:0.032000, train acc:98.795, train f1:98.803, train precision:98.728, train recall:98.878, train auc:99.932
fold:0 epoch:123 step:5 train loss:0.032799, train acc:98.755, train f1:98.751, train precision:98.155, train recall:99.353, train auc:99.938
fold:0 epoch:123 step:6 train loss:0.030772, train acc:98.840, train f1:98.837, train precision:98.614, train recall:99.062, train auc:99.937
fold:0 epoch:123 step:7 train loss:0.035801, train acc:98.621, train f1:98.631, train precision:98.918, train recall:98.345, train auc:99.922
fold:0 epoch:123 step:8 train loss:0.032944, train acc:98.807, train f1:98.805, train precision:98.537, train recall:99.074, train auc:99.930
fold:0 epoch:123 step:9 train loss:0.033773, train acc:98.786, train f1:98.779, train precision:98.448, train recall:99.112, train auc:99.914
fold:0 epoch:123        valid loss:0.061008, valid acc:98.256, valid f1:98.263, valid precision:97.888, valid recall:98.642, valid auc:99.777
[1;31mTest score increased (98.221235 --> 98.256497).[0m
[98.25649732271124, 98.26318870747414, 97.88750648004148, 98.64176570458405, 99.77707033518143]
====================================================================================================
fold:0 epoch:124 step:0 train loss:0.031754, train acc:98.834, train f1:98.837, train precision:98.644, train recall:99.030, train auc:99.929
fold:0 epoch:124 step:1 train loss:0.031324, train acc:98.862, train f1:98.861, train precision:98.948, train recall:98.773, train auc:99.938
fold:0 epoch:124 step:2 train loss:0.032412, train acc:98.801, train f1:98.804, train precision:98.723, train recall:98.885, train auc:99.930
fold:0 epoch:124 step:3 train loss:0.030289, train acc:98.868, train f1:98.867, train precision:98.888, train recall:98.846, train auc:99.941
fold:0 epoch:124 step:4 train loss:0.032073, train acc:98.822, train f1:98.836, train precision:98.830, train recall:98.842, train auc:99.934
fold:0 epoch:124 step:5 train loss:0.033698, train acc:98.706, train f1:98.707, train precision:98.305, train recall:99.112, train auc:99.931
fold:0 epoch:124 step:6 train loss:0.033418, train acc:98.773, train f1:98.774, train precision:98.419, train recall:99.130, train auc:99.925
fold:0 epoch:124 step:7 train loss:0.031200, train acc:98.785, train f1:98.783, train precision:98.868, train recall:98.699, train auc:99.934
fold:0 epoch:124 step:8 train loss:0.032568, train acc:98.810, train f1:98.808, train precision:98.784, train recall:98.832, train auc:99.930
fold:0 epoch:124 step:9 train loss:0.032438, train acc:98.848, train f1:98.840, train precision:98.413, train recall:99.271, train auc:99.930
fold:0 epoch:124        valid loss:0.060636, valid acc:98.284, valid f1:98.291, valid precision:97.891, valid recall:98.694, valid auc:99.782
[1;31mTest score increased (98.256497 --> 98.283923).[0m
[98.28392320752253, 98.29093179335102, 97.89113707609005, 98.69400548517696, 99.78192003162086]
====================================================================================================
fold:0 epoch:125 step:0 train loss:0.030151, train acc:98.813, train f1:98.814, train precision:98.720, train recall:98.907, train auc:99.943
fold:0 epoch:125 step:1 train loss:0.029556, train acc:98.901, train f1:98.903, train precision:99.072, train recall:98.735, train auc:99.947
fold:0 epoch:125 step:2 train loss:0.032341, train acc:98.804, train f1:98.808, train precision:98.706, train recall:98.910, train auc:99.930
fold:0 epoch:125 step:3 train loss:0.033474, train acc:98.730, train f1:98.736, train precision:98.300, train recall:99.176, train auc:99.928
fold:0 epoch:125 step:4 train loss:0.031778, train acc:98.773, train f1:98.776, train precision:98.589, train recall:98.963, train auc:99.931
fold:0 epoch:125 step:5 train loss:0.033276, train acc:98.743, train f1:98.745, train precision:98.727, train recall:98.763, train auc:99.931
fold:0 epoch:125 step:6 train loss:0.031945, train acc:98.810, train f1:98.816, train precision:98.588, train recall:99.044, train auc:99.932
fold:0 epoch:125 step:7 train loss:0.031447, train acc:98.831, train f1:98.836, train precision:98.558, train recall:99.117, train auc:99.934
fold:0 epoch:125 step:8 train loss:0.032198, train acc:98.773, train f1:98.766, train precision:98.645, train recall:98.887, train auc:99.932
fold:0 epoch:125 step:9 train loss:0.031523, train acc:98.821, train f1:98.802, train precision:98.872, train recall:98.731, train auc:99.940
fold:0 epoch:125        valid loss:0.060824, valid acc:98.260, valid f1:98.269, valid precision:97.811, valid recall:98.731, valid auc:99.781
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.28392320752253, 98.29093179335102, 97.89113707609005, 98.69400548517696, 99.78192003162086]
====================================================================================================
fold:0 epoch:126 step:0 train loss:0.033406, train acc:98.761, train f1:98.761, train precision:98.809, train recall:98.713, train auc:99.926
fold:0 epoch:126 step:1 train loss:0.033660, train acc:98.755, train f1:98.761, train precision:98.415, train recall:99.110, train auc:99.928
fold:0 epoch:126 step:2 train loss:0.032879, train acc:98.764, train f1:98.776, train precision:98.363, train recall:99.193, train auc:99.933
fold:0 epoch:126 step:3 train loss:0.031458, train acc:98.828, train f1:98.837, train precision:98.807, train recall:98.867, train auc:99.936
fold:0 epoch:126 step:4 train loss:0.030508, train acc:98.837, train f1:98.836, train precision:98.852, train recall:98.821, train auc:99.940
fold:0 epoch:126 step:5 train loss:0.028893, train acc:98.914, train f1:98.913, train precision:98.817, train recall:99.010, train auc:99.944
fold:0 epoch:126 step:6 train loss:0.031978, train acc:98.758, train f1:98.759, train precision:98.437, train recall:99.082, train auc:99.938
fold:0 epoch:126 step:7 train loss:0.033731, train acc:98.730, train f1:98.725, train precision:98.581, train recall:98.871, train auc:99.922
fold:0 epoch:126 step:8 train loss:0.034417, train acc:98.654, train f1:98.651, train precision:98.569, train recall:98.732, train auc:99.923
fold:0 epoch:126 step:9 train loss:0.030323, train acc:98.865, train f1:98.859, train precision:98.955, train recall:98.763, train auc:99.941
fold:0 epoch:126        valid loss:0.061087, valid acc:98.246, valid f1:98.255, valid precision:97.748, valid recall:98.767, valid auc:99.784
[1;31mEarlyStopping counter: 2 out of 50[0m
[98.28392320752253, 98.29093179335102, 97.89113707609005, 98.69400548517696, 99.78192003162086]
====================================================================================================
fold:0 epoch:127 step:0 train loss:0.032348, train acc:98.740, train f1:98.746, train precision:98.570, train recall:98.923, train auc:99.931
fold:0 epoch:127 step:1 train loss:0.031127, train acc:98.798, train f1:98.799, train precision:98.320, train recall:99.283, train auc:99.939
fold:0 epoch:127 step:2 train loss:0.028846, train acc:98.920, train f1:98.925, train precision:98.895, train recall:98.955, train auc:99.947
fold:0 epoch:127 step:3 train loss:0.033031, train acc:98.819, train f1:98.826, train precision:98.997, train recall:98.656, train auc:99.933
fold:0 epoch:127 step:4 train loss:0.029089, train acc:98.956, train f1:98.953, train precision:98.766, train recall:99.141, train auc:99.944
fold:0 epoch:127 step:5 train loss:0.030891, train acc:98.892, train f1:98.895, train precision:98.538, train recall:99.254, train auc:99.939
fold:0 epoch:127 step:6 train loss:0.028703, train acc:98.932, train f1:98.931, train precision:98.720, train recall:99.143, train auc:99.946
fold:0 epoch:127 step:7 train loss:0.030765, train acc:98.880, train f1:98.866, train precision:98.942, train recall:98.789, train auc:99.934
fold:0 epoch:127 step:8 train loss:0.030090, train acc:98.892, train f1:98.895, train precision:99.001, train recall:98.790, train auc:99.944
fold:0 epoch:127 step:9 train loss:0.033335, train acc:98.742, train f1:98.757, train precision:98.852, train recall:98.663, train auc:99.926
fold:0 epoch:127        valid loss:0.059856, valid acc:98.318, valid f1:98.323, valid precision:98.004, valid recall:98.644, valid auc:99.789
[1;31mTest score increased (98.283923 --> 98.317879).[0m
[98.31787906490793, 98.32335329341318, 98.00441157389386, 98.64437769361368, 99.78861947476439]
====================================================================================================
fold:0 epoch:128 step:0 train loss:0.032723, train acc:98.792, train f1:98.788, train precision:98.115, train recall:99.470, train auc:99.941
fold:0 epoch:128 step:1 train loss:0.028632, train acc:98.926, train f1:98.930, train precision:98.576, train recall:99.286, train auc:99.947
fold:0 epoch:128 step:2 train loss:0.030898, train acc:98.859, train f1:98.844, train precision:98.966, train recall:98.722, train auc:99.940
fold:0 epoch:128 step:3 train loss:0.030918, train acc:98.856, train f1:98.847, train precision:99.118, train recall:98.577, train auc:99.938
fold:0 epoch:128 step:4 train loss:0.031508, train acc:98.834, train f1:98.838, train precision:98.467, train recall:99.213, train auc:99.936
fold:0 epoch:128 step:5 train loss:0.032629, train acc:98.737, train f1:98.757, train precision:98.379, train recall:99.138, train auc:99.934
fold:0 epoch:128 step:6 train loss:0.031478, train acc:98.840, train f1:98.853, train precision:98.680, train recall:99.026, train auc:99.932
fold:0 epoch:128 step:7 train loss:0.030949, train acc:98.862, train f1:98.859, train precision:99.056, train recall:98.663, train auc:99.935
fold:0 epoch:128 step:8 train loss:0.031625, train acc:98.828, train f1:98.827, train precision:98.906, train recall:98.749, train auc:99.937
fold:0 epoch:128 step:9 train loss:0.032829, train acc:98.716, train f1:98.737, train precision:98.550, train recall:98.925, train auc:99.923
fold:0 epoch:128        valid loss:0.060257, valid acc:98.319, valid f1:98.324, valid precision:98.034, valid recall:98.616, valid auc:99.790
[1;31mTest score increased (98.317879 --> 98.319185).[0m
[98.31918505942275, 98.32415328723778, 98.03437889488991, 98.61564581428757, 99.79015371557146]
====================================================================================================
fold:0 epoch:129 step:0 train loss:0.032253, train acc:98.770, train f1:98.781, train precision:98.213, train recall:99.355, train auc:99.939
fold:0 epoch:129 step:1 train loss:0.026967, train acc:98.996, train f1:98.999, train precision:98.768, train recall:99.232, train auc:99.955
fold:0 epoch:129 step:2 train loss:0.032284, train acc:98.792, train f1:98.779, train precision:98.907, train recall:98.652, train auc:99.934
fold:0 epoch:129 step:3 train loss:0.032905, train acc:98.773, train f1:98.770, train precision:98.939, train recall:98.601, train auc:99.929
fold:0 epoch:129 step:4 train loss:0.031732, train acc:98.801, train f1:98.795, train precision:98.569, train recall:99.023, train auc:99.933
fold:0 epoch:129 step:5 train loss:0.032304, train acc:98.785, train f1:98.787, train precision:98.247, train recall:99.332, train auc:99.937
fold:0 epoch:129 step:6 train loss:0.031871, train acc:98.740, train f1:98.748, train precision:98.673, train recall:98.823, train auc:99.932
fold:0 epoch:129 step:7 train loss:0.033063, train acc:98.743, train f1:98.746, train precision:98.758, train recall:98.734, train auc:99.928
fold:0 epoch:129 step:8 train loss:0.032639, train acc:98.752, train f1:98.758, train precision:98.635, train recall:98.881, train auc:99.928
fold:0 epoch:129 step:9 train loss:0.030069, train acc:98.962, train f1:98.977, train precision:98.687, train recall:99.270, train auc:99.936
fold:0 epoch:129        valid loss:0.061331, valid acc:98.267, valid f1:98.275, valid precision:97.806, valid recall:98.749, valid auc:99.783
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.31918505942275, 98.32415328723778, 98.03437889488991, 98.61564581428757, 99.79015371557146]
====================================================================================================
fold:0 epoch:130 step:0 train loss:0.031792, train acc:98.807, train f1:98.800, train precision:98.362, train recall:99.242, train auc:99.938
fold:0 epoch:130 step:1 train loss:0.030769, train acc:98.846, train f1:98.855, train precision:98.927, train recall:98.783, train auc:99.939
fold:0 epoch:130 step:2 train loss:0.028930, train acc:98.920, train f1:98.914, train precision:98.805, train recall:99.024, train auc:99.946
fold:0 epoch:130 step:3 train loss:0.029535, train acc:98.904, train f1:98.899, train precision:98.878, train recall:98.920, train auc:99.943
fold:0 epoch:130 step:4 train loss:0.030293, train acc:98.834, train f1:98.840, train precision:98.540, train recall:99.141, train auc:99.939
fold:0 epoch:130 step:5 train loss:0.030187, train acc:98.782, train f1:98.788, train precision:98.588, train recall:98.990, train auc:99.941
fold:0 epoch:130 step:6 train loss:0.029410, train acc:98.898, train f1:98.908, train precision:98.941, train recall:98.875, train auc:99.943
fold:0 epoch:130 step:7 train loss:0.031262, train acc:98.798, train f1:98.799, train precision:98.727, train recall:98.872, train auc:99.935
fold:0 epoch:130 step:8 train loss:0.031999, train acc:98.743, train f1:98.745, train precision:98.404, train recall:99.089, train auc:99.930
fold:0 epoch:130 step:9 train loss:0.033735, train acc:98.734, train f1:98.739, train precision:98.583, train recall:98.895, train auc:99.915
fold:0 epoch:130        valid loss:0.060881, valid acc:98.330, valid f1:98.336, valid precision:97.945, valid recall:98.731, valid auc:99.786
[1;31mTest score increased (98.319185 --> 98.329633).[0m
[98.32963301554133, 98.33630344576402, 97.94516998341625, 98.73057333159201, 99.78608393169812]
====================================================================================================
fold:0 epoch:131 step:0 train loss:0.029408, train acc:98.859, train f1:98.862, train precision:98.664, train recall:99.061, train auc:99.945
fold:0 epoch:131 step:1 train loss:0.029442, train acc:98.935, train f1:98.933, train precision:98.888, train recall:98.979, train auc:99.943
fold:0 epoch:131 step:2 train loss:0.028849, train acc:98.987, train f1:98.994, train precision:98.892, train recall:99.096, train auc:99.941
fold:0 epoch:131 step:3 train loss:0.027347, train acc:98.975, train f1:98.980, train precision:98.836, train recall:99.125, train auc:99.951
fold:0 epoch:131 step:4 train loss:0.028985, train acc:98.920, train f1:98.918, train precision:98.725, train recall:99.112, train auc:99.943
fold:0 epoch:131 step:5 train loss:0.030077, train acc:98.837, train f1:98.837, train precision:98.684, train recall:98.991, train auc:99.942
fold:0 epoch:131 step:6 train loss:0.029703, train acc:98.932, train f1:98.936, train precision:98.726, train recall:99.147, train auc:99.942
fold:0 epoch:131 step:7 train loss:0.029216, train acc:98.874, train f1:98.868, train precision:98.865, train recall:98.871, train auc:99.947
fold:0 epoch:131 step:8 train loss:0.033585, train acc:98.740, train f1:98.744, train precision:98.723, train recall:98.765, train auc:99.927
fold:0 epoch:131 step:9 train loss:0.027909, train acc:98.962, train f1:98.952, train precision:98.654, train recall:99.251, train auc:99.951
fold:0 epoch:131        valid loss:0.063526, valid acc:98.195, valid f1:98.206, valid precision:97.598, valid recall:98.822, valid auc:99.774
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.32963301554133, 98.33630344576402, 97.94516998341625, 98.73057333159201, 99.78608393169812]
====================================================================================================
fold:0 epoch:132 step:0 train loss:0.030451, train acc:98.773, train f1:98.774, train precision:98.847, train recall:98.702, train auc:99.940
fold:0 epoch:132 step:1 train loss:0.028086, train acc:98.941, train f1:98.935, train precision:98.702, train recall:99.169, train auc:99.950
fold:0 epoch:132 step:2 train loss:0.028600, train acc:98.877, train f1:98.875, train precision:98.712, train recall:99.039, train auc:99.949
fold:0 epoch:132 step:3 train loss:0.029195, train acc:98.932, train f1:98.935, train precision:98.695, train recall:99.177, train auc:99.946
fold:0 epoch:132 step:4 train loss:0.028588, train acc:98.978, train f1:98.979, train precision:98.837, train recall:99.121, train auc:99.946
fold:0 epoch:132 step:5 train loss:0.033156, train acc:98.706, train f1:98.707, train precision:98.599, train recall:98.815, train auc:99.928
fold:0 epoch:132 step:6 train loss:0.028226, train acc:98.920, train f1:98.912, train precision:98.961, train recall:98.864, train auc:99.948
fold:0 epoch:132 step:7 train loss:0.029855, train acc:98.901, train f1:98.907, train precision:98.703, train recall:99.112, train auc:99.941
fold:0 epoch:132 step:8 train loss:0.031382, train acc:98.914, train f1:98.933, train precision:98.784, train recall:99.081, train auc:99.929
fold:0 epoch:132 step:9 train loss:0.031412, train acc:98.804, train f1:98.796, train precision:98.430, train recall:99.165, train auc:99.933
fold:0 epoch:132        valid loss:0.063495, valid acc:98.234, valid f1:98.245, valid precision:97.676, valid recall:98.819, valid auc:99.789
[1;31mEarlyStopping counter: 2 out of 50[0m
[98.32963301554133, 98.33630344576402, 97.94516998341625, 98.73057333159201, 99.78608393169812]
====================================================================================================
fold:0 epoch:133 step:0 train loss:0.029905, train acc:98.810, train f1:98.813, train precision:99.024, train recall:98.603, train auc:99.945
fold:0 epoch:133 step:1 train loss:0.028821, train acc:98.907, train f1:98.908, train precision:98.854, train recall:98.962, train auc:99.944
fold:0 epoch:133 step:2 train loss:0.030972, train acc:98.813, train f1:98.805, train precision:98.409, train recall:99.204, train auc:99.936
fold:0 epoch:133 step:3 train loss:0.029037, train acc:98.953, train f1:98.957, train precision:98.660, train recall:99.256, train auc:99.946
fold:0 epoch:133 step:4 train loss:0.027642, train acc:98.950, train f1:98.951, train precision:98.975, train recall:98.927, train auc:99.952
fold:0 epoch:133 step:5 train loss:0.030075, train acc:98.877, train f1:98.882, train precision:98.900, train recall:98.863, train auc:99.943
fold:0 epoch:133 step:6 train loss:0.029437, train acc:98.926, train f1:98.928, train precision:98.580, train recall:99.279, train auc:99.946
fold:0 epoch:133 step:7 train loss:0.028676, train acc:98.935, train f1:98.939, train precision:98.672, train recall:99.207, train auc:99.947
fold:0 epoch:133 step:8 train loss:0.031252, train acc:98.828, train f1:98.833, train precision:98.948, train recall:98.719, train auc:99.938
fold:0 epoch:133 step:9 train loss:0.031817, train acc:98.883, train f1:98.865, train precision:98.785, train recall:98.945, train auc:99.933
fold:0 epoch:133        valid loss:0.058927, valid acc:98.340, valid f1:98.347, valid precision:97.916, valid recall:98.783, valid auc:99.791
[1;31mTest score increased (98.329633 --> 98.340081).[0m
[98.34008097165992, 98.347397573756, 97.91580364540182, 98.78281311218493, 99.79108788456162]
====================================================================================================
fold:0 epoch:134 step:0 train loss:0.029239, train acc:98.898, train f1:98.916, train precision:98.759, train recall:99.074, train auc:99.944
fold:0 epoch:134 step:1 train loss:0.029362, train acc:98.865, train f1:98.872, train precision:98.448, train recall:99.299, train auc:99.945
fold:0 epoch:134 step:2 train loss:0.031216, train acc:98.874, train f1:98.871, train precision:98.819, train recall:98.922, train auc:99.935
fold:0 epoch:134 step:3 train loss:0.030683, train acc:98.901, train f1:98.893, train precision:99.039, train recall:98.748, train auc:99.940
fold:0 epoch:134 step:4 train loss:0.029322, train acc:98.929, train f1:98.927, train precision:98.852, train recall:99.003, train auc:99.940
fold:0 epoch:134 step:5 train loss:0.027086, train acc:99.023, train f1:99.028, train precision:98.806, train recall:99.251, train auc:99.952
fold:0 epoch:134 step:6 train loss:0.029114, train acc:98.904, train f1:98.899, train precision:98.485, train recall:99.316, train auc:99.948
fold:0 epoch:134 step:7 train loss:0.029441, train acc:98.923, train f1:98.923, train precision:99.029, train recall:98.818, train auc:99.946
fold:0 epoch:134 step:8 train loss:0.028966, train acc:98.856, train f1:98.854, train precision:98.900, train recall:98.809, train auc:99.948
fold:0 epoch:134 step:9 train loss:0.030544, train acc:98.909, train f1:98.914, train precision:98.621, train recall:99.210, train auc:99.934
fold:0 epoch:134        valid loss:0.059429, valid acc:98.358, valid f1:98.365, valid precision:97.956, valid recall:98.778, valid auc:99.797
[1;31mTest score increased (98.340081 --> 98.358365).[0m
[98.35836489486745, 98.36521829602944, 97.95627622649329, 98.77758913412563, 99.79695706518726]
====================================================================================================
fold:0 epoch:135 step:0 train loss:0.027968, train acc:98.959, train f1:98.955, train precision:98.843, train recall:99.067, train auc:99.948
fold:0 epoch:135 step:1 train loss:0.027771, train acc:98.969, train f1:98.965, train precision:98.795, train recall:99.135, train auc:99.949
fold:0 epoch:135 step:2 train loss:0.027904, train acc:98.944, train f1:98.943, train precision:98.769, train recall:99.119, train auc:99.949
fold:0 epoch:135 step:3 train loss:0.028206, train acc:98.947, train f1:98.946, train precision:99.010, train recall:98.883, train auc:99.948
fold:0 epoch:135 step:4 train loss:0.027765, train acc:98.969, train f1:98.973, train precision:98.823, train recall:99.124, train auc:99.948
fold:0 epoch:135 step:5 train loss:0.029329, train acc:98.901, train f1:98.910, train precision:98.820, train recall:99.000, train auc:99.944
fold:0 epoch:135 step:6 train loss:0.028431, train acc:98.947, train f1:98.950, train precision:98.785, train recall:99.116, train auc:99.947
fold:0 epoch:135 step:7 train loss:0.030051, train acc:98.816, train f1:98.816, train precision:98.599, train recall:99.034, train auc:99.941
fold:0 epoch:135 step:8 train loss:0.030136, train acc:98.859, train f1:98.858, train precision:98.870, train recall:98.846, train auc:99.936
fold:0 epoch:135 step:9 train loss:0.025961, train acc:99.050, train f1:99.061, train precision:99.199, train recall:98.923, train auc:99.957
fold:0 epoch:135        valid loss:0.060862, valid acc:98.326, valid f1:98.334, valid precision:97.833, valid recall:98.840, valid auc:99.795
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.35836489486745, 98.36521829602944, 97.95627622649329, 98.77758913412563, 99.79695706518726]
====================================================================================================
fold:0 epoch:136 step:0 train loss:0.028635, train acc:98.941, train f1:98.943, train precision:98.483, train recall:99.406, train auc:99.948
fold:0 epoch:136 step:1 train loss:0.029066, train acc:98.953, train f1:98.953, train precision:98.619, train recall:99.289, train auc:99.939
fold:0 epoch:136 step:2 train loss:0.027763, train acc:99.030, train f1:99.032, train precision:99.038, train recall:99.026, train auc:99.950
fold:0 epoch:136 step:3 train loss:0.026424, train acc:99.008, train f1:99.009, train precision:99.055, train recall:98.964, train auc:99.954
fold:0 epoch:136 step:4 train loss:0.028348, train acc:98.944, train f1:98.950, train precision:98.746, train recall:99.155, train auc:99.945
fold:0 epoch:136 step:5 train loss:0.030781, train acc:98.825, train f1:98.840, train precision:98.653, train recall:99.028, train auc:99.938
fold:0 epoch:136 step:6 train loss:0.027450, train acc:98.993, train f1:98.988, train precision:98.751, train recall:99.225, train auc:99.948
fold:0 epoch:136 step:7 train loss:0.029101, train acc:98.904, train f1:98.899, train precision:98.933, train recall:98.866, train auc:99.945
fold:0 epoch:136 step:8 train loss:0.028424, train acc:98.904, train f1:98.902, train precision:99.044, train recall:98.760, train auc:99.951
fold:0 epoch:136 step:9 train loss:0.027757, train acc:98.927, train f1:98.933, train precision:98.725, train recall:99.141, train auc:99.950
fold:0 epoch:136        valid loss:0.060479, valid acc:98.347, valid f1:98.354, valid precision:97.909, valid recall:98.804, valid auc:99.795
[1;31mEarlyStopping counter: 2 out of 50[0m
[98.35836489486745, 98.36521829602944, 97.95627622649329, 98.77758913412563, 99.79695706518726]
====================================================================================================
fold:0 epoch:137 step:0 train loss:0.026152, train acc:99.057, train f1:99.061, train precision:98.776, train recall:99.348, train auc:99.955
fold:0 epoch:137 step:1 train loss:0.027936, train acc:98.969, train f1:98.972, train precision:98.834, train recall:99.111, train auc:99.947
fold:0 epoch:137 step:2 train loss:0.027519, train acc:99.017, train f1:99.022, train precision:98.884, train recall:99.161, train auc:99.950
fold:0 epoch:137 step:3 train loss:0.025539, train acc:99.042, train f1:99.041, train precision:98.944, train recall:99.137, train auc:99.957
fold:0 epoch:137 step:4 train loss:0.026860, train acc:98.969, train f1:98.963, train precision:98.678, train recall:99.249, train auc:99.954
fold:0 epoch:137 step:5 train loss:0.026204, train acc:99.014, train f1:99.020, train precision:98.945, train recall:99.095, train auc:99.955
fold:0 epoch:137 step:6 train loss:0.029191, train acc:98.865, train f1:98.866, train precision:98.800, train recall:98.932, train auc:99.944
fold:0 epoch:137 step:7 train loss:0.029889, train acc:98.889, train f1:98.878, train precision:98.994, train recall:98.763, train auc:99.943
fold:0 epoch:137 step:8 train loss:0.030095, train acc:98.816, train f1:98.827, train precision:98.713, train recall:98.940, train auc:99.943
fold:0 epoch:137 step:9 train loss:0.030812, train acc:98.830, train f1:98.826, train precision:98.573, train recall:99.079, train auc:99.939
fold:0 epoch:137        valid loss:0.060796, valid acc:98.354, valid f1:98.361, valid precision:97.978, valid recall:98.746, valid auc:99.792
[1;31mEarlyStopping counter: 3 out of 50[0m
[98.35836489486745, 98.36521829602944, 97.95627622649329, 98.77758913412563, 99.79695706518726]
====================================================================================================
fold:0 epoch:138 step:0 train loss:0.023793, train acc:99.130, train f1:99.133, train precision:98.914, train recall:99.354, train auc:99.965
fold:0 epoch:138 step:1 train loss:0.032711, train acc:98.788, train f1:98.786, train precision:98.807, train recall:98.764, train auc:99.931
fold:0 epoch:138 step:2 train loss:0.030039, train acc:98.917, train f1:98.911, train precision:98.908, train recall:98.914, train auc:99.940
fold:0 epoch:138 step:3 train loss:0.026976, train acc:98.965, train f1:98.968, train precision:98.911, train recall:99.026, train auc:99.953
fold:0 epoch:138 step:4 train loss:0.027676, train acc:98.929, train f1:98.937, train precision:98.737, train recall:99.138, train auc:99.950
fold:0 epoch:138 step:5 train loss:0.028641, train acc:98.911, train f1:98.912, train precision:98.663, train recall:99.163, train auc:99.948
fold:0 epoch:138 step:6 train loss:0.029895, train acc:98.911, train f1:98.910, train precision:98.949, train recall:98.870, train auc:99.941
fold:0 epoch:138 step:7 train loss:0.028336, train acc:99.008, train f1:99.010, train precision:98.928, train recall:99.091, train auc:99.945
fold:0 epoch:138 step:8 train loss:0.027604, train acc:98.926, train f1:98.928, train precision:98.681, train recall:99.176, train auc:99.952
fold:0 epoch:138 step:9 train loss:0.030754, train acc:98.857, train f1:98.855, train precision:98.664, train recall:99.047, train auc:99.938
fold:0 epoch:138        valid loss:0.059149, valid acc:98.362, valid f1:98.368, valid precision:98.001, valid recall:98.738, valid auc:99.797
[1;31mTest score increased (98.358365 --> 98.362283).[0m
[98.36228287841192, 98.36841968305187, 98.00119254400747, 98.73840929868093, 99.7967856843216]
====================================================================================================
fold:0 epoch:139 step:0 train loss:0.028375, train acc:98.883, train f1:98.878, train precision:98.733, train recall:99.024, train auc:99.946
fold:0 epoch:139 step:1 train loss:0.026977, train acc:98.981, train f1:98.980, train precision:99.004, train recall:98.956, train auc:99.953
fold:0 epoch:139 step:2 train loss:0.028745, train acc:98.868, train f1:98.865, train precision:98.959, train recall:98.772, train auc:99.948
fold:0 epoch:139 step:3 train loss:0.026587, train acc:98.972, train f1:98.976, train precision:98.763, train recall:99.190, train auc:99.954
fold:0 epoch:139 step:4 train loss:0.027532, train acc:98.950, train f1:98.952, train precision:98.670, train recall:99.237, train auc:99.951
fold:0 epoch:139 step:5 train loss:0.027445, train acc:99.005, train f1:99.000, train precision:99.079, train recall:98.921, train auc:99.949
fold:0 epoch:139 step:6 train loss:0.029364, train acc:98.975, train f1:98.976, train precision:98.874, train recall:99.079, train auc:99.937
fold:0 epoch:139 step:7 train loss:0.029858, train acc:98.868, train f1:98.873, train precision:98.379, train recall:99.371, train auc:99.945
fold:0 epoch:139 step:8 train loss:0.030308, train acc:98.843, train f1:98.854, train precision:98.726, train recall:98.983, train auc:99.940
fold:0 epoch:139 step:9 train loss:0.028761, train acc:98.918, train f1:98.925, train precision:99.177, train recall:98.675, train auc:99.949
fold:0 epoch:139        valid loss:0.060758, valid acc:98.336, valid f1:98.342, valid precision:98.013, valid recall:98.673, valid auc:99.786
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.36228287841192, 98.36841968305187, 98.00119254400747, 98.73840929868093, 99.7967856843216]
====================================================================================================
fold:0 epoch:140 step:0 train loss:0.027626, train acc:98.956, train f1:98.966, train precision:98.709, train recall:99.224, train auc:99.951
fold:0 epoch:140 step:1 train loss:0.025625, train acc:99.051, train f1:99.055, train precision:98.901, train recall:99.208, train auc:99.957
fold:0 epoch:140 step:2 train loss:0.027988, train acc:98.907, train f1:98.906, train precision:98.791, train recall:99.021, train auc:99.951
fold:0 epoch:140 step:3 train loss:0.028171, train acc:99.008, train f1:99.012, train precision:99.033, train recall:98.991, train auc:99.945
fold:0 epoch:140 step:4 train loss:0.027032, train acc:99.042, train f1:99.037, train precision:98.801, train recall:99.275, train auc:99.950
fold:0 epoch:140 step:5 train loss:0.028791, train acc:98.904, train f1:98.907, train precision:98.627, train recall:99.187, train auc:99.946
fold:0 epoch:140 step:6 train loss:0.027576, train acc:99.005, train f1:99.008, train precision:98.893, train recall:99.122, train auc:99.949
fold:0 epoch:140 step:7 train loss:0.027801, train acc:98.956, train f1:98.962, train precision:99.004, train recall:98.920, train auc:99.949
fold:0 epoch:140 step:8 train loss:0.026892, train acc:99.014, train f1:99.007, train precision:98.810, train recall:99.205, train auc:99.952
fold:0 epoch:140 step:9 train loss:0.028291, train acc:99.050, train f1:99.052, train precision:98.792, train recall:99.314, train auc:99.943
fold:0 epoch:140        valid loss:0.059816, valid acc:98.345, valid f1:98.353, valid precision:97.899, valid recall:98.812, valid auc:99.795
[1;31mEarlyStopping counter: 2 out of 50[0m
[98.36228287841192, 98.36841968305187, 98.00119254400747, 98.73840929868093, 99.7967856843216]
====================================================================================================
fold:0 epoch:141 step:0 train loss:0.024859, train acc:99.060, train f1:99.060, train precision:99.121, train recall:99.000, train auc:99.960
fold:0 epoch:141 step:1 train loss:0.026110, train acc:99.002, train f1:98.998, train precision:98.995, train recall:99.001, train auc:99.955
fold:0 epoch:141 step:2 train loss:0.026774, train acc:98.996, train f1:99.000, train precision:98.823, train recall:99.178, train auc:99.953
fold:0 epoch:141 step:3 train loss:0.027290, train acc:98.938, train f1:98.938, train precision:98.523, train recall:99.356, train auc:99.955
fold:0 epoch:141 step:4 train loss:0.028764, train acc:98.926, train f1:98.935, train precision:99.037, train recall:98.833, train auc:99.949
fold:0 epoch:141 step:5 train loss:0.026805, train acc:99.020, train f1:99.028, train precision:98.923, train recall:99.133, train auc:99.954
fold:0 epoch:141 step:6 train loss:0.028997, train acc:98.959, train f1:98.964, train precision:98.769, train recall:99.160, train auc:99.942
fold:0 epoch:141 step:7 train loss:0.029461, train acc:98.895, train f1:98.901, train precision:98.590, train recall:99.214, train auc:99.943
fold:0 epoch:141 step:8 train loss:0.025143, train acc:99.023, train f1:99.009, train precision:98.941, train recall:99.076, train auc:99.960
fold:0 epoch:141 step:9 train loss:0.029169, train acc:99.024, train f1:99.018, train precision:99.502, train recall:98.539, train auc:99.955
fold:0 epoch:141        valid loss:0.060399, valid acc:98.378, valid f1:98.383, valid precision:98.064, valid recall:98.704, valid auc:99.793
[1;31mTest score increased (98.362283 --> 98.377955).[0m
[98.37795481258979, 98.38323353293413, 98.06409757363436, 98.70445344129554, 99.79284000148115]
====================================================================================================
fold:0 epoch:142 step:0 train loss:0.026260, train acc:99.008, train f1:99.012, train precision:98.763, train recall:99.263, train auc:99.954
fold:0 epoch:142 step:1 train loss:0.029489, train acc:98.901, train f1:98.901, train precision:98.438, train recall:99.368, train auc:99.950
fold:0 epoch:142 step:2 train loss:0.023236, train acc:99.115, train f1:99.112, train precision:98.978, train recall:99.245, train auc:99.964
fold:0 epoch:142 step:3 train loss:0.027560, train acc:98.990, train f1:98.992, train precision:99.243, train recall:98.743, train auc:99.957
fold:0 epoch:142 step:4 train loss:0.027526, train acc:98.953, train f1:98.950, train precision:98.869, train recall:99.032, train auc:99.951
fold:0 epoch:142 step:5 train loss:0.029248, train acc:98.880, train f1:98.880, train precision:98.301, train recall:99.466, train auc:99.950
fold:0 epoch:142 step:6 train loss:0.026221, train acc:99.069, train f1:99.078, train precision:99.075, train recall:99.081, train auc:99.953
fold:0 epoch:142 step:7 train loss:0.028655, train acc:98.953, train f1:98.962, train precision:99.049, train recall:98.876, train auc:99.948
fold:0 epoch:142 step:8 train loss:0.026720, train acc:98.987, train f1:98.982, train precision:98.848, train recall:99.115, train auc:99.955
fold:0 epoch:142 step:9 train loss:0.026739, train acc:99.068, train f1:99.069, train precision:98.948, train recall:99.191, train auc:99.954
fold:0 epoch:142        valid loss:0.062784, valid acc:98.301, valid f1:98.310, valid precision:97.808, valid recall:98.817, valid auc:99.790
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.37795481258979, 98.38323353293413, 98.06409757363436, 98.70445344129554, 99.79284000148115]
====================================================================================================
fold:0 epoch:143 step:0 train loss:0.025518, train acc:99.011, train f1:99.014, train precision:98.744, train recall:99.286, train auc:99.957
fold:0 epoch:143 step:1 train loss:0.029419, train acc:98.871, train f1:98.872, train precision:98.788, train recall:98.957, train auc:99.944
fold:0 epoch:143 step:2 train loss:0.024655, train acc:99.081, train f1:99.085, train precision:99.251, train recall:98.920, train auc:99.962
fold:0 epoch:143 step:3 train loss:0.026613, train acc:99.036, train f1:99.026, train precision:98.892, train recall:99.161, train auc:99.953
fold:0 epoch:143 step:4 train loss:0.027413, train acc:98.935, train f1:98.940, train precision:98.578, train recall:99.305, train auc:99.953
fold:0 epoch:143 step:5 train loss:0.027450, train acc:98.996, train f1:98.997, train precision:98.976, train recall:99.018, train auc:99.952
fold:0 epoch:143 step:6 train loss:0.025126, train acc:99.051, train f1:99.046, train precision:99.006, train recall:99.085, train auc:99.959
fold:0 epoch:143 step:7 train loss:0.026153, train acc:99.045, train f1:99.048, train precision:99.087, train recall:99.009, train auc:99.954
fold:0 epoch:143 step:8 train loss:0.028449, train acc:98.953, train f1:98.961, train precision:98.802, train recall:99.120, train auc:99.947
fold:0 epoch:143 step:9 train loss:0.024375, train acc:99.103, train f1:99.101, train precision:98.684, train recall:99.522, train auc:99.969
fold:0 epoch:143        valid loss:0.061034, valid acc:98.366, valid f1:98.376, valid precision:97.818, valid recall:98.940, valid auc:99.799
[1;31mEarlyStopping counter: 2 out of 50[0m
[98.37795481258979, 98.38323353293413, 98.06409757363436, 98.70445344129554, 99.79284000148115]
====================================================================================================
fold:0 epoch:144 step:0 train loss:0.025997, train acc:99.054, train f1:99.051, train precision:98.985, train recall:99.118, train auc:99.956
fold:0 epoch:144 step:1 train loss:0.027824, train acc:99.008, train f1:99.007, train precision:99.295, train recall:98.721, train auc:99.954
fold:0 epoch:144 step:2 train loss:0.026556, train acc:98.984, train f1:98.991, train precision:98.838, train recall:99.144, train auc:99.955
fold:0 epoch:144 step:3 train loss:0.026974, train acc:98.990, train f1:98.986, train precision:98.591, train recall:99.385, train auc:99.957
fold:0 epoch:144 step:4 train loss:0.026810, train acc:99.057, train f1:99.062, train precision:98.981, train recall:99.143, train auc:99.950
fold:0 epoch:144 step:5 train loss:0.031205, train acc:98.807, train f1:98.809, train precision:99.156, train recall:98.464, train auc:99.946
fold:0 epoch:144 step:6 train loss:0.026109, train acc:99.048, train f1:99.045, train precision:98.755, train recall:99.337, train auc:99.955
fold:0 epoch:144 step:7 train loss:0.026534, train acc:98.941, train f1:98.940, train precision:98.804, train recall:99.076, train auc:99.955
fold:0 epoch:144 step:8 train loss:0.024266, train acc:99.063, train f1:99.063, train precision:99.054, train recall:99.072, train auc:99.963
fold:0 epoch:144 step:9 train loss:0.023963, train acc:99.085, train f1:99.093, train precision:98.990, train recall:99.197, train auc:99.961
fold:0 epoch:144        valid loss:0.063989, valid acc:98.354, valid f1:98.364, valid precision:97.775, valid recall:98.960, valid auc:99.789
[1;31mEarlyStopping counter: 3 out of 50[0m
[98.37795481258979, 98.38323353293413, 98.06409757363436, 98.70445344129554, 99.79284000148115]
====================================================================================================
fold:0 epoch:145 step:0 train loss:0.025244, train acc:99.017, train f1:99.023, train precision:98.705, train recall:99.342, train auc:99.962
fold:0 epoch:145 step:1 train loss:0.024161, train acc:99.115, train f1:99.112, train precision:98.985, train recall:99.240, train auc:99.963
fold:0 epoch:145 step:2 train loss:0.024517, train acc:99.051, train f1:99.056, train precision:99.035, train recall:99.077, train auc:99.964
fold:0 epoch:145 step:3 train loss:0.027698, train acc:98.969, train f1:98.963, train precision:98.739, train recall:99.188, train auc:99.950
fold:0 epoch:145 step:4 train loss:0.023646, train acc:99.167, train f1:99.170, train precision:99.173, train recall:99.167, train auc:99.963
fold:0 epoch:145 step:5 train loss:0.026258, train acc:98.969, train f1:98.970, train precision:98.873, train recall:99.067, train auc:99.954
fold:0 epoch:145 step:6 train loss:0.029210, train acc:98.904, train f1:98.901, train precision:98.668, train recall:99.135, train auc:99.944
fold:0 epoch:145 step:7 train loss:0.026995, train acc:98.926, train f1:98.932, train precision:99.101, train recall:98.765, train auc:99.956
fold:0 epoch:145 step:8 train loss:0.026317, train acc:99.014, train f1:99.013, train precision:98.835, train recall:99.192, train auc:99.953
fold:0 epoch:145 step:9 train loss:0.022895, train acc:99.270, train f1:99.272, train precision:99.125, train recall:99.421, train auc:99.964
fold:0 epoch:145        valid loss:0.061255, valid acc:98.353, valid f1:98.360, valid precision:97.944, valid recall:98.780, valid auc:99.789
[1;31mEarlyStopping counter: 4 out of 50[0m
[98.37795481258979, 98.38323353293413, 98.06409757363436, 98.70445344129554, 99.79284000148115]
====================================================================================================
fold:0 epoch:146 step:0 train loss:0.025388, train acc:99.036, train f1:99.030, train precision:98.746, train recall:99.317, train auc:99.958
fold:0 epoch:146 step:1 train loss:0.025772, train acc:99.091, train f1:99.090, train precision:99.272, train recall:98.909, train auc:99.959
fold:0 epoch:146 step:2 train loss:0.025541, train acc:99.014, train f1:99.022, train precision:99.019, train recall:99.025, train auc:99.958
fold:0 epoch:146 step:3 train loss:0.026024, train acc:98.972, train f1:98.978, train precision:98.539, train recall:99.421, train auc:99.960
fold:0 epoch:146 step:4 train loss:0.025217, train acc:99.118, train f1:99.121, train precision:99.004, train recall:99.239, train auc:99.957
fold:0 epoch:146 step:5 train loss:0.026572, train acc:98.969, train f1:98.967, train precision:99.179, train recall:98.756, train auc:99.959
fold:0 epoch:146 step:6 train loss:0.028918, train acc:98.907, train f1:98.904, train precision:98.855, train recall:98.952, train auc:99.946
fold:0 epoch:146 step:7 train loss:0.026014, train acc:99.112, train f1:99.108, train precision:98.893, train recall:99.324, train auc:99.955
fold:0 epoch:146 step:8 train loss:0.028047, train acc:98.981, train f1:98.982, train precision:98.832, train recall:99.133, train auc:99.948
fold:0 epoch:146 step:9 train loss:0.025120, train acc:98.997, train f1:99.007, train precision:99.301, train recall:98.715, train auc:99.965
fold:0 epoch:146        valid loss:0.061407, valid acc:98.366, valid f1:98.375, valid precision:97.840, valid recall:98.916, valid auc:99.794
[1;31mEarlyStopping counter: 5 out of 50[0m
[98.37795481258979, 98.38323353293413, 98.06409757363436, 98.70445344129554, 99.79284000148115]
====================================================================================================
fold:0 epoch:147 step:0 train loss:0.026046, train acc:98.990, train f1:98.990, train precision:98.753, train recall:99.230, train auc:99.954
fold:0 epoch:147 step:1 train loss:0.028943, train acc:98.944, train f1:98.934, train precision:98.437, train recall:99.437, train auc:99.946
fold:0 epoch:147 step:2 train loss:0.026939, train acc:99.023, train f1:99.029, train precision:99.222, train recall:98.837, train auc:99.957
fold:0 epoch:147 step:3 train loss:0.025594, train acc:99.051, train f1:99.056, train precision:98.897, train recall:99.216, train auc:99.955
fold:0 epoch:147 step:4 train loss:0.024297, train acc:99.088, train f1:99.090, train precision:98.834, train recall:99.347, train auc:99.962
fold:0 epoch:147 step:5 train loss:0.025980, train acc:99.023, train f1:99.031, train precision:98.990, train recall:99.073, train auc:99.958
fold:0 epoch:147 step:6 train loss:0.025956, train acc:98.999, train f1:99.002, train precision:98.899, train recall:99.104, train auc:99.955
fold:0 epoch:147 step:7 train loss:0.025743, train acc:99.097, train f1:99.090, train precision:99.090, train recall:99.090, train auc:99.956
fold:0 epoch:147 step:8 train loss:0.027119, train acc:98.996, train f1:98.997, train precision:98.976, train recall:99.018, train auc:99.953
fold:0 epoch:147 step:9 train loss:0.028846, train acc:98.901, train f1:98.906, train precision:98.759, train recall:99.053, train auc:99.946
fold:0 epoch:147        valid loss:0.060663, valid acc:98.390, valid f1:98.396, valid precision:98.035, valid recall:98.759, valid auc:99.792
[1;31mTest score increased (98.377955 --> 98.389709).[0m
[98.3897087632232, 98.39563842660664, 98.03464011615847, 98.75930521091811, 99.79178967965508]
====================================================================================================
fold:0 epoch:148 step:0 train loss:0.026411, train acc:99.014, train f1:99.015, train precision:98.849, train recall:99.181, train auc:99.956
fold:0 epoch:148 step:1 train loss:0.026941, train acc:99.039, train f1:99.033, train precision:98.854, train recall:99.213, train auc:99.948
fold:0 epoch:148 step:2 train loss:0.025684, train acc:99.002, train f1:99.004, train precision:99.091, train recall:98.916, train auc:99.960
fold:0 epoch:148 step:3 train loss:0.026223, train acc:99.005, train f1:99.006, train precision:98.910, train recall:99.103, train auc:99.955
fold:0 epoch:148 step:4 train loss:0.025557, train acc:99.066, train f1:99.070, train precision:99.010, train recall:99.130, train auc:99.954
fold:0 epoch:148 step:5 train loss:0.025434, train acc:99.005, train f1:99.009, train precision:98.745, train recall:99.275, train auc:99.957
fold:0 epoch:148 step:6 train loss:0.026419, train acc:99.091, train f1:99.094, train precision:98.872, train recall:99.318, train auc:99.953
fold:0 epoch:148 step:7 train loss:0.027873, train acc:99.011, train f1:99.012, train precision:98.982, train recall:99.043, train auc:99.945
fold:0 epoch:148 step:8 train loss:0.026390, train acc:99.014, train f1:99.013, train precision:99.253, train recall:98.775, train auc:99.954
fold:0 epoch:148 step:9 train loss:0.028038, train acc:98.945, train f1:98.938, train precision:98.764, train recall:99.114, train auc:99.951
fold:0 epoch:148        valid loss:0.061267, valid acc:98.387, valid f1:98.394, valid precision:97.972, valid recall:98.819, valid auc:99.802
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.3897087632232, 98.39563842660664, 98.03464011615847, 98.75930521091811, 99.79178967965508]
====================================================================================================
fold:0 epoch:149 step:0 train loss:0.025516, train acc:99.097, train f1:99.103, train precision:98.786, train recall:99.422, train auc:99.957
fold:0 epoch:149 step:1 train loss:0.024475, train acc:99.066, train f1:99.055, train precision:99.080, train recall:99.031, train auc:99.960
fold:0 epoch:149 step:2 train loss:0.024311, train acc:99.127, train f1:99.126, train precision:99.277, train recall:98.974, train auc:99.962
fold:0 epoch:149 step:3 train loss:0.024110, train acc:99.103, train f1:99.106, train precision:99.160, train recall:99.051, train auc:99.964
fold:0 epoch:149 step:4 train loss:0.027598, train acc:99.026, train f1:99.032, train precision:98.748, train recall:99.318, train auc:99.949
fold:0 epoch:149 step:5 train loss:0.024752, train acc:99.088, train f1:99.089, train precision:98.857, train recall:99.322, train auc:99.959
fold:0 epoch:149 step:6 train loss:0.026550, train acc:99.039, train f1:99.040, train precision:98.989, train recall:99.091, train auc:99.951
fold:0 epoch:149 step:7 train loss:0.024748, train acc:99.146, train f1:99.142, train precision:99.239, train recall:99.045, train auc:99.958
fold:0 epoch:149 step:8 train loss:0.025353, train acc:99.078, train f1:99.087, train precision:98.985, train recall:99.189, train auc:99.956
fold:0 epoch:149 step:9 train loss:0.030266, train acc:98.839, train f1:98.831, train precision:98.326, train recall:99.341, train auc:99.948
fold:0 epoch:149        valid loss:0.063384, valid acc:98.298, valid f1:98.309, valid precision:97.697, valid recall:98.929, valid auc:99.794
[1;31mEarlyStopping counter: 2 out of 50[0m
[98.3897087632232, 98.39563842660664, 98.03464011615847, 98.75930521091811, 99.79178967965508]
====================================================================================================
fold:0 epoch:150 step:0 train loss:0.022778, train acc:99.164, train f1:99.165, train precision:99.183, train recall:99.147, train auc:99.964
fold:0 epoch:150 step:1 train loss:0.024558, train acc:99.066, train f1:99.062, train precision:99.196, train recall:98.929, train auc:99.964
fold:0 epoch:150 step:2 train loss:0.028035, train acc:98.956, train f1:98.952, train precision:99.037, train recall:98.867, train auc:99.951
fold:0 epoch:150 step:3 train loss:0.027750, train acc:98.969, train f1:98.974, train precision:98.466, train recall:99.488, train auc:99.956
fold:0 epoch:150 step:4 train loss:0.024661, train acc:99.084, train f1:99.089, train precision:98.849, train recall:99.330, train auc:99.960
fold:0 epoch:150 step:5 train loss:0.027147, train acc:99.008, train f1:99.009, train precision:99.211, train recall:98.807, train auc:99.957
fold:0 epoch:150 step:6 train loss:0.023996, train acc:99.097, train f1:99.096, train precision:99.187, train recall:99.005, train auc:99.965
fold:0 epoch:150 step:7 train loss:0.024737, train acc:99.124, train f1:99.128, train precision:98.830, train recall:99.427, train auc:99.961
fold:0 epoch:150 step:8 train loss:0.026315, train acc:99.017, train f1:99.022, train precision:98.609, train recall:99.439, train auc:99.956
fold:0 epoch:150 step:9 train loss:0.027324, train acc:99.077, train f1:99.071, train precision:99.062, train recall:99.079, train auc:99.936
fold:0 epoch:150        valid loss:0.061849, valid acc:98.307, valid f1:98.316, valid precision:97.810, valid recall:98.827, valid auc:99.797
[1;31mEarlyStopping counter: 3 out of 50[0m
[98.3897087632232, 98.39563842660664, 98.03464011615847, 98.75930521091811, 99.79178967965508]
====================================================================================================
fold:0 epoch:151 step:0 train loss:0.025502, train acc:99.030, train f1:99.022, train precision:99.303, train recall:98.743, train auc:99.960
fold:0 epoch:151 step:1 train loss:0.024326, train acc:99.142, train f1:99.146, train precision:98.999, train recall:99.294, train auc:99.958
fold:0 epoch:151 step:2 train loss:0.027626, train acc:98.941, train f1:98.937, train precision:98.590, train recall:99.287, train auc:99.953
fold:0 epoch:151 step:3 train loss:0.026372, train acc:99.011, train f1:99.019, train precision:98.852, train recall:99.187, train auc:99.952
fold:0 epoch:151 step:4 train loss:0.026617, train acc:98.969, train f1:98.961, train precision:99.028, train recall:98.895, train auc:99.952
fold:0 epoch:151 step:5 train loss:0.024482, train acc:99.057, train f1:99.050, train precision:99.144, train recall:98.955, train auc:99.963
fold:0 epoch:151 step:6 train loss:0.024938, train acc:99.039, train f1:99.046, train precision:98.887, train recall:99.205, train auc:99.958
fold:0 epoch:151 step:7 train loss:0.026709, train acc:99.042, train f1:99.047, train precision:98.646, train recall:99.451, train auc:99.955
fold:0 epoch:151 step:8 train loss:0.027125, train acc:98.950, train f1:98.960, train precision:98.829, train recall:99.092, train auc:99.954
fold:0 epoch:151 step:9 train loss:0.025042, train acc:99.147, train f1:99.142, train precision:99.397, train recall:98.888, train auc:99.959
fold:0 epoch:151        valid loss:0.063052, valid acc:98.328, valid f1:98.338, valid precision:97.787, valid recall:98.895, valid auc:99.797
[1;31mEarlyStopping counter: 4 out of 50[0m
[98.3897087632232, 98.39563842660664, 98.03464011615847, 98.75930521091811, 99.79178967965508]
====================================================================================================
fold:0 epoch:152 step:0 train loss:0.022643, train acc:99.164, train f1:99.160, train precision:99.233, train recall:99.088, train auc:99.967
fold:0 epoch:152 step:1 train loss:0.025520, train acc:99.066, train f1:99.074, train precision:98.752, train recall:99.399, train auc:99.957
fold:0 epoch:152 step:2 train loss:0.028300, train acc:98.956, train f1:98.972, train precision:98.563, train recall:99.384, train auc:99.949
fold:0 epoch:152 step:3 train loss:0.024308, train acc:99.030, train f1:99.036, train precision:98.946, train recall:99.126, train auc:99.964
fold:0 epoch:152 step:4 train loss:0.024466, train acc:99.100, train f1:99.102, train precision:99.317, train recall:98.888, train auc:99.964
fold:0 epoch:152 step:5 train loss:0.023580, train acc:99.142, train f1:99.131, train precision:98.969, train recall:99.294, train auc:99.965
fold:0 epoch:152 step:6 train loss:0.024779, train acc:99.094, train f1:99.094, train precision:98.946, train recall:99.242, train auc:99.959
fold:0 epoch:152 step:7 train loss:0.023873, train acc:99.124, train f1:99.121, train precision:99.076, train recall:99.167, train auc:99.964
fold:0 epoch:152 step:8 train loss:0.024125, train acc:99.081, train f1:99.075, train precision:98.902, train recall:99.249, train auc:99.961
fold:0 epoch:152 step:9 train loss:0.024968, train acc:99.120, train f1:99.132, train precision:99.184, train recall:99.081, train auc:99.959
fold:0 epoch:152        valid loss:0.060643, valid acc:98.378, valid f1:98.386, valid precision:97.883, valid recall:98.895, valid auc:99.804
[1;31mEarlyStopping counter: 5 out of 50[0m
[98.3897087632232, 98.39563842660664, 98.03464011615847, 98.75930521091811, 99.79178967965508]
====================================================================================================
fold:0 epoch:153 step:0 train loss:0.023348, train acc:99.158, train f1:99.158, train precision:99.206, train recall:99.109, train auc:99.964
fold:0 epoch:153 step:1 train loss:0.022221, train acc:99.203, train f1:99.203, train precision:99.073, train recall:99.333, train auc:99.967
fold:0 epoch:153 step:2 train loss:0.023451, train acc:99.121, train f1:99.128, train precision:98.967, train recall:99.291, train auc:99.962
fold:0 epoch:153 step:3 train loss:0.025207, train acc:99.100, train f1:99.097, train precision:98.767, train recall:99.428, train auc:99.959
fold:0 epoch:153 step:4 train loss:0.025136, train acc:99.039, train f1:99.038, train precision:99.096, train recall:98.981, train auc:99.960
fold:0 epoch:153 step:5 train loss:0.022784, train acc:99.185, train f1:99.181, train precision:99.214, train recall:99.147, train auc:99.964
fold:0 epoch:153 step:6 train loss:0.026691, train acc:98.996, train f1:99.004, train precision:98.893, train recall:99.115, train auc:99.954
fold:0 epoch:153 step:7 train loss:0.024330, train acc:99.081, train f1:99.081, train precision:98.945, train recall:99.217, train auc:99.960
fold:0 epoch:153 step:8 train loss:0.023949, train acc:99.136, train f1:99.132, train precision:99.159, train recall:99.104, train auc:99.961
fold:0 epoch:153 step:9 train loss:0.021960, train acc:99.147, train f1:99.162, train precision:99.222, train recall:99.103, train auc:99.970
fold:0 epoch:153        valid loss:0.062215, valid acc:98.387, valid f1:98.394, valid precision:97.985, valid recall:98.806, valid auc:99.797
[1;31mEarlyStopping counter: 6 out of 50[0m
[98.3897087632232, 98.39563842660664, 98.03464011615847, 98.75930521091811, 99.79178967965508]
====================================================================================================
fold:0 epoch:154 step:0 train loss:0.021833, train acc:99.207, train f1:99.215, train precision:99.143, train recall:99.287, train auc:99.968
fold:0 epoch:154 step:1 train loss:0.025855, train acc:99.011, train f1:99.012, train precision:98.627, train recall:99.400, train auc:99.959
fold:0 epoch:154 step:2 train loss:0.024958, train acc:99.103, train f1:99.100, train precision:99.028, train recall:99.173, train auc:99.960
fold:0 epoch:154 step:3 train loss:0.024117, train acc:99.155, train f1:99.157, train precision:99.372, train recall:98.943, train auc:99.964
fold:0 epoch:154 step:4 train loss:0.024923, train acc:99.063, train f1:99.066, train precision:98.949, train recall:99.184, train auc:99.958
fold:0 epoch:154 step:5 train loss:0.027968, train acc:99.002, train f1:99.001, train precision:98.595, train recall:99.411, train auc:99.952
fold:0 epoch:154 step:6 train loss:0.022888, train acc:99.182, train f1:99.178, train precision:99.142, train recall:99.215, train auc:99.965
fold:0 epoch:154 step:7 train loss:0.025054, train acc:99.054, train f1:99.053, train precision:99.296, train recall:98.812, train auc:99.965
fold:0 epoch:154 step:8 train loss:0.022609, train acc:99.197, train f1:99.200, train precision:99.041, train recall:99.360, train auc:99.966
fold:0 epoch:154 step:9 train loss:0.028356, train acc:98.971, train f1:98.962, train precision:98.587, train recall:99.341, train auc:99.945
fold:0 epoch:154        valid loss:0.059842, valid acc:98.433, valid f1:98.439, valid precision:98.081, valid recall:98.798, valid auc:99.803
[1;31mTest score increased (98.389709 --> 98.432807).[0m
[98.43280658221235, 98.43851659076122, 98.081161675094, 98.7984850463628, 99.80330235304666]
====================================================================================================
fold:0 epoch:155 step:0 train loss:0.021882, train acc:99.203, train f1:99.207, train precision:99.222, train recall:99.192, train auc:99.969
fold:0 epoch:155 step:1 train loss:0.025579, train acc:99.124, train f1:99.119, train precision:98.994, train recall:99.244, train auc:99.953
fold:0 epoch:155 step:2 train loss:0.023300, train acc:99.161, train f1:99.161, train precision:99.231, train recall:99.092, train auc:99.966
fold:0 epoch:155 step:3 train loss:0.021924, train acc:99.164, train f1:99.167, train precision:98.987, train recall:99.349, train auc:99.968
fold:0 epoch:155 step:4 train loss:0.025731, train acc:99.048, train f1:99.053, train precision:98.891, train recall:99.216, train auc:99.956
fold:0 epoch:155 step:5 train loss:0.022476, train acc:99.164, train f1:99.162, train precision:99.077, train recall:99.247, train auc:99.966
fold:0 epoch:155 step:6 train loss:0.022597, train acc:99.139, train f1:99.138, train precision:99.065, train recall:99.210, train auc:99.968
fold:0 epoch:155 step:7 train loss:0.024660, train acc:99.155, train f1:99.154, train precision:99.078, train recall:99.230, train auc:99.955
fold:0 epoch:155 step:8 train loss:0.024209, train acc:99.066, train f1:99.071, train precision:98.998, train recall:99.143, train auc:99.959
fold:0 epoch:155 step:9 train loss:0.022563, train acc:99.147, train f1:99.143, train precision:98.960, train recall:99.327, train auc:99.970
fold:0 epoch:155        valid loss:0.059058, valid acc:98.417, valid f1:98.423, valid precision:98.058, valid recall:98.791, valid auc:99.806
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.43280658221235, 98.43851659076122, 98.081161675094, 98.7984850463628, 99.80330235304666]
====================================================================================================
fold:0 epoch:156 step:0 train loss:0.022574, train acc:99.176, train f1:99.177, train precision:99.051, train recall:99.304, train auc:99.967
fold:0 epoch:156 step:1 train loss:0.021569, train acc:99.240, train f1:99.239, train precision:99.242, train recall:99.236, train auc:99.969
fold:0 epoch:156 step:2 train loss:0.024502, train acc:99.106, train f1:99.108, train precision:99.129, train recall:99.087, train auc:99.958
fold:0 epoch:156 step:3 train loss:0.024056, train acc:99.091, train f1:99.092, train precision:98.845, train recall:99.340, train auc:99.962
fold:0 epoch:156 step:4 train loss:0.024052, train acc:99.136, train f1:99.142, train precision:99.037, train recall:99.247, train auc:99.961
fold:0 epoch:156 step:5 train loss:0.023584, train acc:99.167, train f1:99.165, train precision:99.156, train recall:99.174, train auc:99.960
fold:0 epoch:156 step:6 train loss:0.021997, train acc:99.152, train f1:99.145, train precision:99.176, train recall:99.115, train auc:99.969
fold:0 epoch:156 step:7 train loss:0.023549, train acc:99.124, train f1:99.131, train precision:98.984, train recall:99.278, train auc:99.961
fold:0 epoch:156 step:8 train loss:0.023602, train acc:99.097, train f1:99.097, train precision:98.814, train recall:99.382, train auc:99.962
fold:0 epoch:156 step:9 train loss:0.023721, train acc:99.120, train f1:99.121, train precision:98.930, train recall:99.313, train auc:99.960
fold:0 epoch:156        valid loss:0.059797, valid acc:98.418, valid f1:98.425, valid precision:98.041, valid recall:98.812, valid auc:99.804
[1;31mEarlyStopping counter: 2 out of 50[0m
[98.43280658221235, 98.43851659076122, 98.081161675094, 98.7984850463628, 99.80330235304666]
====================================================================================================
fold:0 epoch:157 step:0 train loss:0.022313, train acc:99.200, train f1:99.193, train precision:99.328, train recall:99.059, train auc:99.966
fold:0 epoch:157 step:1 train loss:0.022095, train acc:99.252, train f1:99.249, train precision:99.307, train recall:99.192, train auc:99.964
fold:0 epoch:157 step:2 train loss:0.022103, train acc:99.149, train f1:99.152, train precision:99.077, train recall:99.227, train auc:99.966
fold:0 epoch:157 step:3 train loss:0.023354, train acc:99.118, train f1:99.127, train precision:98.814, train recall:99.443, train auc:99.966
fold:0 epoch:157 step:4 train loss:0.023366, train acc:99.124, train f1:99.133, train precision:99.106, train recall:99.160, train auc:99.964
fold:0 epoch:157 step:5 train loss:0.024177, train acc:99.142, train f1:99.139, train precision:99.160, train recall:99.118, train auc:99.960
fold:0 epoch:157 step:6 train loss:0.023341, train acc:99.084, train f1:99.074, train precision:99.001, train recall:99.148, train auc:99.964
fold:0 epoch:157 step:7 train loss:0.025676, train acc:99.023, train f1:99.035, train precision:98.885, train recall:99.184, train auc:99.957
fold:0 epoch:157 step:8 train loss:0.021509, train acc:99.225, train f1:99.225, train precision:98.990, train recall:99.462, train auc:99.972
fold:0 epoch:157 step:9 train loss:0.024697, train acc:99.156, train f1:99.155, train precision:98.998, train recall:99.312, train auc:99.962
fold:0 epoch:157        valid loss:0.061480, valid acc:98.420, valid f1:98.428, valid precision:97.931, valid recall:98.929, valid auc:99.799
[1;31mEarlyStopping counter: 3 out of 50[0m
[98.43280658221235, 98.43851659076122, 98.081161675094, 98.7984850463628, 99.80330235304666]
====================================================================================================
fold:0 epoch:158 step:0 train loss:0.024243, train acc:99.133, train f1:99.128, train precision:99.446, train recall:98.813, train auc:99.965
fold:0 epoch:158 step:1 train loss:0.021918, train acc:99.179, train f1:99.176, train precision:99.058, train recall:99.295, train auc:99.968
fold:0 epoch:158 step:2 train loss:0.024279, train acc:99.023, train f1:99.029, train precision:98.604, train recall:99.458, train auc:99.965
fold:0 epoch:158 step:3 train loss:0.023123, train acc:99.176, train f1:99.176, train precision:99.115, train recall:99.236, train auc:99.962
fold:0 epoch:158 step:4 train loss:0.025228, train acc:99.063, train f1:99.065, train precision:99.128, train recall:99.002, train auc:99.959
fold:0 epoch:158 step:5 train loss:0.023450, train acc:99.118, train f1:99.114, train precision:99.075, train recall:99.154, train auc:99.963
fold:0 epoch:158 step:6 train loss:0.023122, train acc:99.109, train f1:99.116, train precision:98.931, train recall:99.303, train auc:99.967
fold:0 epoch:158 step:7 train loss:0.024072, train acc:99.158, train f1:99.162, train precision:98.898, train recall:99.428, train auc:99.961
fold:0 epoch:158 step:8 train loss:0.021446, train acc:99.167, train f1:99.167, train precision:99.273, train recall:99.061, train auc:99.971
fold:0 epoch:158 step:9 train loss:0.026195, train acc:99.085, train f1:99.083, train precision:99.135, train recall:99.030, train auc:99.956
fold:0 epoch:158        valid loss:0.062504, valid acc:98.379, valid f1:98.387, valid precision:97.900, valid recall:98.879, valid auc:99.800
[1;31mEarlyStopping counter: 4 out of 50[0m
[98.43280658221235, 98.43851659076122, 98.081161675094, 98.7984850463628, 99.80330235304666]
====================================================================================================
fold:0 epoch:159 step:0 train loss:0.022215, train acc:99.152, train f1:99.152, train precision:99.159, train recall:99.146, train auc:99.969
fold:0 epoch:159 step:1 train loss:0.022029, train acc:99.130, train f1:99.125, train precision:98.861, train recall:99.391, train auc:99.972
fold:0 epoch:159 step:2 train loss:0.022388, train acc:99.149, train f1:99.147, train precision:98.944, train recall:99.351, train auc:99.967
fold:0 epoch:159 step:3 train loss:0.023109, train acc:99.130, train f1:99.133, train precision:99.124, train recall:99.142, train auc:99.965
fold:0 epoch:159 step:4 train loss:0.022363, train acc:99.231, train f1:99.237, train precision:99.321, train recall:99.153, train auc:99.967
fold:0 epoch:159 step:5 train loss:0.024059, train acc:99.057, train f1:99.052, train precision:98.789, train recall:99.317, train auc:99.963
fold:0 epoch:159 step:6 train loss:0.025683, train acc:98.996, train f1:99.001, train precision:98.866, train recall:99.136, train auc:99.958
fold:0 epoch:159 step:7 train loss:0.023121, train acc:99.170, train f1:99.168, train precision:99.180, train recall:99.156, train auc:99.963
fold:0 epoch:159 step:8 train loss:0.024479, train acc:99.094, train f1:99.100, train precision:99.121, train recall:99.079, train auc:99.959
fold:0 epoch:159 step:9 train loss:0.022328, train acc:99.103, train f1:99.102, train precision:98.910, train recall:99.294, train auc:99.967
fold:0 epoch:159        valid loss:0.061303, valid acc:98.433, valid f1:98.439, valid precision:98.059, valid recall:98.822, valid auc:99.799
[1;31mEarlyStopping counter: 5 out of 50[0m
[98.43280658221235, 98.43851659076122, 98.081161675094, 98.7984850463628, 99.80330235304666]
====================================================================================================
fold:0 epoch:160 step:0 train loss:0.024081, train acc:99.139, train f1:99.140, train precision:99.140, train recall:99.140, train auc:99.964
fold:0 epoch:160 step:1 train loss:0.023880, train acc:99.084, train f1:99.085, train precision:99.109, train recall:99.061, train auc:99.961
fold:0 epoch:160 step:2 train loss:0.023273, train acc:99.127, train f1:99.122, train precision:99.018, train recall:99.225, train auc:99.965
fold:0 epoch:160 step:3 train loss:0.021462, train acc:99.161, train f1:99.165, train precision:99.012, train recall:99.319, train auc:99.971
fold:0 epoch:160 step:4 train loss:0.020514, train acc:99.234, train f1:99.232, train precision:99.065, train recall:99.399, train auc:99.975
fold:0 epoch:160 step:5 train loss:0.022900, train acc:99.161, train f1:99.161, train precision:99.122, train recall:99.201, train auc:99.966
fold:0 epoch:160 step:6 train loss:0.022050, train acc:99.203, train f1:99.208, train precision:99.362, train recall:99.054, train auc:99.968
fold:0 epoch:160 step:7 train loss:0.022992, train acc:99.121, train f1:99.128, train precision:98.936, train recall:99.320, train auc:99.966
fold:0 epoch:160 step:8 train loss:0.024256, train acc:99.100, train f1:99.099, train precision:98.825, train recall:99.375, train auc:99.958
fold:0 epoch:160 step:9 train loss:0.023418, train acc:99.094, train f1:99.087, train precision:99.184, train recall:98.991, train auc:99.964
fold:0 epoch:160        valid loss:0.060435, valid acc:98.426, valid f1:98.433, valid precision:97.984, valid recall:98.887, valid auc:99.806
[1;31mEarlyStopping counter: 6 out of 50[0m
[98.43280658221235, 98.43851659076122, 98.081161675094, 98.7984850463628, 99.80330235304666]
====================================================================================================
fold:0 epoch:161 step:0 train loss:0.021699, train acc:99.167, train f1:99.160, train precision:99.145, train recall:99.175, train auc:99.970
fold:0 epoch:161 step:1 train loss:0.023258, train acc:99.158, train f1:99.161, train precision:99.215, train recall:99.107, train auc:99.963
fold:0 epoch:161 step:2 train loss:0.024593, train acc:99.063, train f1:99.059, train precision:98.692, train recall:99.427, train auc:99.964
fold:0 epoch:161 step:3 train loss:0.022323, train acc:99.222, train f1:99.224, train precision:99.136, train recall:99.311, train auc:99.965
fold:0 epoch:161 step:4 train loss:0.022517, train acc:99.167, train f1:99.172, train precision:99.217, train recall:99.127, train auc:99.969
fold:0 epoch:161 step:5 train loss:0.023106, train acc:99.133, train f1:99.131, train precision:99.149, train recall:99.113, train auc:99.966
fold:0 epoch:161 step:6 train loss:0.021599, train acc:99.219, train f1:99.219, train precision:99.140, train recall:99.298, train auc:99.967
fold:0 epoch:161 step:7 train loss:0.021322, train acc:99.222, train f1:99.223, train precision:99.003, train recall:99.444, train auc:99.972
fold:0 epoch:161 step:8 train loss:0.021751, train acc:99.207, train f1:99.213, train precision:99.171, train recall:99.255, train auc:99.968
fold:0 epoch:161 step:9 train loss:0.021184, train acc:99.200, train f1:99.199, train precision:99.103, train recall:99.295, train auc:99.972
fold:0 epoch:161        valid loss:0.061722, valid acc:98.405, valid f1:98.413, valid precision:97.951, valid recall:98.879, valid auc:99.801
[1;31mEarlyStopping counter: 7 out of 50[0m
[98.43280658221235, 98.43851659076122, 98.081161675094, 98.7984850463628, 99.80330235304666]
====================================================================================================
fold:0 epoch:162 step:0 train loss:0.021350, train acc:99.216, train f1:99.215, train precision:99.333, train recall:99.097, train auc:99.971
fold:0 epoch:162 step:1 train loss:0.021843, train acc:99.173, train f1:99.174, train precision:98.947, train recall:99.401, train auc:99.968
fold:0 epoch:162 step:2 train loss:0.020702, train acc:99.210, train f1:99.208, train precision:99.059, train recall:99.357, train auc:99.973
fold:0 epoch:162 step:3 train loss:0.023183, train acc:99.142, train f1:99.145, train precision:99.045, train recall:99.244, train auc:99.965
fold:0 epoch:162 step:4 train loss:0.022634, train acc:99.188, train f1:99.193, train precision:99.133, train recall:99.254, train auc:99.965
fold:0 epoch:162 step:5 train loss:0.022196, train acc:99.149, train f1:99.142, train precision:99.084, train recall:99.200, train auc:99.967
fold:0 epoch:162 step:6 train loss:0.022806, train acc:99.078, train f1:99.077, train precision:99.302, train recall:98.854, train auc:99.971
fold:0 epoch:162 step:7 train loss:0.021890, train acc:99.170, train f1:99.171, train precision:98.804, train recall:99.541, train auc:99.971
fold:0 epoch:162 step:8 train loss:0.023789, train acc:99.118, train f1:99.124, train precision:98.900, train recall:99.350, train auc:99.961
fold:0 epoch:162 step:9 train loss:0.022131, train acc:99.182, train f1:99.186, train precision:99.334, train recall:99.039, train auc:99.970
fold:0 epoch:162        valid loss:0.061738, valid acc:98.428, valid f1:98.434, valid precision:98.061, valid recall:98.809, valid auc:99.803
[1;31mEarlyStopping counter: 8 out of 50[0m
[98.43280658221235, 98.43851659076122, 98.081161675094, 98.7984850463628, 99.80330235304666]
====================================================================================================
fold:0 epoch:163 step:0 train loss:0.021469, train acc:99.170, train f1:99.170, train precision:99.109, train recall:99.230, train auc:99.970
fold:0 epoch:163 step:1 train loss:0.020965, train acc:99.252, train f1:99.257, train precision:99.050, train recall:99.465, train auc:99.971
fold:0 epoch:163 step:2 train loss:0.022814, train acc:99.200, train f1:99.199, train precision:98.945, train recall:99.454, train auc:99.964
fold:0 epoch:163 step:3 train loss:0.022269, train acc:99.146, train f1:99.143, train precision:99.162, train recall:99.125, train auc:99.970
fold:0 epoch:163 step:4 train loss:0.023177, train acc:99.170, train f1:99.168, train precision:99.253, train recall:99.083, train auc:99.965
fold:0 epoch:163 step:5 train loss:0.021567, train acc:99.182, train f1:99.182, train precision:98.965, train recall:99.400, train auc:99.970
fold:0 epoch:163 step:6 train loss:0.021212, train acc:99.203, train f1:99.204, train precision:99.122, train recall:99.285, train auc:99.971
fold:0 epoch:163 step:7 train loss:0.021147, train acc:99.213, train f1:99.213, train precision:99.225, train recall:99.201, train auc:99.971
fold:0 epoch:163 step:8 train loss:0.024045, train acc:99.060, train f1:99.068, train precision:99.140, train recall:98.996, train auc:99.964
fold:0 epoch:163 step:9 train loss:0.022944, train acc:99.164, train f1:99.161, train precision:98.995, train recall:99.327, train auc:99.969
fold:0 epoch:163        valid loss:0.061302, valid acc:98.415, valid f1:98.420, valid precision:98.053, valid recall:98.791, valid auc:99.805
[1;31mEarlyStopping counter: 9 out of 50[0m
[98.43280658221235, 98.43851659076122, 98.081161675094, 98.7984850463628, 99.80330235304666]
====================================================================================================
fold:0 epoch:164 step:0 train loss:0.023207, train acc:99.121, train f1:99.117, train precision:98.916, train recall:99.318, train auc:99.964
fold:0 epoch:164 step:1 train loss:0.020714, train acc:99.234, train f1:99.236, train precision:99.263, train recall:99.209, train auc:99.973
fold:0 epoch:164 step:2 train loss:0.021464, train acc:99.152, train f1:99.149, train precision:99.112, train recall:99.185, train auc:99.969
fold:0 epoch:164 step:3 train loss:0.022457, train acc:99.152, train f1:99.148, train precision:99.178, train recall:99.117, train auc:99.968
fold:0 epoch:164 step:4 train loss:0.022800, train acc:99.118, train f1:99.125, train precision:99.122, train recall:99.128, train auc:99.966
fold:0 epoch:164 step:5 train loss:0.020699, train acc:99.197, train f1:99.202, train precision:98.947, train recall:99.459, train auc:99.974
fold:0 epoch:164 step:6 train loss:0.022510, train acc:99.158, train f1:99.157, train precision:98.849, train recall:99.467, train auc:99.970
fold:0 epoch:164 step:7 train loss:0.023787, train acc:99.091, train f1:99.093, train precision:99.359, train recall:98.829, train auc:99.964
fold:0 epoch:164 step:8 train loss:0.021821, train acc:99.185, train f1:99.181, train precision:99.160, train recall:99.203, train auc:99.970
fold:0 epoch:164 step:9 train loss:0.021035, train acc:99.279, train f1:99.289, train precision:99.152, train recall:99.427, train auc:99.970
fold:0 epoch:164        valid loss:0.061214, valid acc:98.430, valid f1:98.435, valid precision:98.143, valid recall:98.728, valid auc:99.804
[1;31mEarlyStopping counter: 10 out of 50[0m
[98.43280658221235, 98.43851659076122, 98.081161675094, 98.7984850463628, 99.80330235304666]
====================================================================================================
fold:0 epoch:165 step:0 train loss:0.023744, train acc:99.203, train f1:99.207, train precision:98.892, train recall:99.525, train auc:99.965
fold:0 epoch:165 step:1 train loss:0.020319, train acc:99.258, train f1:99.255, train precision:99.191, train recall:99.319, train auc:99.971
fold:0 epoch:165 step:2 train loss:0.022059, train acc:99.188, train f1:99.190, train precision:99.365, train recall:99.015, train auc:99.970
fold:0 epoch:165 step:3 train loss:0.021447, train acc:99.228, train f1:99.226, train precision:99.369, train recall:99.084, train auc:99.971
fold:0 epoch:165 step:4 train loss:0.021281, train acc:99.222, train f1:99.221, train precision:98.958, train recall:99.485, train auc:99.971
fold:0 epoch:165 step:5 train loss:0.022614, train acc:99.139, train f1:99.147, train precision:98.740, train recall:99.556, train auc:99.970
fold:0 epoch:165 step:6 train loss:0.022291, train acc:99.216, train f1:99.221, train precision:99.230, train recall:99.212, train auc:99.966
fold:0 epoch:165 step:7 train loss:0.021499, train acc:99.240, train f1:99.235, train precision:99.324, train recall:99.147, train auc:99.970
fold:0 epoch:165 step:8 train loss:0.020844, train acc:99.185, train f1:99.182, train precision:99.143, train recall:99.222, train auc:99.971
fold:0 epoch:165 step:9 train loss:0.022346, train acc:99.217, train f1:99.226, train precision:99.045, train recall:99.407, train auc:99.968
fold:0 epoch:165        valid loss:0.060738, valid acc:98.432, valid f1:98.438, valid precision:98.049, valid recall:98.830, valid auc:99.807
[1;31mEarlyStopping counter: 11 out of 50[0m
[98.43280658221235, 98.43851659076122, 98.081161675094, 98.7984850463628, 99.80330235304666]
====================================================================================================
fold:0 epoch:166 step:0 train loss:0.020523, train acc:99.237, train f1:99.238, train precision:99.117, train recall:99.359, train auc:99.974
fold:0 epoch:166 step:1 train loss:0.022922, train acc:99.213, train f1:99.218, train precision:99.290, train recall:99.146, train auc:99.959
fold:0 epoch:166 step:2 train loss:0.021156, train acc:99.231, train f1:99.225, train precision:99.085, train recall:99.366, train auc:99.971
fold:0 epoch:166 step:3 train loss:0.022288, train acc:99.158, train f1:99.158, train precision:99.068, train recall:99.249, train auc:99.968
fold:0 epoch:166 step:4 train loss:0.021142, train acc:99.210, train f1:99.210, train precision:99.081, train recall:99.341, train auc:99.971
fold:0 epoch:166 step:5 train loss:0.023776, train acc:99.115, train f1:99.112, train precision:99.015, train recall:99.209, train auc:99.960
fold:0 epoch:166 step:6 train loss:0.022328, train acc:99.158, train f1:99.160, train precision:99.057, train recall:99.262, train auc:99.966
fold:0 epoch:166 step:7 train loss:0.022884, train acc:99.097, train f1:99.104, train precision:99.116, train recall:99.092, train auc:99.967
fold:0 epoch:166 step:8 train loss:0.020604, train acc:99.255, train f1:99.256, train precision:99.009, train recall:99.505, train auc:99.974
fold:0 epoch:166 step:9 train loss:0.022718, train acc:99.173, train f1:99.169, train precision:99.064, train recall:99.275, train auc:99.967
fold:0 epoch:166        valid loss:0.061785, valid acc:98.409, valid f1:98.418, valid precision:97.877, valid recall:98.966, valid auc:99.810
[1;31mEarlyStopping counter: 12 out of 50[0m
[98.43280658221235, 98.43851659076122, 98.081161675094, 98.7984850463628, 99.80330235304666]
====================================================================================================
fold:0 epoch:167 step:0 train loss:0.022135, train acc:99.130, train f1:99.137, train precision:99.224, train recall:99.050, train auc:99.969
fold:0 epoch:167 step:1 train loss:0.019559, train acc:99.295, train f1:99.291, train precision:99.318, train recall:99.263, train auc:99.977
fold:0 epoch:167 step:2 train loss:0.021178, train acc:99.203, train f1:99.203, train precision:99.006, train recall:99.400, train auc:99.972
fold:0 epoch:167 step:3 train loss:0.020447, train acc:99.188, train f1:99.193, train precision:99.079, train recall:99.308, train auc:99.973
fold:0 epoch:167 step:4 train loss:0.021406, train acc:99.216, train f1:99.219, train precision:99.240, train recall:99.198, train auc:99.969
fold:0 epoch:167 step:5 train loss:0.020699, train acc:99.216, train f1:99.217, train precision:99.166, train recall:99.269, train auc:99.973
fold:0 epoch:167 step:6 train loss:0.020059, train acc:99.274, train f1:99.269, train precision:99.098, train recall:99.440, train auc:99.972
fold:0 epoch:167 step:7 train loss:0.022197, train acc:99.197, train f1:99.201, train precision:99.144, train recall:99.258, train auc:99.966
fold:0 epoch:167 step:8 train loss:0.020526, train acc:99.252, train f1:99.249, train precision:99.185, train recall:99.313, train auc:99.971
fold:0 epoch:167 step:9 train loss:0.022274, train acc:99.059, train f1:99.058, train precision:98.857, train recall:99.259, train auc:99.972
fold:0 epoch:167        valid loss:0.059416, valid acc:98.493, valid f1:98.499, valid precision:98.126, valid recall:98.874, valid auc:99.813
[1;31mTest score increased (98.432807 --> 98.492882).[0m
[98.49288232989421, 98.49860789466837, 98.12582626953883, 98.87423272822254, 99.81295037260074]
====================================================================================================
fold:0 epoch:168 step:0 train loss:0.022118, train acc:99.152, train f1:99.146, train precision:99.152, train recall:99.140, train auc:99.970
fold:0 epoch:168 step:1 train loss:0.020212, train acc:99.249, train f1:99.244, train precision:99.330, train recall:99.159, train auc:99.973
fold:0 epoch:168 step:2 train loss:0.020022, train acc:99.258, train f1:99.261, train precision:99.090, train recall:99.434, train auc:99.975
fold:0 epoch:168 step:3 train loss:0.020552, train acc:99.265, train f1:99.268, train precision:99.175, train recall:99.362, train auc:99.973
fold:0 epoch:168 step:4 train loss:0.020211, train acc:99.274, train f1:99.276, train precision:99.077, train recall:99.476, train auc:99.972
fold:0 epoch:168 step:5 train loss:0.021811, train acc:99.188, train f1:99.189, train precision:99.225, train recall:99.153, train auc:99.970
fold:0 epoch:168 step:6 train loss:0.022100, train acc:99.136, train f1:99.140, train precision:99.209, train recall:99.070, train auc:99.968
fold:0 epoch:168 step:7 train loss:0.022896, train acc:99.115, train f1:99.110, train precision:98.934, train recall:99.287, train auc:99.965
fold:0 epoch:168 step:8 train loss:0.021603, train acc:99.228, train f1:99.232, train precision:99.211, train recall:99.253, train auc:99.968
fold:0 epoch:168 step:9 train loss:0.019584, train acc:99.252, train f1:99.257, train precision:98.989, train recall:99.527, train auc:99.976
fold:0 epoch:168        valid loss:0.058020, valid acc:98.514, valid f1:98.518, valid precision:98.224, valid recall:98.814, valid auc:99.816
[1;31mTest score increased (98.492882 --> 98.513778).[0m
[98.51377824213138, 98.51822916666667, 98.22406854472283, 98.81415698054067, 99.81573524344313]
====================================================================================================
fold:0 epoch:169 step:0 train loss:0.020027, train acc:99.301, train f1:99.301, train precision:99.523, train recall:99.080, train auc:99.975
fold:0 epoch:169 step:1 train loss:0.021297, train acc:99.203, train f1:99.201, train precision:99.235, train recall:99.168, train auc:99.971
fold:0 epoch:169 step:2 train loss:0.019949, train acc:99.283, train f1:99.280, train precision:99.064, train recall:99.496, train auc:99.975
fold:0 epoch:169 step:3 train loss:0.021674, train acc:99.203, train f1:99.210, train precision:98.998, train recall:99.424, train auc:99.969
fold:0 epoch:169 step:4 train loss:0.020402, train acc:99.225, train f1:99.222, train precision:99.197, train recall:99.246, train auc:99.970
fold:0 epoch:169 step:5 train loss:0.022086, train acc:99.127, train f1:99.122, train precision:99.275, train recall:98.971, train auc:99.969
fold:0 epoch:169 step:6 train loss:0.020843, train acc:99.228, train f1:99.224, train precision:99.142, train recall:99.307, train auc:99.972
fold:0 epoch:169 step:7 train loss:0.020453, train acc:99.274, train f1:99.279, train precision:99.153, train recall:99.406, train auc:99.971
fold:0 epoch:169 step:8 train loss:0.020483, train acc:99.261, train f1:99.270, train precision:99.126, train recall:99.414, train auc:99.971
fold:0 epoch:169 step:9 train loss:0.020511, train acc:99.208, train f1:99.205, train precision:99.100, train recall:99.311, train auc:99.970
fold:0 epoch:169        valid loss:0.059212, valid acc:98.468, valid f1:98.476, valid precision:97.961, valid recall:98.997, valid auc:99.815
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.51377824213138, 98.51822916666667, 98.22406854472283, 98.81415698054067, 99.81573524344313]
====================================================================================================
fold:0 epoch:170 step:0 train loss:0.021470, train acc:99.219, train f1:99.222, train precision:99.192, train recall:99.252, train auc:99.969
fold:0 epoch:170 step:1 train loss:0.021856, train acc:99.179, train f1:99.179, train precision:99.345, train recall:99.012, train auc:99.970
fold:0 epoch:170 step:2 train loss:0.019783, train acc:99.252, train f1:99.256, train precision:99.169, train recall:99.344, train auc:99.974
fold:0 epoch:170 step:3 train loss:0.020023, train acc:99.268, train f1:99.263, train precision:99.074, train recall:99.452, train auc:99.974
fold:0 epoch:170 step:4 train loss:0.019781, train acc:99.228, train f1:99.225, train precision:99.004, train recall:99.448, train auc:99.976
fold:0 epoch:170 step:5 train loss:0.022920, train acc:99.179, train f1:99.175, train precision:99.367, train recall:98.984, train auc:99.968
fold:0 epoch:170 step:6 train loss:0.021423, train acc:99.210, train f1:99.215, train precision:99.369, train recall:99.062, train auc:99.972
fold:0 epoch:170 step:7 train loss:0.023889, train acc:99.097, train f1:99.101, train precision:98.777, train recall:99.427, train auc:99.965
fold:0 epoch:170 step:8 train loss:0.023228, train acc:99.100, train f1:99.100, train precision:98.735, train recall:99.467, train auc:99.967
fold:0 epoch:170 step:9 train loss:0.022892, train acc:99.120, train f1:99.120, train precision:99.348, train recall:98.894, train auc:99.970
fold:0 epoch:170        valid loss:0.062760, valid acc:98.361, valid f1:98.372, valid precision:97.690, valid recall:99.065, valid auc:99.812
[1;31mEarlyStopping counter: 2 out of 50[0m
[98.51377824213138, 98.51822916666667, 98.22406854472283, 98.81415698054067, 99.81573524344313]
====================================================================================================
fold:0 epoch:171 step:0 train loss:0.020604, train acc:99.176, train f1:99.165, train precision:99.337, train recall:98.994, train auc:99.975
fold:0 epoch:171 step:1 train loss:0.019029, train acc:99.283, train f1:99.283, train precision:99.232, train recall:99.335, train auc:99.977
fold:0 epoch:171 step:2 train loss:0.022661, train acc:99.149, train f1:99.146, train precision:98.690, train recall:99.606, train auc:99.971
fold:0 epoch:171 step:3 train loss:0.020970, train acc:99.194, train f1:99.199, train precision:99.169, train recall:99.229, train auc:99.972
fold:0 epoch:171 step:4 train loss:0.020613, train acc:99.228, train f1:99.219, train precision:99.277, train recall:99.161, train auc:99.972
fold:0 epoch:171 step:5 train loss:0.023213, train acc:99.161, train f1:99.166, train precision:99.319, train recall:99.012, train auc:99.965
fold:0 epoch:171 step:6 train loss:0.020578, train acc:99.240, train f1:99.250, train precision:99.211, train recall:99.289, train auc:99.971
fold:0 epoch:171 step:7 train loss:0.022627, train acc:99.179, train f1:99.183, train precision:98.861, train recall:99.506, train auc:99.967
fold:0 epoch:171 step:8 train loss:0.021213, train acc:99.271, train f1:99.271, train precision:99.099, train recall:99.444, train auc:99.969
fold:0 epoch:171 step:9 train loss:0.023381, train acc:99.041, train f1:99.051, train precision:99.284, train recall:98.819, train auc:99.970
fold:0 epoch:171        valid loss:0.061475, valid acc:98.458, valid f1:98.464, valid precision:98.067, valid recall:98.864, valid auc:99.817
[1;31mEarlyStopping counter: 3 out of 50[0m
[98.51377824213138, 98.51822916666667, 98.22406854472283, 98.81415698054067, 99.81573524344313]
====================================================================================================
fold:0 epoch:172 step:0 train loss:0.019622, train acc:99.298, train f1:99.305, train precision:99.341, train recall:99.269, train auc:99.975
fold:0 epoch:172 step:1 train loss:0.020000, train acc:99.280, train f1:99.285, train precision:99.015, train recall:99.556, train auc:99.976
fold:0 epoch:172 step:2 train loss:0.019557, train acc:99.271, train f1:99.274, train precision:98.965, train recall:99.586, train auc:99.977
fold:0 epoch:172 step:3 train loss:0.020634, train acc:99.231, train f1:99.223, train precision:99.088, train recall:99.357, train auc:99.971
fold:0 epoch:172 step:4 train loss:0.024789, train acc:99.075, train f1:99.072, train precision:99.416, train recall:98.730, train auc:99.965
fold:0 epoch:172 step:5 train loss:0.020578, train acc:99.222, train f1:99.227, train precision:99.248, train recall:99.206, train auc:99.973
fold:0 epoch:172 step:6 train loss:0.019964, train acc:99.225, train f1:99.226, train precision:98.864, train recall:99.590, train auc:99.974
fold:0 epoch:172 step:7 train loss:0.020351, train acc:99.249, train f1:99.246, train precision:98.889, train recall:99.606, train auc:99.977
fold:0 epoch:172 step:8 train loss:0.022322, train acc:99.246, train f1:99.242, train precision:99.337, train recall:99.148, train auc:99.969
fold:0 epoch:172 step:9 train loss:0.023104, train acc:99.200, train f1:99.207, train precision:99.493, train recall:98.923, train auc:99.970
fold:0 epoch:172        valid loss:0.059707, valid acc:98.475, valid f1:98.479, valid precision:98.183, valid recall:98.778, valid auc:99.812
[1;31mEarlyStopping counter: 4 out of 50[0m
[98.51377824213138, 98.51822916666667, 98.22406854472283, 98.81415698054067, 99.81573524344313]
====================================================================================================
fold:0 epoch:173 step:0 train loss:0.022534, train acc:99.152, train f1:99.147, train precision:98.892, train recall:99.403, train auc:99.969
fold:0 epoch:173 step:1 train loss:0.022657, train acc:99.155, train f1:99.153, train precision:98.866, train recall:99.442, train auc:99.970
fold:0 epoch:173 step:2 train loss:0.018741, train acc:99.292, train f1:99.291, train precision:99.146, train recall:99.437, train auc:99.978
fold:0 epoch:173 step:3 train loss:0.023363, train acc:99.109, train f1:99.113, train precision:99.415, train recall:98.812, train auc:99.968
fold:0 epoch:173 step:4 train loss:0.021341, train acc:99.234, train f1:99.237, train precision:99.391, train recall:99.083, train auc:99.972
fold:0 epoch:173 step:5 train loss:0.023297, train acc:99.240, train f1:99.244, train precision:98.898, train recall:99.592, train auc:99.965
fold:0 epoch:173 step:6 train loss:0.020194, train acc:99.255, train f1:99.263, train precision:99.059, train recall:99.467, train auc:99.974
fold:0 epoch:173 step:7 train loss:0.022076, train acc:99.182, train f1:99.183, train precision:99.068, train recall:99.298, train auc:99.967
fold:0 epoch:173 step:8 train loss:0.023143, train acc:99.100, train f1:99.090, train precision:99.215, train recall:98.964, train auc:99.966
fold:0 epoch:173 step:9 train loss:0.024451, train acc:99.103, train f1:99.109, train precision:99.422, train recall:98.799, train auc:99.964
fold:0 epoch:173        valid loss:0.060201, valid acc:98.459, valid f1:98.464, valid precision:98.130, valid recall:98.801, valid auc:99.809
[1;31mEarlyStopping counter: 5 out of 50[0m
[98.51377824213138, 98.51822916666667, 98.22406854472283, 98.81415698054067, 99.81573524344313]
====================================================================================================
fold:0 epoch:174 step:0 train loss:0.021942, train acc:99.243, train f1:99.239, train precision:98.898, train recall:99.581, train auc:99.971
fold:0 epoch:174 step:1 train loss:0.022752, train acc:99.149, train f1:99.153, train precision:98.701, train recall:99.610, train auc:99.973
fold:0 epoch:174 step:2 train loss:0.020744, train acc:99.286, train f1:99.288, train precision:99.132, train recall:99.446, train auc:99.970
fold:0 epoch:174 step:3 train loss:0.022403, train acc:99.188, train f1:99.187, train precision:99.564, train recall:98.812, train auc:99.972
fold:0 epoch:174 step:4 train loss:0.020590, train acc:99.216, train f1:99.219, train precision:99.409, train recall:99.029, train auc:99.975
fold:0 epoch:174 step:5 train loss:0.021182, train acc:99.243, train f1:99.242, train precision:99.042, train recall:99.443, train auc:99.971
fold:0 epoch:174 step:6 train loss:0.022473, train acc:99.173, train f1:99.177, train precision:98.778, train recall:99.579, train auc:99.969
fold:0 epoch:174 step:7 train loss:0.019947, train acc:99.261, train f1:99.265, train precision:99.169, train recall:99.362, train auc:99.972
fold:0 epoch:174 step:8 train loss:0.023571, train acc:99.109, train f1:99.107, train precision:99.174, train recall:99.040, train auc:99.964
fold:0 epoch:174 step:9 train loss:0.022626, train acc:99.235, train f1:99.231, train precision:99.205, train recall:99.257, train auc:99.968
fold:0 epoch:174        valid loss:0.058951, valid acc:98.515, valid f1:98.520, valid precision:98.219, valid recall:98.822, valid auc:99.813
[1;31mTest score increased (98.513778 --> 98.515084).[0m
[98.51508423664622, 98.51962762840961, 98.21910695742471, 98.82199294762962, 99.81348297002427]
====================================================================================================
fold:0 epoch:175 step:0 train loss:0.020685, train acc:99.246, train f1:99.242, train precision:99.105, train recall:99.379, train auc:99.973
fold:0 epoch:175 step:1 train loss:0.021332, train acc:99.286, train f1:99.285, train precision:99.242, train recall:99.327, train auc:99.967
fold:0 epoch:175 step:2 train loss:0.018613, train acc:99.310, train f1:99.309, train precision:99.164, train recall:99.455, train auc:99.978
fold:0 epoch:175 step:3 train loss:0.021417, train acc:99.194, train f1:99.199, train precision:99.120, train recall:99.277, train auc:99.970
fold:0 epoch:175 step:4 train loss:0.019713, train acc:99.252, train f1:99.249, train precision:99.234, train recall:99.265, train auc:99.975
fold:0 epoch:175 step:5 train loss:0.022380, train acc:99.219, train f1:99.222, train precision:99.300, train recall:99.143, train auc:99.966
fold:0 epoch:175 step:6 train loss:0.021135, train acc:99.255, train f1:99.262, train precision:99.298, train recall:99.226, train auc:99.968
fold:0 epoch:175 step:7 train loss:0.023154, train acc:99.091, train f1:99.090, train precision:98.800, train recall:99.381, train auc:99.967
fold:0 epoch:175 step:8 train loss:0.018281, train acc:99.347, train f1:99.350, train precision:99.193, train recall:99.507, train auc:99.979
fold:0 epoch:175 step:9 train loss:0.021475, train acc:99.226, train f1:99.224, train precision:99.452, train recall:98.997, train auc:99.970
fold:0 epoch:175        valid loss:0.058371, valid acc:98.509, valid f1:98.515, valid precision:98.084, valid recall:98.950, valid auc:99.817
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.51508423664622, 98.51962762840961, 98.21910695742471, 98.82199294762962, 99.81348297002427]
====================================================================================================
fold:0 epoch:176 step:0 train loss:0.018670, train acc:99.338, train f1:99.338, train precision:99.238, train recall:99.438, train auc:99.975
fold:0 epoch:176 step:1 train loss:0.020370, train acc:99.274, train f1:99.276, train precision:99.191, train recall:99.360, train auc:99.971
fold:0 epoch:176 step:2 train loss:0.018889, train acc:99.258, train f1:99.258, train precision:99.115, train recall:99.400, train auc:99.977
fold:0 epoch:176 step:3 train loss:0.019349, train acc:99.304, train f1:99.306, train precision:99.270, train recall:99.343, train auc:99.976
fold:0 epoch:176 step:4 train loss:0.020602, train acc:99.243, train f1:99.249, train precision:99.075, train recall:99.424, train auc:99.972
fold:0 epoch:176 step:5 train loss:0.021132, train acc:99.216, train f1:99.219, train precision:99.247, train recall:99.192, train auc:99.970
fold:0 epoch:176 step:6 train loss:0.020390, train acc:99.243, train f1:99.240, train precision:99.106, train recall:99.374, train auc:99.973
fold:0 epoch:176 step:7 train loss:0.023195, train acc:99.228, train f1:99.221, train precision:99.047, train recall:99.396, train auc:99.963
fold:0 epoch:176 step:8 train loss:0.020223, train acc:99.243, train f1:99.244, train precision:99.389, train recall:99.098, train auc:99.975
fold:0 epoch:176 step:9 train loss:0.021820, train acc:99.147, train f1:99.150, train precision:99.089, train recall:99.211, train auc:99.971
fold:0 epoch:176        valid loss:0.059937, valid acc:98.516, valid f1:98.523, valid precision:98.072, valid recall:98.979, valid auc:99.816
[1;31mTest score increased (98.515084 --> 98.516390).[0m
[98.51639023116104, 98.52321772138734, 98.07189627060741, 98.97871228940839, 99.81564126368897]
====================================================================================================
fold:0 epoch:177 step:0 train loss:0.020047, train acc:99.243, train f1:99.246, train precision:99.180, train recall:99.313, train auc:99.975
fold:0 epoch:177 step:1 train loss:0.020875, train acc:99.246, train f1:99.246, train precision:99.037, train recall:99.455, train auc:99.969
fold:0 epoch:177 step:2 train loss:0.019505, train acc:99.249, train f1:99.255, train precision:99.104, train recall:99.405, train auc:99.975
fold:0 epoch:177 step:3 train loss:0.021166, train acc:99.207, train f1:99.209, train precision:99.282, train recall:99.137, train auc:99.971
fold:0 epoch:177 step:4 train loss:0.018828, train acc:99.316, train f1:99.317, train precision:99.262, train recall:99.371, train auc:99.977
fold:0 epoch:177 step:5 train loss:0.020706, train acc:99.249, train f1:99.253, train precision:99.078, train recall:99.428, train auc:99.972
fold:0 epoch:177 step:6 train loss:0.019292, train acc:99.323, train f1:99.322, train precision:99.195, train recall:99.449, train auc:99.976
fold:0 epoch:177 step:7 train loss:0.018926, train acc:99.347, train f1:99.344, train precision:99.313, train recall:99.374, train auc:99.974
fold:0 epoch:177 step:8 train loss:0.021344, train acc:99.191, train f1:99.187, train precision:99.239, train recall:99.135, train auc:99.971
fold:0 epoch:177 step:9 train loss:0.020971, train acc:99.138, train f1:99.135, train precision:99.187, train recall:99.082, train auc:99.974
fold:0 epoch:177        valid loss:0.057973, valid acc:98.512, valid f1:98.517, valid precision:98.189, valid recall:98.848, valid auc:99.820
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.51639023116104, 98.52321772138734, 98.07189627060741, 98.97871228940839, 99.81564126368897]
====================================================================================================
fold:0 epoch:178 step:0 train loss:0.021482, train acc:99.216, train f1:99.221, train precision:98.846, train recall:99.598, train auc:99.973
fold:0 epoch:178 step:1 train loss:0.020647, train acc:99.182, train f1:99.181, train precision:98.934, train recall:99.430, train auc:99.974
fold:0 epoch:178 step:2 train loss:0.019738, train acc:99.255, train f1:99.254, train precision:99.552, train recall:98.957, train auc:99.978
fold:0 epoch:178 step:3 train loss:0.019403, train acc:99.289, train f1:99.285, train precision:99.416, train recall:99.154, train auc:99.978
fold:0 epoch:178 step:4 train loss:0.018973, train acc:99.301, train f1:99.309, train precision:99.342, train recall:99.276, train auc:99.975
fold:0 epoch:178 step:5 train loss:0.020775, train acc:99.219, train f1:99.216, train precision:98.846, train recall:99.588, train auc:99.974
fold:0 epoch:178 step:6 train loss:0.021454, train acc:99.268, train f1:99.276, train precision:98.954, train recall:99.601, train auc:99.968
fold:0 epoch:178 step:7 train loss:0.021126, train acc:99.225, train f1:99.220, train precision:99.299, train recall:99.141, train auc:99.970
fold:0 epoch:178 step:8 train loss:0.020884, train acc:99.222, train f1:99.217, train precision:99.397, train recall:99.038, train auc:99.972
fold:0 epoch:178 step:9 train loss:0.019364, train acc:99.296, train f1:99.298, train precision:99.280, train recall:99.315, train auc:99.975
fold:0 epoch:178        valid loss:0.058941, valid acc:98.523, valid f1:98.529, valid precision:98.149, valid recall:98.911, valid auc:99.818
[1;31mTest score increased (98.516390 --> 98.522920).[0m
[98.52292020373514, 98.52862736935226, 98.14939609144162, 98.91080057463759, 99.81766904318326]
====================================================================================================
fold:0 epoch:179 step:0 train loss:0.020461, train acc:99.246, train f1:99.241, train precision:98.982, train recall:99.501, train auc:99.974
fold:0 epoch:179 step:1 train loss:0.020703, train acc:99.231, train f1:99.228, train precision:98.900, train recall:99.557, train auc:99.975
fold:0 epoch:179 step:2 train loss:0.019826, train acc:99.301, train f1:99.305, train precision:99.416, train recall:99.193, train auc:99.974
fold:0 epoch:179 step:3 train loss:0.021887, train acc:99.203, train f1:99.205, train precision:99.541, train recall:98.870, train auc:99.973
fold:0 epoch:179 step:4 train loss:0.021226, train acc:99.149, train f1:99.151, train precision:98.998, train recall:99.306, train auc:99.973
fold:0 epoch:179 step:5 train loss:0.022867, train acc:99.170, train f1:99.175, train precision:98.750, train recall:99.604, train auc:99.970
fold:0 epoch:179 step:6 train loss:0.019180, train acc:99.319, train f1:99.328, train precision:99.242, train recall:99.415, train auc:99.974
fold:0 epoch:179 step:7 train loss:0.017685, train acc:99.347, train f1:99.345, train precision:99.278, train recall:99.412, train auc:99.980
fold:0 epoch:179 step:8 train loss:0.018983, train acc:99.286, train f1:99.279, train precision:99.414, train recall:99.145, train auc:99.976
fold:0 epoch:179 step:9 train loss:0.020834, train acc:99.305, train f1:99.309, train precision:99.335, train recall:99.283, train auc:99.971
fold:0 epoch:179        valid loss:0.059018, valid acc:98.505, valid f1:98.509, valid precision:98.219, valid recall:98.801, valid auc:99.814
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.52292020373514, 98.52862736935226, 98.14939609144162, 98.91080057463759, 99.81766904318326]
====================================================================================================
fold:0 epoch:180 step:0 train loss:0.020171, train acc:99.268, train f1:99.274, train precision:99.340, train recall:99.208, train auc:99.973
fold:0 epoch:180 step:1 train loss:0.018829, train acc:99.246, train f1:99.251, train precision:98.846, train recall:99.659, train auc:99.981
fold:0 epoch:180 step:2 train loss:0.019631, train acc:99.310, train f1:99.316, train precision:99.075, train recall:99.557, train auc:99.972
fold:0 epoch:180 step:3 train loss:0.019718, train acc:99.252, train f1:99.245, train precision:99.187, train recall:99.304, train auc:99.975
fold:0 epoch:180 step:4 train loss:0.022098, train acc:99.179, train f1:99.177, train precision:99.412, train recall:98.945, train auc:99.969
fold:0 epoch:180 step:5 train loss:0.021105, train acc:99.255, train f1:99.250, train precision:99.244, train recall:99.256, train auc:99.970
fold:0 epoch:180 step:6 train loss:0.019908, train acc:99.316, train f1:99.320, train precision:99.236, train recall:99.405, train auc:99.973
fold:0 epoch:180 step:7 train loss:0.021191, train acc:99.216, train f1:99.216, train precision:98.965, train recall:99.468, train auc:99.972
fold:0 epoch:180 step:8 train loss:0.021007, train acc:99.249, train f1:99.251, train precision:99.076, train recall:99.427, train auc:99.970
fold:0 epoch:180 step:9 train loss:0.017432, train acc:99.402, train f1:99.394, train precision:99.554, train recall:99.235, train auc:99.982
fold:0 epoch:180        valid loss:0.059352, valid acc:98.469, valid f1:98.476, valid precision:98.025, valid recall:98.932, valid auc:99.816
[1;31mEarlyStopping counter: 2 out of 50[0m
[98.52292020373514, 98.52862736935226, 98.14939609144162, 98.91080057463759, 99.81766904318326]
====================================================================================================
fold:0 epoch:181 step:0 train loss:0.021183, train acc:99.222, train f1:99.212, train precision:99.578, train recall:98.848, train auc:99.976
fold:0 epoch:181 step:1 train loss:0.019253, train acc:99.289, train f1:99.284, train precision:99.153, train recall:99.415, train auc:99.976
fold:0 epoch:181 step:2 train loss:0.020508, train acc:99.261, train f1:99.261, train precision:98.910, train recall:99.614, train auc:99.976
fold:0 epoch:181 step:3 train loss:0.018068, train acc:99.292, train f1:99.302, train precision:99.188, train recall:99.415, train auc:99.980
fold:0 epoch:181 step:4 train loss:0.018289, train acc:99.341, train f1:99.346, train precision:99.400, train recall:99.292, train auc:99.979
fold:0 epoch:181 step:5 train loss:0.017186, train acc:99.384, train f1:99.382, train precision:99.473, train recall:99.291, train auc:99.982
fold:0 epoch:181 step:6 train loss:0.019174, train acc:99.408, train f1:99.415, train precision:99.415, train recall:99.415, train auc:99.971
fold:0 epoch:181 step:7 train loss:0.019915, train acc:99.307, train f1:99.306, train precision:99.012, train recall:99.601, train auc:99.971
fold:0 epoch:181 step:8 train loss:0.021371, train acc:99.234, train f1:99.234, train precision:98.930, train recall:99.541, train auc:99.971
fold:0 epoch:181 step:9 train loss:0.018625, train acc:99.358, train f1:99.355, train precision:99.399, train recall:99.311, train auc:99.979
fold:0 epoch:181        valid loss:0.058253, valid acc:98.480, valid f1:98.487, valid precision:98.056, valid recall:98.921, valid auc:99.824
[1;31mEarlyStopping counter: 3 out of 50[0m
[98.52292020373514, 98.52862736935226, 98.14939609144162, 98.91080057463759, 99.81766904318326]
====================================================================================================
fold:0 epoch:182 step:0 train loss:0.019388, train acc:99.274, train f1:99.272, train precision:99.528, train recall:99.018, train auc:99.978
fold:0 epoch:182 step:1 train loss:0.016393, train acc:99.353, train f1:99.349, train precision:99.428, train recall:99.269, train auc:99.984
fold:0 epoch:182 step:2 train loss:0.020960, train acc:99.261, train f1:99.266, train precision:99.128, train recall:99.405, train auc:99.970
fold:0 epoch:182 step:3 train loss:0.020912, train acc:99.188, train f1:99.191, train precision:98.914, train recall:99.469, train auc:99.975
fold:0 epoch:182 step:4 train loss:0.019285, train acc:99.274, train f1:99.280, train precision:98.938, train recall:99.623, train auc:99.977
fold:0 epoch:182 step:5 train loss:0.021108, train acc:99.194, train f1:99.188, train precision:99.347, train recall:99.030, train auc:99.973
fold:0 epoch:182 step:6 train loss:0.020095, train acc:99.240, train f1:99.242, train precision:99.366, train recall:99.118, train auc:99.974
fold:0 epoch:182 step:7 train loss:0.018743, train acc:99.356, train f1:99.354, train precision:99.235, train recall:99.472, train auc:99.976
fold:0 epoch:182 step:8 train loss:0.020623, train acc:99.219, train f1:99.223, train precision:98.887, train recall:99.561, train auc:99.976
fold:0 epoch:182 step:9 train loss:0.023079, train acc:99.085, train f1:99.085, train precision:98.928, train recall:99.242, train auc:99.967
fold:0 epoch:182        valid loss:0.058443, valid acc:98.543, valid f1:98.546, valid precision:98.315, valid recall:98.778, valid auc:99.818
[1;31mTest score increased (98.522920 --> 98.542510).[0m
[98.5425101214575, 98.54592833876221, 98.31535161835436, 98.77758913412563, 99.81845895069233]
====================================================================================================
fold:0 epoch:183 step:0 train loss:0.019219, train acc:99.280, train f1:99.280, train precision:99.322, train recall:99.237, train auc:99.977
fold:0 epoch:183 step:1 train loss:0.019652, train acc:99.274, train f1:99.269, train precision:99.372, train recall:99.165, train auc:99.976
fold:0 epoch:183 step:2 train loss:0.019837, train acc:99.258, train f1:99.262, train precision:99.350, train recall:99.175, train auc:99.974
fold:0 epoch:183 step:3 train loss:0.019170, train acc:99.323, train f1:99.322, train precision:99.122, train recall:99.523, train auc:99.976
fold:0 epoch:183 step:4 train loss:0.019506, train acc:99.255, train f1:99.259, train precision:99.049, train recall:99.471, train auc:99.976
fold:0 epoch:183 step:5 train loss:0.020354, train acc:99.274, train f1:99.272, train precision:99.223, train recall:99.320, train auc:99.972
fold:0 epoch:183 step:6 train loss:0.019931, train acc:99.265, train f1:99.266, train precision:99.360, train recall:99.173, train auc:99.972
fold:0 epoch:183 step:7 train loss:0.019131, train acc:99.295, train f1:99.288, train precision:99.120, train recall:99.457, train auc:99.975
fold:0 epoch:183 step:8 train loss:0.020277, train acc:99.265, train f1:99.270, train precision:99.261, train recall:99.279, train auc:99.972
fold:0 epoch:183 step:9 train loss:0.018541, train acc:99.349, train f1:99.354, train precision:99.199, train recall:99.511, train auc:99.974
fold:0 epoch:183        valid loss:0.060731, valid acc:98.510, valid f1:98.516, valid precision:98.119, valid recall:98.916, valid auc:99.817
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.5425101214575, 98.54592833876221, 98.31535161835436, 98.77758913412563, 99.81845895069233]
====================================================================================================
fold:0 epoch:184 step:0 train loss:0.016074, train acc:99.414, train f1:99.416, train precision:99.234, train recall:99.598, train auc:99.984
fold:0 epoch:184 step:1 train loss:0.018367, train acc:99.353, train f1:99.352, train precision:99.230, train recall:99.473, train auc:99.977
fold:0 epoch:184 step:2 train loss:0.019165, train acc:99.252, train f1:99.249, train precision:99.258, train recall:99.240, train auc:99.977
fold:0 epoch:184 step:3 train loss:0.020864, train acc:99.258, train f1:99.260, train precision:99.415, train recall:99.106, train auc:99.971
fold:0 epoch:184 step:4 train loss:0.018451, train acc:99.347, train f1:99.350, train precision:99.296, train recall:99.405, train auc:99.976
fold:0 epoch:184 step:5 train loss:0.019159, train acc:99.307, train f1:99.307, train precision:98.978, train recall:99.639, train auc:99.979
fold:0 epoch:184 step:6 train loss:0.020894, train acc:99.249, train f1:99.251, train precision:99.082, train recall:99.420, train auc:99.972
fold:0 epoch:184 step:7 train loss:0.021193, train acc:99.231, train f1:99.233, train precision:99.457, train recall:99.010, train auc:99.971
fold:0 epoch:184 step:8 train loss:0.018944, train acc:99.304, train f1:99.307, train precision:99.398, train recall:99.216, train auc:99.975
fold:0 epoch:184 step:9 train loss:0.016969, train acc:99.446, train f1:99.434, train precision:99.194, train recall:99.676, train auc:99.983
fold:0 epoch:184        valid loss:0.057510, valid acc:98.571, valid f1:98.573, valid precision:98.462, valid recall:98.684, valid auc:99.820
[1;31mTest score increased (98.542510 --> 98.571242).[0m
[98.5712420007836, 98.57284491755375, 98.46238044356414, 98.68355752905838, 99.81962429964403]
====================================================================================================
fold:0 epoch:185 step:0 train loss:0.016844, train acc:99.448, train f1:99.447, train precision:99.335, train recall:99.560, train auc:99.980
fold:0 epoch:185 step:1 train loss:0.020181, train acc:99.268, train f1:99.271, train precision:99.343, train recall:99.198, train auc:99.973
fold:0 epoch:185 step:2 train loss:0.018529, train acc:99.332, train f1:99.332, train precision:99.395, train recall:99.268, train auc:99.978
fold:0 epoch:185 step:3 train loss:0.017925, train acc:99.350, train f1:99.357, train precision:99.384, train recall:99.330, train auc:99.978
fold:0 epoch:185 step:4 train loss:0.017147, train acc:99.423, train f1:99.424, train precision:99.209, train recall:99.639, train auc:99.980
fold:0 epoch:185 step:5 train loss:0.018839, train acc:99.332, train f1:99.328, train precision:99.100, train recall:99.557, train auc:99.977
fold:0 epoch:185 step:6 train loss:0.018552, train acc:99.368, train f1:99.362, train precision:99.420, train recall:99.304, train auc:99.978
fold:0 epoch:185 step:7 train loss:0.019250, train acc:99.298, train f1:99.301, train precision:99.452, train recall:99.150, train auc:99.976
fold:0 epoch:185 step:8 train loss:0.018073, train acc:99.344, train f1:99.344, train precision:99.213, train recall:99.474, train auc:99.979
fold:0 epoch:185 step:9 train loss:0.019765, train acc:99.288, train f1:99.288, train precision:99.105, train recall:99.471, train auc:99.976
fold:0 epoch:185        valid loss:0.059583, valid acc:98.462, valid f1:98.470, valid precision:97.938, valid recall:99.007, valid auc:99.822
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.5712420007836, 98.57284491755375, 98.46238044356414, 98.68355752905838, 99.81962429964403]
====================================================================================================
fold:0 epoch:186 step:0 train loss:0.019475, train acc:99.261, train f1:99.264, train precision:99.337, train recall:99.192, train auc:99.975
fold:0 epoch:186 step:1 train loss:0.019307, train acc:99.268, train f1:99.268, train precision:99.365, train recall:99.171, train auc:99.976
fold:0 epoch:186 step:2 train loss:0.017943, train acc:99.353, train f1:99.356, train precision:99.248, train recall:99.465, train auc:99.976
fold:0 epoch:186 step:3 train loss:0.018194, train acc:99.323, train f1:99.327, train precision:99.135, train recall:99.520, train auc:99.979
fold:0 epoch:186 step:4 train loss:0.019553, train acc:99.255, train f1:99.246, train precision:99.002, train recall:99.492, train auc:99.976
fold:0 epoch:186 step:5 train loss:0.017210, train acc:99.377, train f1:99.378, train precision:99.499, train recall:99.257, train auc:99.982
fold:0 epoch:186 step:6 train loss:0.020602, train acc:99.210, train f1:99.209, train precision:99.290, train recall:99.127, train auc:99.973
fold:0 epoch:186 step:7 train loss:0.018701, train acc:99.283, train f1:99.286, train precision:99.151, train recall:99.422, train auc:99.975
fold:0 epoch:186 step:8 train loss:0.018923, train acc:99.341, train f1:99.340, train precision:99.055, train recall:99.626, train auc:99.976
fold:0 epoch:186 step:9 train loss:0.017922, train acc:99.376, train f1:99.375, train precision:99.314, train recall:99.437, train auc:99.978
fold:0 epoch:186        valid loss:0.060754, valid acc:98.479, valid f1:98.486, valid precision:98.008, valid recall:98.968, valid auc:99.820
[1;31mEarlyStopping counter: 2 out of 50[0m
[98.5712420007836, 98.57284491755375, 98.46238044356414, 98.68355752905838, 99.81962429964403]
====================================================================================================
fold:0 epoch:187 step:0 train loss:0.018280, train acc:99.350, train f1:99.353, train precision:99.465, train recall:99.242, train auc:99.979
fold:0 epoch:187 step:1 train loss:0.016510, train acc:99.402, train f1:99.406, train precision:99.461, train recall:99.352, train auc:99.981
fold:0 epoch:187 step:2 train loss:0.018941, train acc:99.329, train f1:99.332, train precision:99.097, train recall:99.568, train auc:99.977
fold:0 epoch:187 step:3 train loss:0.018077, train acc:99.368, train f1:99.368, train precision:99.075, train recall:99.663, train auc:99.979
fold:0 epoch:187 step:4 train loss:0.018207, train acc:99.307, train f1:99.307, train precision:99.401, train recall:99.213, train auc:99.979
fold:0 epoch:187 step:5 train loss:0.018889, train acc:99.350, train f1:99.350, train precision:99.414, train recall:99.287, train auc:99.977
fold:0 epoch:187 step:6 train loss:0.018587, train acc:99.332, train f1:99.328, train precision:99.215, train recall:99.441, train auc:99.977
fold:0 epoch:187 step:7 train loss:0.018166, train acc:99.329, train f1:99.328, train precision:99.279, train recall:99.376, train auc:99.976
fold:0 epoch:187 step:8 train loss:0.018183, train acc:99.344, train f1:99.340, train precision:99.185, train recall:99.496, train auc:99.975
fold:0 epoch:187 step:9 train loss:0.019124, train acc:99.235, train f1:99.232, train precision:99.504, train recall:98.961, train auc:99.978
fold:0 epoch:187        valid loss:0.059277, valid acc:98.511, valid f1:98.515, valid precision:98.259, valid recall:98.772, valid auc:99.820
[1;31mEarlyStopping counter: 3 out of 50[0m
[98.5712420007836, 98.57284491755375, 98.46238044356414, 98.68355752905838, 99.81962429964403]
====================================================================================================
fold:0 epoch:188 step:0 train loss:0.019010, train acc:99.280, train f1:99.279, train precision:98.947, train recall:99.614, train auc:99.978
fold:0 epoch:188 step:1 train loss:0.018138, train acc:99.326, train f1:99.330, train precision:99.128, train recall:99.532, train auc:99.979
fold:0 epoch:188 step:2 train loss:0.016890, train acc:99.374, train f1:99.374, train precision:99.341, train recall:99.407, train auc:99.979
fold:0 epoch:188 step:3 train loss:0.018578, train acc:99.347, train f1:99.347, train precision:99.553, train recall:99.141, train auc:99.979
fold:0 epoch:188 step:4 train loss:0.018837, train acc:99.295, train f1:99.294, train precision:99.340, train recall:99.249, train auc:99.977
fold:0 epoch:188 step:5 train loss:0.020172, train acc:99.225, train f1:99.230, train precision:98.835, train recall:99.629, train auc:99.978
fold:0 epoch:188 step:6 train loss:0.021666, train acc:99.265, train f1:99.266, train precision:99.214, train recall:99.317, train auc:99.962
fold:0 epoch:188 step:7 train loss:0.019359, train acc:99.277, train f1:99.278, train precision:99.463, train recall:99.094, train auc:99.976
fold:0 epoch:188 step:8 train loss:0.018887, train acc:99.307, train f1:99.303, train precision:99.379, train recall:99.227, train auc:99.976
fold:0 epoch:188 step:9 train loss:0.021485, train acc:99.120, train f1:99.118, train precision:99.048, train recall:99.188, train auc:99.970
fold:0 epoch:188        valid loss:0.058590, valid acc:98.522, valid f1:98.526, valid precision:98.234, valid recall:98.819, valid auc:99.820
[1;31mEarlyStopping counter: 4 out of 50[0m
[98.5712420007836, 98.57284491755375, 98.46238044356414, 98.68355752905838, 99.81962429964403]
====================================================================================================
fold:0 epoch:189 step:0 train loss:0.017807, train acc:99.298, train f1:99.297, train precision:98.946, train recall:99.650, train auc:99.983
fold:0 epoch:189 step:1 train loss:0.019534, train acc:99.268, train f1:99.268, train precision:99.069, train recall:99.468, train auc:99.976
fold:0 epoch:189 step:2 train loss:0.020762, train acc:99.277, train f1:99.281, train precision:99.374, train recall:99.187, train auc:99.970
fold:0 epoch:189 step:3 train loss:0.016490, train acc:99.414, train f1:99.416, train precision:99.488, train recall:99.343, train auc:99.983
fold:0 epoch:189 step:4 train loss:0.018922, train acc:99.338, train f1:99.336, train precision:99.285, train recall:99.388, train auc:99.975
fold:0 epoch:189 step:5 train loss:0.018838, train acc:99.323, train f1:99.323, train precision:99.263, train recall:99.384, train auc:99.976
fold:0 epoch:189 step:6 train loss:0.018566, train acc:99.310, train f1:99.310, train precision:99.159, train recall:99.462, train auc:99.978
fold:0 epoch:189 step:7 train loss:0.016767, train acc:99.442, train f1:99.441, train precision:99.426, train recall:99.456, train auc:99.980
fold:0 epoch:189 step:8 train loss:0.018979, train acc:99.277, train f1:99.277, train precision:99.195, train recall:99.359, train auc:99.977
fold:0 epoch:189 step:9 train loss:0.014910, train acc:99.437, train f1:99.441, train precision:99.319, train recall:99.563, train auc:99.988
fold:0 epoch:189        valid loss:0.060637, valid acc:98.518, valid f1:98.524, valid precision:98.134, valid recall:98.916, valid auc:99.814
[1;31mEarlyStopping counter: 5 out of 50[0m
[98.5712420007836, 98.57284491755375, 98.46238044356414, 98.68355752905838, 99.81962429964403]
====================================================================================================
fold:0 epoch:190 step:0 train loss:0.017085, train acc:99.405, train f1:99.405, train precision:99.402, train recall:99.408, train auc:99.979
fold:0 epoch:190 step:1 train loss:0.017561, train acc:99.411, train f1:99.413, train precision:99.386, train recall:99.440, train auc:99.979
fold:0 epoch:190 step:2 train loss:0.018325, train acc:99.341, train f1:99.341, train precision:99.305, train recall:99.378, train auc:99.978
fold:0 epoch:190 step:3 train loss:0.018267, train acc:99.280, train f1:99.273, train precision:99.194, train recall:99.353, train auc:99.978
fold:0 epoch:190 step:4 train loss:0.017655, train acc:99.332, train f1:99.335, train precision:99.289, train recall:99.380, train auc:99.980
fold:0 epoch:190 step:5 train loss:0.018630, train acc:99.301, train f1:99.299, train precision:99.216, train recall:99.381, train auc:99.978
fold:0 epoch:190 step:6 train loss:0.019586, train acc:99.292, train f1:99.293, train precision:99.215, train recall:99.372, train auc:99.976
fold:0 epoch:190 step:7 train loss:0.017742, train acc:99.319, train f1:99.321, train precision:99.360, train recall:99.281, train auc:99.979
fold:0 epoch:190 step:8 train loss:0.020199, train acc:99.237, train f1:99.241, train precision:99.241, train recall:99.241, train auc:99.974
fold:0 epoch:190 step:9 train loss:0.017324, train acc:99.296, train f1:99.294, train precision:99.137, train recall:99.452, train auc:99.982
fold:0 epoch:190        valid loss:0.059767, valid acc:98.516, valid f1:98.519, valid precision:98.317, valid recall:98.723, valid auc:99.815
[1;31mEarlyStopping counter: 6 out of 50[0m
[98.5712420007836, 98.57284491755375, 98.46238044356414, 98.68355752905838, 99.81962429964403]
====================================================================================================
fold:0 epoch:191 step:0 train loss:0.019799, train acc:99.292, train f1:99.297, train precision:99.207, train recall:99.388, train auc:99.972
fold:0 epoch:191 step:1 train loss:0.017783, train acc:99.338, train f1:99.335, train precision:99.405, train recall:99.265, train auc:99.979
fold:0 epoch:191 step:2 train loss:0.018011, train acc:99.304, train f1:99.305, train precision:99.287, train recall:99.323, train auc:99.977
fold:0 epoch:191 step:3 train loss:0.015161, train acc:99.408, train f1:99.410, train precision:99.476, train recall:99.343, train auc:99.986
fold:0 epoch:191 step:4 train loss:0.017910, train acc:99.295, train f1:99.297, train precision:99.004, train recall:99.591, train auc:99.981
fold:0 epoch:191 step:5 train loss:0.018938, train acc:99.316, train f1:99.315, train precision:99.248, train recall:99.382, train auc:99.974
fold:0 epoch:191 step:6 train loss:0.018910, train acc:99.316, train f1:99.313, train precision:99.411, train recall:99.216, train auc:99.975
fold:0 epoch:191 step:7 train loss:0.017483, train acc:99.344, train f1:99.342, train precision:99.406, train recall:99.278, train auc:99.981
fold:0 epoch:191 step:8 train loss:0.017802, train acc:99.356, train f1:99.357, train precision:99.281, train recall:99.433, train auc:99.979
fold:0 epoch:191 step:9 train loss:0.018147, train acc:99.420, train f1:99.423, train precision:99.249, train recall:99.597, train auc:99.974
fold:0 epoch:191        valid loss:0.060018, valid acc:98.544, valid f1:98.548, valid precision:98.243, valid recall:98.856, valid auc:99.822
[1;31mEarlyStopping counter: 7 out of 50[0m
[98.5712420007836, 98.57284491755375, 98.46238044356414, 98.68355752905838, 99.81962429964403]
====================================================================================================
fold:0 epoch:192 step:0 train loss:0.016784, train acc:99.387, train f1:99.388, train precision:99.451, train recall:99.324, train auc:99.982
fold:0 epoch:192 step:1 train loss:0.017665, train acc:99.368, train f1:99.367, train precision:99.285, train recall:99.449, train auc:99.980
fold:0 epoch:192 step:2 train loss:0.015423, train acc:99.460, train f1:99.457, train precision:99.338, train recall:99.576, train auc:99.981
fold:0 epoch:192 step:3 train loss:0.017796, train acc:99.332, train f1:99.336, train precision:99.526, train recall:99.147, train auc:99.982
fold:0 epoch:192 step:4 train loss:0.017706, train acc:99.368, train f1:99.370, train precision:99.282, train recall:99.458, train auc:99.979
fold:0 epoch:192 step:5 train loss:0.019528, train acc:99.292, train f1:99.299, train precision:99.167, train recall:99.431, train auc:99.975
fold:0 epoch:192 step:6 train loss:0.017410, train acc:99.371, train f1:99.375, train precision:99.182, train recall:99.568, train auc:99.981
fold:0 epoch:192 step:7 train loss:0.017493, train acc:99.335, train f1:99.329, train precision:99.299, train recall:99.360, train auc:99.980
fold:0 epoch:192 step:8 train loss:0.017677, train acc:99.362, train f1:99.360, train precision:99.345, train recall:99.375, train auc:99.979
fold:0 epoch:192 step:9 train loss:0.021153, train acc:99.182, train f1:99.176, train precision:99.185, train recall:99.167, train auc:99.971
fold:0 epoch:192        valid loss:0.061350, valid acc:98.514, valid f1:98.520, valid precision:98.114, valid recall:98.929, valid auc:99.820
[1;31mEarlyStopping counter: 8 out of 50[0m
[98.5712420007836, 98.57284491755375, 98.46238044356414, 98.68355752905838, 99.81962429964403]
====================================================================================================
fold:0 epoch:193 step:0 train loss:0.016633, train acc:99.307, train f1:99.300, train precision:99.279, train recall:99.321, train auc:99.983
fold:0 epoch:193 step:1 train loss:0.017942, train acc:99.298, train f1:99.302, train precision:99.062, train recall:99.544, train auc:99.979
fold:0 epoch:193 step:2 train loss:0.018280, train acc:99.347, train f1:99.346, train precision:99.297, train recall:99.394, train auc:99.979
fold:0 epoch:193 step:3 train loss:0.018148, train acc:99.374, train f1:99.375, train precision:99.390, train recall:99.360, train auc:99.976
fold:0 epoch:193 step:4 train loss:0.019108, train acc:99.313, train f1:99.315, train precision:99.336, train recall:99.294, train auc:99.974
fold:0 epoch:193 step:5 train loss:0.017044, train acc:99.438, train f1:99.445, train precision:99.313, train recall:99.577, train auc:99.980
fold:0 epoch:193 step:6 train loss:0.018531, train acc:99.338, train f1:99.340, train precision:99.108, train recall:99.573, train auc:99.980
fold:0 epoch:193 step:7 train loss:0.017106, train acc:99.326, train f1:99.324, train precision:99.290, train recall:99.357, train auc:99.981
fold:0 epoch:193 step:8 train loss:0.019321, train acc:99.295, train f1:99.296, train precision:99.414, train recall:99.178, train auc:99.977
fold:0 epoch:193 step:9 train loss:0.021343, train acc:99.296, train f1:99.289, train precision:99.201, train recall:99.377, train auc:99.969
fold:0 epoch:193        valid loss:0.059159, valid acc:98.550, valid f1:98.557, valid precision:98.113, valid recall:99.005, valid auc:99.826
[1;31mEarlyStopping counter: 9 out of 50[0m
[98.5712420007836, 98.57284491755375, 98.46238044356414, 98.68355752905838, 99.81962429964403]
====================================================================================================
fold:0 epoch:194 step:0 train loss:0.018179, train acc:99.292, train f1:99.292, train precision:99.292, train recall:99.292, train auc:99.979
fold:0 epoch:194 step:1 train loss:0.017016, train acc:99.365, train f1:99.367, train precision:99.247, train recall:99.488, train auc:99.980
fold:0 epoch:194 step:2 train loss:0.017117, train acc:99.396, train f1:99.392, train precision:99.264, train recall:99.521, train auc:99.979
fold:0 epoch:194 step:3 train loss:0.020848, train acc:99.243, train f1:99.241, train precision:99.473, train recall:99.011, train auc:99.974
fold:0 epoch:194 step:4 train loss:0.019083, train acc:99.277, train f1:99.278, train precision:99.196, train recall:99.359, train auc:99.975
fold:0 epoch:194 step:5 train loss:0.018432, train acc:99.295, train f1:99.297, train precision:98.992, train recall:99.603, train auc:99.979
fold:0 epoch:194 step:6 train loss:0.018330, train acc:99.310, train f1:99.315, train precision:99.327, train recall:99.303, train auc:99.979
fold:0 epoch:194 step:7 train loss:0.017342, train acc:99.393, train f1:99.393, train precision:99.396, train recall:99.390, train auc:99.980
fold:0 epoch:194 step:8 train loss:0.016713, train acc:99.402, train f1:99.399, train precision:99.357, train recall:99.442, train auc:99.982
fold:0 epoch:194 step:9 train loss:0.021993, train acc:99.226, train f1:99.233, train precision:99.233, train recall:99.233, train auc:99.960
fold:0 epoch:194        valid loss:0.058824, valid acc:98.550, valid f1:98.556, valid precision:98.145, valid recall:98.971, valid auc:99.822
[1;31mEarlyStopping counter: 10 out of 50[0m
[98.5712420007836, 98.57284491755375, 98.46238044356414, 98.68355752905838, 99.81962429964403]
====================================================================================================
fold:0 epoch:195 step:0 train loss:0.017573, train acc:99.359, train f1:99.359, train precision:99.123, train recall:99.596, train auc:99.980
fold:0 epoch:195 step:1 train loss:0.017692, train acc:99.347, train f1:99.346, train precision:99.182, train recall:99.510, train auc:99.973
fold:0 epoch:195 step:2 train loss:0.019714, train acc:99.286, train f1:99.285, train precision:99.498, train recall:99.073, train auc:99.971
fold:0 epoch:195 step:3 train loss:0.019965, train acc:99.347, train f1:99.346, train precision:99.280, train recall:99.413, train auc:99.960
fold:0 epoch:195 step:4 train loss:0.018247, train acc:99.304, train f1:99.302, train precision:98.993, train recall:99.613, train auc:99.980
fold:0 epoch:195 step:5 train loss:0.018211, train acc:99.411, train f1:99.415, train precision:99.339, train recall:99.490, train auc:99.976
fold:0 epoch:195 step:6 train loss:0.018971, train acc:99.329, train f1:99.325, train precision:99.441, train recall:99.209, train auc:99.976
fold:0 epoch:195 step:7 train loss:0.020366, train acc:99.219, train f1:99.218, train precision:99.242, train recall:99.194, train auc:99.975
fold:0 epoch:195 step:8 train loss:0.021018, train acc:99.261, train f1:99.268, train precision:99.244, train recall:99.292, train auc:99.968
fold:0 epoch:195 step:9 train loss:0.023231, train acc:99.226, train f1:99.236, train precision:99.098, train recall:99.374, train auc:99.959
fold:0 epoch:195        valid loss:0.058812, valid acc:98.529, valid f1:98.535, valid precision:98.152, valid recall:98.921, valid auc:99.822
[1;31mEarlyStopping counter: 11 out of 50[0m
[98.5712420007836, 98.57284491755375, 98.46238044356414, 98.68355752905838, 99.81962429964403]
====================================================================================================
fold:0 epoch:196 step:0 train loss:0.016262, train acc:99.426, train f1:99.429, train precision:99.212, train recall:99.647, train auc:99.984
fold:0 epoch:196 step:1 train loss:0.017932, train acc:99.329, train f1:99.325, train precision:99.209, train recall:99.441, train auc:99.977
fold:0 epoch:196 step:2 train loss:0.020000, train acc:99.246, train f1:99.242, train precision:99.459, train recall:99.026, train auc:99.976
fold:0 epoch:196 step:3 train loss:0.015425, train acc:99.417, train f1:99.421, train precision:99.333, train recall:99.508, train auc:99.984
fold:0 epoch:196 step:4 train loss:0.016236, train acc:99.393, train f1:99.393, train precision:99.209, train recall:99.578, train auc:99.984
fold:0 epoch:196 step:5 train loss:0.021226, train acc:99.185, train f1:99.186, train precision:98.876, train recall:99.499, train auc:99.972
fold:0 epoch:196 step:6 train loss:0.020556, train acc:99.255, train f1:99.262, train precision:99.370, train recall:99.154, train auc:99.971
fold:0 epoch:196 step:7 train loss:0.019392, train acc:99.286, train f1:99.282, train precision:99.410, train recall:99.155, train auc:99.975
fold:0 epoch:196 step:8 train loss:0.018629, train acc:99.277, train f1:99.274, train precision:99.289, train recall:99.259, train auc:99.977
fold:0 epoch:196 step:9 train loss:0.018419, train acc:99.314, train f1:99.320, train precision:99.199, train recall:99.442, train auc:99.977
fold:0 epoch:196        valid loss:0.057998, valid acc:98.569, valid f1:98.575, valid precision:98.159, valid recall:98.994, valid auc:99.824
[1;31mEarlyStopping counter: 12 out of 50[0m
[98.5712420007836, 98.57284491755375, 98.46238044356414, 98.68355752905838, 99.81962429964403]
====================================================================================================
fold:0 epoch:197 step:0 train loss:0.018210, train acc:99.310, train f1:99.313, train precision:99.181, train recall:99.446, train auc:99.980
fold:0 epoch:197 step:1 train loss:0.017249, train acc:99.396, train f1:99.395, train precision:99.371, train recall:99.420, train auc:99.979
fold:0 epoch:197 step:2 train loss:0.016801, train acc:99.371, train f1:99.370, train precision:99.407, train recall:99.334, train auc:99.982
fold:0 epoch:197 step:3 train loss:0.016750, train acc:99.390, train f1:99.388, train precision:99.290, train recall:99.485, train auc:99.981
fold:0 epoch:197 step:4 train loss:0.017836, train acc:99.362, train f1:99.361, train precision:99.280, train recall:99.443, train auc:99.979
fold:0 epoch:197 step:5 train loss:0.017022, train acc:99.393, train f1:99.398, train precision:99.461, train recall:99.335, train auc:99.982
fold:0 epoch:197 step:6 train loss:0.017935, train acc:99.377, train f1:99.376, train precision:99.230, train recall:99.522, train auc:99.980
fold:0 epoch:197 step:7 train loss:0.018141, train acc:99.316, train f1:99.317, train precision:99.165, train recall:99.468, train auc:99.980
fold:0 epoch:197 step:8 train loss:0.018259, train acc:99.347, train f1:99.348, train precision:99.493, train recall:99.203, train auc:99.977
fold:0 epoch:197 step:9 train loss:0.014923, train acc:99.499, train f1:99.497, train precision:99.366, train recall:99.629, train auc:99.984
fold:0 epoch:197        valid loss:0.060126, valid acc:98.545, valid f1:98.552, valid precision:98.083, valid recall:99.026, valid auc:99.824
[1;31mEarlyStopping counter: 13 out of 50[0m
[98.5712420007836, 98.57284491755375, 98.46238044356414, 98.68355752905838, 99.81962429964403]
====================================================================================================
fold:0 epoch:198 step:0 train loss:0.016353, train acc:99.368, train f1:99.370, train precision:99.289, train recall:99.452, train auc:99.984
fold:0 epoch:198 step:1 train loss:0.020134, train acc:99.261, train f1:99.269, train precision:99.365, train recall:99.173, train auc:99.975
fold:0 epoch:198 step:2 train loss:0.016405, train acc:99.387, train f1:99.385, train precision:99.145, train recall:99.626, train auc:99.982
fold:0 epoch:198 step:3 train loss:0.015329, train acc:99.475, train f1:99.478, train precision:99.417, train recall:99.538, train auc:99.984
fold:0 epoch:198 step:4 train loss:0.017667, train acc:99.359, train f1:99.359, train precision:99.299, train recall:99.420, train auc:99.978
fold:0 epoch:198 step:5 train loss:0.018400, train acc:99.350, train f1:99.354, train precision:99.490, train recall:99.218, train auc:99.976
fold:0 epoch:198 step:6 train loss:0.018962, train acc:99.286, train f1:99.284, train precision:99.108, train recall:99.460, train auc:99.978
fold:0 epoch:198 step:7 train loss:0.018006, train acc:99.323, train f1:99.322, train precision:99.177, train recall:99.468, train auc:99.979
fold:0 epoch:198 step:8 train loss:0.015563, train acc:99.374, train f1:99.367, train precision:99.401, train recall:99.334, train auc:99.985
fold:0 epoch:198 step:9 train loss:0.016590, train acc:99.393, train f1:99.390, train precision:99.451, train recall:99.328, train auc:99.982
fold:0 epoch:198        valid loss:0.057238, valid acc:98.573, valid f1:98.577, valid precision:98.304, valid recall:98.851, valid auc:99.827
[1;31mTest score increased (98.571242 --> 98.572548).[0m
[98.57254799529842, 98.57650782074157, 98.30380799002546, 98.85072482695573, 99.82707002049425]
====================================================================================================
fold:0 epoch:199 step:0 train loss:0.017621, train acc:99.341, train f1:99.345, train precision:99.447, train recall:99.242, train auc:99.980
fold:0 epoch:199 step:1 train loss:0.016376, train acc:99.408, train f1:99.407, train precision:99.177, train recall:99.639, train auc:99.982
fold:0 epoch:199 step:2 train loss:0.015514, train acc:99.429, train f1:99.433, train precision:99.351, train recall:99.514, train auc:99.983
fold:0 epoch:199 step:3 train loss:0.019951, train acc:99.222, train f1:99.224, train precision:99.179, train recall:99.269, train auc:99.974
fold:0 epoch:199 step:4 train loss:0.017325, train acc:99.335, train f1:99.335, train precision:99.299, train recall:99.372, train auc:99.981
fold:0 epoch:199 step:5 train loss:0.016154, train acc:99.387, train f1:99.383, train precision:99.532, train recall:99.233, train auc:99.985
fold:0 epoch:199 step:6 train loss:0.015719, train acc:99.420, train f1:99.422, train precision:99.302, train recall:99.543, train auc:99.983
fold:0 epoch:199 step:7 train loss:0.018318, train acc:99.304, train f1:99.306, train precision:98.981, train recall:99.634, train auc:99.981
fold:0 epoch:199 step:8 train loss:0.017871, train acc:99.341, train f1:99.336, train precision:99.177, train recall:99.495, train auc:99.979
fold:0 epoch:199 step:9 train loss:0.020223, train acc:99.270, train f1:99.272, train precision:99.455, train recall:99.089, train auc:99.975
fold:0 epoch:199        valid loss:0.057941, valid acc:98.569, valid f1:98.574, valid precision:98.229, valid recall:98.921, valid auc:99.826
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.57254799529842, 98.57650782074157, 98.30380799002546, 98.85072482695573, 99.82707002049425]
====================================================================================================
fold:0 epoch:200 step:0 train loss:0.015265, train acc:99.377, train f1:99.376, train precision:99.485, train recall:99.266, train auc:99.986
fold:0 epoch:200 step:1 train loss:0.016754, train acc:99.353, train f1:99.357, train precision:99.315, train recall:99.399, train auc:99.982
fold:0 epoch:200 step:2 train loss:0.016224, train acc:99.426, train f1:99.427, train precision:99.293, train recall:99.560, train auc:99.984
fold:0 epoch:200 step:3 train loss:0.016932, train acc:99.365, train f1:99.364, train precision:99.164, train recall:99.565, train auc:99.982
fold:0 epoch:200 step:4 train loss:0.017441, train acc:99.344, train f1:99.344, train precision:99.480, train recall:99.208, train auc:99.980
fold:0 epoch:200 step:5 train loss:0.019424, train acc:99.304, train f1:99.307, train precision:99.494, train recall:99.120, train auc:99.977
fold:0 epoch:200 step:6 train loss:0.017593, train acc:99.380, train f1:99.378, train precision:99.308, train recall:99.448, train auc:99.978
fold:0 epoch:200 step:7 train loss:0.017388, train acc:99.350, train f1:99.350, train precision:98.996, train recall:99.706, train auc:99.983
fold:0 epoch:200 step:8 train loss:0.018125, train acc:99.323, train f1:99.323, train precision:99.305, train recall:99.341, train auc:99.979
fold:0 epoch:200 step:9 train loss:0.017788, train acc:99.332, train f1:99.335, train precision:99.387, train recall:99.283, train auc:99.980
fold:0 epoch:200        valid loss:0.058883, valid acc:98.586, valid f1:98.591, valid precision:98.227, valid recall:98.958, valid auc:99.822
[1;31mTest score increased (98.572548 --> 98.585608).[0m
[98.58560794044665, 98.59085290482076, 98.22660098522168, 98.95781637717121, 99.82172688770488]
====================================================================================================
fold:0 epoch:201 step:0 train loss:0.015607, train acc:99.414, train f1:99.418, train precision:99.461, train recall:99.376, train auc:99.985
fold:0 epoch:201 step:1 train loss:0.016444, train acc:99.402, train f1:99.399, train precision:99.210, train recall:99.588, train auc:99.982
fold:0 epoch:201 step:2 train loss:0.016415, train acc:99.344, train f1:99.344, train precision:99.311, train recall:99.377, train auc:99.983
fold:0 epoch:201 step:3 train loss:0.015941, train acc:99.362, train f1:99.363, train precision:99.402, train recall:99.324, train auc:99.984
fold:0 epoch:201 step:4 train loss:0.015796, train acc:99.451, train f1:99.454, train precision:99.418, train recall:99.491, train auc:99.982
fold:0 epoch:201 step:5 train loss:0.018403, train acc:99.329, train f1:99.326, train precision:99.187, train recall:99.467, train auc:99.978
fold:0 epoch:201 step:6 train loss:0.017134, train acc:99.350, train f1:99.350, train precision:99.226, train recall:99.475, train auc:99.981
fold:0 epoch:201 step:7 train loss:0.015130, train acc:99.445, train f1:99.438, train precision:99.512, train recall:99.364, train auc:99.985
fold:0 epoch:201 step:8 train loss:0.018645, train acc:99.313, train f1:99.318, train precision:99.411, train recall:99.224, train auc:99.978
fold:0 epoch:201 step:9 train loss:0.018195, train acc:99.314, train f1:99.318, train precision:99.110, train recall:99.527, train auc:99.981
fold:0 epoch:201        valid loss:0.058340, valid acc:98.623, valid f1:98.628, valid precision:98.331, valid recall:98.926, valid auc:99.826
[1;31mTest score increased (98.585608 --> 98.623482).[0m
[98.62348178137653, 98.62763990521081, 98.33060726432484, 98.92647250881546, 99.8257606488485]
====================================================================================================
fold:0 epoch:202 step:0 train loss:0.015717, train acc:99.396, train f1:99.393, train precision:99.199, train recall:99.589, train auc:99.985
fold:0 epoch:202 step:1 train loss:0.016665, train acc:99.411, train f1:99.407, train precision:99.404, train recall:99.410, train auc:99.980
fold:0 epoch:202 step:2 train loss:0.016452, train acc:99.390, train f1:99.390, train precision:99.591, train recall:99.191, train auc:99.984
fold:0 epoch:202 step:3 train loss:0.016737, train acc:99.420, train f1:99.423, train precision:99.465, train recall:99.380, train auc:99.979
fold:0 epoch:202 step:4 train loss:0.018555, train acc:99.292, train f1:99.299, train precision:98.965, train recall:99.636, train auc:99.981
fold:0 epoch:202 step:5 train loss:0.016988, train acc:99.335, train f1:99.336, train precision:99.209, train recall:99.464, train auc:99.982
fold:0 epoch:202 step:6 train loss:0.018144, train acc:99.350, train f1:99.352, train precision:99.464, train recall:99.240, train auc:99.980
fold:0 epoch:202 step:7 train loss:0.016081, train acc:99.423, train f1:99.423, train precision:99.560, train recall:99.286, train auc:99.985
fold:0 epoch:202 step:8 train loss:0.015701, train acc:99.417, train f1:99.417, train precision:99.323, train recall:99.511, train auc:99.983
fold:0 epoch:202 step:9 train loss:0.018939, train acc:99.314, train f1:99.302, train precision:99.125, train recall:99.480, train auc:99.978
fold:0 epoch:202        valid loss:0.060651, valid acc:98.558, valid f1:98.564, valid precision:98.143, valid recall:98.989, valid auc:99.821
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.62348178137653, 98.62763990521081, 98.33060726432484, 98.92647250881546, 99.8257606488485]
====================================================================================================
fold:0 epoch:203 step:0 train loss:0.017182, train acc:99.390, train f1:99.384, train precision:99.409, train recall:99.360, train auc:99.981
fold:0 epoch:203 step:1 train loss:0.018643, train acc:99.301, train f1:99.303, train precision:99.518, train recall:99.089, train auc:99.980
fold:0 epoch:203 step:2 train loss:0.015375, train acc:99.387, train f1:99.391, train precision:99.225, train recall:99.557, train auc:99.984
fold:0 epoch:203 step:3 train loss:0.015879, train acc:99.411, train f1:99.410, train precision:99.237, train recall:99.584, train auc:99.984
fold:0 epoch:203 step:4 train loss:0.015937, train acc:99.384, train f1:99.388, train precision:99.298, train recall:99.478, train auc:99.983
fold:0 epoch:203 step:5 train loss:0.017426, train acc:99.362, train f1:99.364, train precision:99.379, train recall:99.349, train auc:99.980
fold:0 epoch:203 step:6 train loss:0.017290, train acc:99.377, train f1:99.380, train precision:99.447, train recall:99.314, train auc:99.979
fold:0 epoch:203 step:7 train loss:0.016798, train acc:99.359, train f1:99.353, train precision:99.237, train recall:99.470, train auc:99.982
fold:0 epoch:203 step:8 train loss:0.017868, train acc:99.329, train f1:99.330, train precision:99.185, train recall:99.475, train auc:99.980
fold:0 epoch:203 step:9 train loss:0.018442, train acc:99.358, train f1:99.351, train precision:99.431, train recall:99.272, train auc:99.975
fold:0 epoch:203        valid loss:0.058090, valid acc:98.593, valid f1:98.597, valid precision:98.315, valid recall:98.882, valid auc:99.825
[1;31mEarlyStopping counter: 2 out of 50[0m
[98.62348178137653, 98.62763990521081, 98.33060726432484, 98.92647250881546, 99.8257606488485]
====================================================================================================
fold:0 epoch:204 step:0 train loss:0.016545, train acc:99.402, train f1:99.402, train precision:99.566, train recall:99.239, train auc:99.982
fold:0 epoch:204 step:1 train loss:0.016857, train acc:99.338, train f1:99.338, train precision:99.153, train recall:99.523, train auc:99.983
fold:0 epoch:204 step:2 train loss:0.016540, train acc:99.402, train f1:99.402, train precision:99.330, train recall:99.475, train auc:99.983
fold:0 epoch:204 step:3 train loss:0.016460, train acc:99.387, train f1:99.386, train precision:99.292, train recall:99.480, train auc:99.982
fold:0 epoch:204 step:4 train loss:0.017599, train acc:99.307, train f1:99.306, train precision:99.206, train recall:99.406, train auc:99.981
fold:0 epoch:204 step:5 train loss:0.017319, train acc:99.341, train f1:99.339, train precision:99.412, train recall:99.266, train auc:99.980
fold:0 epoch:204 step:6 train loss:0.018103, train acc:99.344, train f1:99.347, train precision:99.386, train recall:99.308, train auc:99.980
fold:0 epoch:204 step:7 train loss:0.017529, train acc:99.359, train f1:99.363, train precision:99.237, train recall:99.490, train auc:99.980
fold:0 epoch:204 step:8 train loss:0.016740, train acc:99.374, train f1:99.376, train precision:99.197, train recall:99.555, train auc:99.983
fold:0 epoch:204 step:9 train loss:0.015945, train acc:99.437, train f1:99.429, train precision:99.341, train recall:99.518, train auc:99.985
fold:0 epoch:204        valid loss:0.060373, valid acc:98.552, valid f1:98.558, valid precision:98.133, valid recall:98.987, valid auc:99.820
[1;31mEarlyStopping counter: 3 out of 50[0m
[98.62348178137653, 98.62763990521081, 98.33060726432484, 98.92647250881546, 99.8257606488485]
====================================================================================================
fold:0 epoch:205 step:0 train loss:0.017967, train acc:99.350, train f1:99.355, train precision:99.629, train recall:99.082, train auc:99.982
fold:0 epoch:205 step:1 train loss:0.016531, train acc:99.384, train f1:99.383, train precision:99.438, train recall:99.329, train auc:99.982
fold:0 epoch:205 step:2 train loss:0.016978, train acc:99.350, train f1:99.351, train precision:99.005, train recall:99.701, train auc:99.985
fold:0 epoch:205 step:3 train loss:0.017396, train acc:99.368, train f1:99.368, train precision:99.128, train recall:99.608, train auc:99.982
fold:0 epoch:205 step:4 train loss:0.017854, train acc:99.344, train f1:99.343, train precision:99.449, train recall:99.237, train auc:99.980
fold:0 epoch:205 step:5 train loss:0.016523, train acc:99.451, train f1:99.445, train precision:99.500, train recall:99.390, train auc:99.982
fold:0 epoch:205 step:6 train loss:0.016295, train acc:99.362, train f1:99.361, train precision:99.510, train recall:99.212, train auc:99.983
fold:0 epoch:205 step:7 train loss:0.014920, train acc:99.393, train f1:99.401, train precision:99.201, train recall:99.602, train auc:99.986
fold:0 epoch:205 step:8 train loss:0.017500, train acc:99.344, train f1:99.345, train precision:99.082, train recall:99.609, train auc:99.983
fold:0 epoch:205 step:9 train loss:0.016556, train acc:99.411, train f1:99.402, train precision:99.057, train recall:99.749, train auc:99.982
fold:0 epoch:205        valid loss:0.058993, valid acc:98.571, valid f1:98.577, valid precision:98.159, valid recall:99.000, valid auc:99.823
[1;31mEarlyStopping counter: 4 out of 50[0m
[98.62348178137653, 98.62763990521081, 98.33060726432484, 98.92647250881546, 99.8257606488485]
====================================================================================================
fold:0 epoch:206 step:0 train loss:0.022054, train acc:99.130, train f1:99.129, train precision:99.772, train recall:98.495, train auc:99.983
fold:0 epoch:206 step:1 train loss:0.015730, train acc:99.414, train f1:99.413, train precision:99.577, train recall:99.249, train auc:99.985
fold:0 epoch:206 step:2 train loss:0.017549, train acc:99.329, train f1:99.331, train precision:98.970, train recall:99.695, train auc:99.984
fold:0 epoch:206 step:3 train loss:0.018657, train acc:99.286, train f1:99.294, train precision:98.954, train recall:99.637, train auc:99.981
fold:0 epoch:206 step:4 train loss:0.016777, train acc:99.359, train f1:99.360, train precision:99.209, train recall:99.512, train auc:99.981
fold:0 epoch:206 step:5 train loss:0.016330, train acc:99.411, train f1:99.406, train precision:99.630, train recall:99.182, train auc:99.983
fold:0 epoch:206 step:6 train loss:0.020159, train acc:99.280, train f1:99.284, train precision:99.623, train recall:98.948, train auc:99.976
fold:0 epoch:206 step:7 train loss:0.017812, train acc:99.338, train f1:99.332, train precision:99.091, train recall:99.574, train auc:99.981
fold:0 epoch:206 step:8 train loss:0.018138, train acc:99.374, train f1:99.375, train precision:98.974, train recall:99.780, train auc:99.982
fold:0 epoch:206 step:9 train loss:0.015980, train acc:99.332, train f1:99.327, train precision:99.117, train recall:99.539, train auc:99.986
fold:0 epoch:206        valid loss:0.059938, valid acc:98.561, valid f1:98.568, valid precision:98.101, valid recall:99.039, valid auc:99.823
[1;31mEarlyStopping counter: 5 out of 50[0m
[98.62348178137653, 98.62763990521081, 98.33060726432484, 98.92647250881546, 99.8257606488485]
====================================================================================================
fold:0 epoch:207 step:0 train loss:0.015099, train acc:99.445, train f1:99.443, train precision:99.540, train recall:99.346, train auc:99.987
fold:0 epoch:207 step:1 train loss:0.021696, train acc:99.210, train f1:99.210, train precision:99.669, train recall:98.754, train auc:99.978
fold:0 epoch:207 step:2 train loss:0.015370, train acc:99.442, train f1:99.440, train precision:99.388, train recall:99.492, train auc:99.984
fold:0 epoch:207 step:3 train loss:0.017005, train acc:99.365, train f1:99.365, train precision:99.129, train recall:99.602, train auc:99.982
fold:0 epoch:207 step:4 train loss:0.016640, train acc:99.399, train f1:99.397, train precision:99.109, train recall:99.687, train auc:99.984
fold:0 epoch:207 step:5 train loss:0.014122, train acc:99.493, train f1:99.492, train precision:99.377, train recall:99.608, train auc:99.987
fold:0 epoch:207 step:6 train loss:0.016792, train acc:99.359, train f1:99.359, train precision:99.572, train recall:99.148, train auc:99.983
fold:0 epoch:207 step:7 train loss:0.015350, train acc:99.460, train f1:99.458, train precision:99.589, train recall:99.327, train auc:99.985
fold:0 epoch:207 step:8 train loss:0.016167, train acc:99.377, train f1:99.383, train precision:99.353, train recall:99.414, train auc:99.984
fold:0 epoch:207 step:9 train loss:0.015206, train acc:99.428, train f1:99.434, train precision:99.356, train recall:99.512, train auc:99.986
fold:0 epoch:207        valid loss:0.058648, valid acc:98.607, valid f1:98.611, valid precision:98.305, valid recall:98.919, valid auc:99.827
[1;31mEarlyStopping counter: 6 out of 50[0m
[98.62348178137653, 98.62763990521081, 98.33060726432484, 98.92647250881546, 99.8257606488485]
====================================================================================================
fold:0 epoch:208 step:0 train loss:0.017034, train acc:99.335, train f1:99.337, train precision:99.030, train recall:99.646, train auc:99.985
fold:0 epoch:208 step:1 train loss:0.015575, train acc:99.438, train f1:99.441, train precision:99.272, train recall:99.611, train auc:99.983
fold:0 epoch:208 step:2 train loss:0.019421, train acc:99.271, train f1:99.271, train precision:99.401, train recall:99.141, train auc:99.976
fold:0 epoch:208 step:3 train loss:0.015900, train acc:99.438, train f1:99.437, train precision:99.602, train recall:99.273, train auc:99.984
fold:0 epoch:208 step:4 train loss:0.015930, train acc:99.466, train f1:99.466, train precision:99.487, train recall:99.445, train auc:99.982
fold:0 epoch:208 step:5 train loss:0.016883, train acc:99.365, train f1:99.364, train precision:99.194, train recall:99.534, train auc:99.982
fold:0 epoch:208 step:6 train loss:0.018385, train acc:99.338, train f1:99.336, train precision:99.024, train recall:99.650, train auc:99.981
fold:0 epoch:208 step:7 train loss:0.017128, train acc:99.359, train f1:99.362, train precision:99.440, train recall:99.283, train auc:99.981
fold:0 epoch:208 step:8 train loss:0.016494, train acc:99.384, train f1:99.383, train precision:99.364, train recall:99.401, train auc:99.983
fold:0 epoch:208 step:9 train loss:0.014227, train acc:99.420, train f1:99.420, train precision:99.543, train recall:99.298, train auc:99.989
fold:0 epoch:208        valid loss:0.058485, valid acc:98.573, valid f1:98.577, valid precision:98.284, valid recall:98.872, valid auc:99.820
[1;31mEarlyStopping counter: 7 out of 50[0m
[98.62348178137653, 98.62763990521081, 98.33060726432484, 98.92647250881546, 99.8257606488485]
====================================================================================================
fold:0 epoch:209 step:0 train loss:0.016240, train acc:99.399, train f1:99.401, train precision:99.500, train recall:99.301, train auc:99.983
fold:0 epoch:209 step:1 train loss:0.017582, train acc:99.335, train f1:99.334, train precision:99.135, train recall:99.535, train auc:99.981
fold:0 epoch:209 step:2 train loss:0.019041, train acc:99.319, train f1:99.322, train precision:99.096, train recall:99.549, train auc:99.977
fold:0 epoch:209 step:3 train loss:0.016418, train acc:99.417, train f1:99.418, train precision:99.306, train recall:99.530, train auc:99.983
fold:0 epoch:209 step:4 train loss:0.017358, train acc:99.405, train f1:99.404, train precision:99.431, train recall:99.376, train auc:99.979
fold:0 epoch:209 step:5 train loss:0.016193, train acc:99.429, train f1:99.429, train precision:99.608, train recall:99.250, train auc:99.983
fold:0 epoch:209 step:6 train loss:0.016393, train acc:99.396, train f1:99.396, train precision:99.420, train recall:99.372, train auc:99.981
fold:0 epoch:209 step:7 train loss:0.017058, train acc:99.359, train f1:99.363, train precision:99.243, train recall:99.484, train auc:99.982
fold:0 epoch:209 step:8 train loss:0.017732, train acc:99.326, train f1:99.321, train precision:99.019, train recall:99.624, train auc:99.981
fold:0 epoch:209 step:9 train loss:0.013474, train acc:99.516, train f1:99.521, train precision:99.460, train recall:99.581, train auc:99.988
fold:0 epoch:209        valid loss:0.059887, valid acc:98.549, valid f1:98.556, valid precision:98.118, valid recall:98.997, valid auc:99.822
[1;31mEarlyStopping counter: 8 out of 50[0m
[98.62348178137653, 98.62763990521081, 98.33060726432484, 98.92647250881546, 99.8257606488485]
====================================================================================================
fold:0 epoch:210 step:0 train loss:0.016330, train acc:99.417, train f1:99.414, train precision:99.441, train recall:99.387, train auc:99.982
fold:0 epoch:210 step:1 train loss:0.014515, train acc:99.448, train f1:99.445, train precision:99.564, train recall:99.326, train auc:99.988
fold:0 epoch:210 step:2 train loss:0.016322, train acc:99.408, train f1:99.406, train precision:99.339, train recall:99.473, train auc:99.981
fold:0 epoch:210 step:3 train loss:0.015118, train acc:99.463, train f1:99.464, train precision:99.306, train recall:99.622, train auc:99.985
fold:0 epoch:210 step:4 train loss:0.016296, train acc:99.414, train f1:99.419, train precision:99.257, train recall:99.582, train auc:99.982
fold:0 epoch:210 step:5 train loss:0.015945, train acc:99.411, train f1:99.413, train precision:99.441, train recall:99.386, train auc:99.983
fold:0 epoch:210 step:6 train loss:0.017461, train acc:99.380, train f1:99.382, train precision:99.379, train recall:99.385, train auc:99.978
fold:0 epoch:210 step:7 train loss:0.015069, train acc:99.463, train f1:99.463, train precision:99.536, train recall:99.390, train auc:99.984
fold:0 epoch:210 step:8 train loss:0.016349, train acc:99.399, train f1:99.394, train precision:99.373, train recall:99.416, train auc:99.983
fold:0 epoch:210 step:9 train loss:0.015177, train acc:99.455, train f1:99.462, train precision:99.393, train recall:99.531, train auc:99.984
fold:0 epoch:210        valid loss:0.059120, valid acc:98.629, valid f1:98.633, valid precision:98.313, valid recall:98.955, valid auc:99.824
[1;31mTest score increased (98.623482 --> 98.628706).[0m
[98.6287057594358, 98.63316844571726, 98.31322174646425, 98.95520438814157, 99.82400836136678]
====================================================================================================
fold:0 epoch:211 step:0 train loss:0.015741, train acc:99.387, train f1:99.391, train precision:99.243, train recall:99.539, train auc:99.984
fold:0 epoch:211 step:1 train loss:0.015810, train acc:99.414, train f1:99.416, train precision:99.235, train recall:99.598, train auc:99.984
fold:0 epoch:211 step:2 train loss:0.014247, train acc:99.490, train f1:99.492, train precision:99.489, train recall:99.495, train auc:99.987
fold:0 epoch:211 step:3 train loss:0.014893, train acc:99.472, train f1:99.468, train precision:99.489, train recall:99.446, train auc:99.986
fold:0 epoch:211 step:4 train loss:0.015668, train acc:99.435, train f1:99.438, train precision:99.617, train recall:99.261, train auc:99.983
fold:0 epoch:211 step:5 train loss:0.016005, train acc:99.423, train f1:99.422, train precision:99.298, train recall:99.547, train auc:99.983
fold:0 epoch:211 step:6 train loss:0.017844, train acc:99.298, train f1:99.293, train precision:99.043, train recall:99.544, train auc:99.981
fold:0 epoch:211 step:7 train loss:0.014329, train acc:99.490, train f1:99.491, train precision:99.433, train recall:99.549, train auc:99.985
fold:0 epoch:211 step:8 train loss:0.017075, train acc:99.374, train f1:99.374, train precision:99.474, train recall:99.274, train auc:99.981
fold:0 epoch:211 step:9 train loss:0.016610, train acc:99.420, train f1:99.423, train precision:99.319, train recall:99.527, train auc:99.981
fold:0 epoch:211        valid loss:0.058998, valid acc:98.593, valid f1:98.599, valid precision:98.212, valid recall:98.989, valid auc:99.828
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.6287057594358, 98.63316844571726, 98.31322174646425, 98.95520438814157, 99.82400836136678]
====================================================================================================
fold:0 epoch:212 step:0 train loss:0.015135, train acc:99.435, train f1:99.433, train precision:99.534, train recall:99.333, train auc:99.986
fold:0 epoch:212 step:1 train loss:0.016071, train acc:99.448, train f1:99.448, train precision:99.323, train recall:99.572, train auc:99.982
fold:0 epoch:212 step:2 train loss:0.018302, train acc:99.341, train f1:99.342, train precision:99.082, train recall:99.603, train auc:99.976
fold:0 epoch:212 step:3 train loss:0.015712, train acc:99.451, train f1:99.449, train precision:99.455, train recall:99.443, train auc:99.983
fold:0 epoch:212 step:4 train loss:0.014736, train acc:99.414, train f1:99.409, train precision:99.477, train recall:99.342, train auc:99.987
fold:0 epoch:212 step:5 train loss:0.013987, train acc:99.500, train f1:99.500, train precision:99.555, train recall:99.445, train auc:99.987
fold:0 epoch:212 step:6 train loss:0.015607, train acc:99.438, train f1:99.440, train precision:99.422, train recall:99.458, train auc:99.984
fold:0 epoch:212 step:7 train loss:0.015363, train acc:99.411, train f1:99.416, train precision:99.280, train recall:99.551, train auc:99.985
fold:0 epoch:212 step:8 train loss:0.016184, train acc:99.408, train f1:99.412, train precision:99.219, train recall:99.605, train auc:99.981
fold:0 epoch:212 step:9 train loss:0.016337, train acc:99.428, train f1:99.429, train precision:99.386, train recall:99.473, train auc:99.981
fold:0 epoch:212        valid loss:0.058650, valid acc:98.593, valid f1:98.597, valid precision:98.317, valid recall:98.879, valid auc:99.827
[1;31mEarlyStopping counter: 2 out of 50[0m
[98.6287057594358, 98.63316844571726, 98.31322174646425, 98.95520438814157, 99.82400836136678]
====================================================================================================
fold:0 epoch:213 step:0 train loss:0.016482, train acc:99.362, train f1:99.360, train precision:99.546, train recall:99.175, train auc:99.984
fold:0 epoch:213 step:1 train loss:0.017106, train acc:99.384, train f1:99.387, train precision:99.381, train recall:99.393, train auc:99.980
fold:0 epoch:213 step:2 train loss:0.014388, train acc:99.472, train f1:99.470, train precision:99.425, train recall:99.516, train auc:99.987
fold:0 epoch:213 step:3 train loss:0.016895, train acc:99.365, train f1:99.368, train precision:99.181, train recall:99.555, train auc:99.983
fold:0 epoch:213 step:4 train loss:0.014509, train acc:99.521, train f1:99.517, train precision:99.410, train recall:99.624, train auc:99.985
fold:0 epoch:213 step:5 train loss:0.015579, train acc:99.411, train f1:99.409, train precision:99.497, train recall:99.320, train auc:99.984
fold:0 epoch:213 step:6 train loss:0.016447, train acc:99.402, train f1:99.405, train precision:99.525, train recall:99.284, train auc:99.984
fold:0 epoch:213 step:7 train loss:0.016041, train acc:99.408, train f1:99.407, train precision:99.322, train recall:99.492, train auc:99.983
fold:0 epoch:213 step:8 train loss:0.015289, train acc:99.420, train f1:99.424, train precision:99.250, train recall:99.599, train auc:99.986
fold:0 epoch:213 step:9 train loss:0.014568, train acc:99.437, train f1:99.440, train precision:99.163, train recall:99.719, train auc:99.989
fold:0 epoch:213        valid loss:0.060522, valid acc:98.536, valid f1:98.543, valid precision:98.105, valid recall:98.984, valid auc:99.823
[1;31mEarlyStopping counter: 3 out of 50[0m
[98.6287057594358, 98.63316844571726, 98.31322174646425, 98.95520438814157, 99.82400836136678]
====================================================================================================
fold:0 epoch:214 step:0 train loss:0.013859, train acc:99.500, train f1:99.499, train precision:99.578, train recall:99.420, train auc:99.988
fold:0 epoch:214 step:1 train loss:0.020167, train acc:99.265, train f1:99.266, train precision:99.688, train recall:98.847, train auc:99.979
fold:0 epoch:214 step:2 train loss:0.015105, train acc:99.429, train f1:99.431, train precision:99.549, train recall:99.313, train auc:99.985
fold:0 epoch:214 step:3 train loss:0.018418, train acc:99.326, train f1:99.320, train precision:98.909, train recall:99.734, train auc:99.981
fold:0 epoch:214 step:4 train loss:0.015957, train acc:99.399, train f1:99.397, train precision:99.152, train recall:99.644, train auc:99.985
fold:0 epoch:214 step:5 train loss:0.015217, train acc:99.475, train f1:99.478, train precision:99.521, train recall:99.436, train auc:99.984
fold:0 epoch:214 step:6 train loss:0.016955, train acc:99.350, train f1:99.357, train precision:99.595, train recall:99.121, train auc:99.984
fold:0 epoch:214 step:7 train loss:0.015047, train acc:99.503, train f1:99.500, train precision:99.473, train recall:99.527, train auc:99.981
fold:0 epoch:214 step:8 train loss:0.016228, train acc:99.445, train f1:99.444, train precision:99.238, train recall:99.651, train auc:99.982
fold:0 epoch:214 step:9 train loss:0.015092, train acc:99.411, train f1:99.404, train precision:99.148, train recall:99.661, train auc:99.987
fold:0 epoch:214        valid loss:0.060372, valid acc:98.557, valid f1:98.563, valid precision:98.173, valid recall:98.955, valid auc:99.824
[1;31mEarlyStopping counter: 4 out of 50[0m
[98.6287057594358, 98.63316844571726, 98.31322174646425, 98.95520438814157, 99.82400836136678]
====================================================================================================
fold:0 epoch:215 step:0 train loss:0.013837, train acc:99.557, train f1:99.553, train precision:99.575, train recall:99.532, train auc:99.986
fold:0 epoch:215 step:1 train loss:0.017031, train acc:99.377, train f1:99.378, train precision:99.578, train recall:99.178, train auc:99.982
fold:0 epoch:215 step:2 train loss:0.014254, train acc:99.457, train f1:99.456, train precision:99.571, train recall:99.340, train auc:99.988
fold:0 epoch:215 step:3 train loss:0.015136, train acc:99.469, train f1:99.467, train precision:99.369, train recall:99.564, train auc:99.982
fold:0 epoch:215 step:4 train loss:0.015239, train acc:99.454, train f1:99.460, train precision:99.254, train recall:99.668, train auc:99.986
fold:0 epoch:215 step:5 train loss:0.017137, train acc:99.365, train f1:99.367, train precision:99.150, train recall:99.585, train auc:99.983
fold:0 epoch:215 step:6 train loss:0.014983, train acc:99.457, train f1:99.459, train precision:99.495, train recall:99.423, train auc:99.985
fold:0 epoch:215 step:7 train loss:0.016931, train acc:99.396, train f1:99.392, train precision:99.508, train recall:99.276, train auc:99.982
fold:0 epoch:215 step:8 train loss:0.016764, train acc:99.420, train f1:99.422, train precision:99.404, train recall:99.441, train auc:99.980
fold:0 epoch:215 step:9 train loss:0.019614, train acc:99.367, train f1:99.366, train precision:99.140, train recall:99.594, train auc:99.970
fold:0 epoch:215        valid loss:0.058048, valid acc:98.603, valid f1:98.607, valid precision:98.325, valid recall:98.890, valid auc:99.830
[1;31mEarlyStopping counter: 5 out of 50[0m
[98.6287057594358, 98.63316844571726, 98.31322174646425, 98.95520438814157, 99.82400836136678]
====================================================================================================
fold:0 epoch:216 step:0 train loss:0.014712, train acc:99.417, train f1:99.415, train precision:99.212, train recall:99.620, train auc:99.987
fold:0 epoch:216 step:1 train loss:0.018032, train acc:99.371, train f1:99.376, train precision:99.478, train recall:99.274, train auc:99.979
fold:0 epoch:216 step:2 train loss:0.015294, train acc:99.432, train f1:99.432, train precision:99.608, train recall:99.256, train auc:99.985
fold:0 epoch:216 step:3 train loss:0.014527, train acc:99.451, train f1:99.452, train precision:99.337, train recall:99.567, train auc:99.986
fold:0 epoch:216 step:4 train loss:0.014985, train acc:99.408, train f1:99.410, train precision:99.313, train recall:99.507, train auc:99.986
fold:0 epoch:216 step:5 train loss:0.016931, train acc:99.362, train f1:99.366, train precision:99.105, train recall:99.629, train auc:99.983
fold:0 epoch:216 step:6 train loss:0.017389, train acc:99.359, train f1:99.358, train precision:99.231, train recall:99.486, train auc:99.979
fold:0 epoch:216 step:7 train loss:0.017292, train acc:99.359, train f1:99.356, train precision:99.380, train recall:99.331, train auc:99.981
fold:0 epoch:216 step:8 train loss:0.017600, train acc:99.411, train f1:99.405, train precision:99.586, train recall:99.225, train auc:99.981
fold:0 epoch:216 step:9 train loss:0.013427, train acc:99.525, train f1:99.529, train precision:99.633, train recall:99.425, train auc:99.990
fold:0 epoch:216        valid loss:0.058250, valid acc:98.616, valid f1:98.620, valid precision:98.315, valid recall:98.926, valid auc:99.827
[1;31mEarlyStopping counter: 6 out of 50[0m
[98.6287057594358, 98.63316844571726, 98.31322174646425, 98.95520438814157, 99.82400836136678]
====================================================================================================
fold:0 epoch:217 step:0 train loss:0.017717, train acc:99.344, train f1:99.343, train precision:98.995, train recall:99.693, train auc:99.983
fold:0 epoch:217 step:1 train loss:0.015402, train acc:99.396, train f1:99.398, train precision:99.211, train recall:99.586, train auc:99.986
fold:0 epoch:217 step:2 train loss:0.014046, train acc:99.472, train f1:99.476, train precision:99.539, train recall:99.412, train auc:99.988
fold:0 epoch:217 step:3 train loss:0.015719, train acc:99.411, train f1:99.413, train precision:99.610, train recall:99.217, train auc:99.984
fold:0 epoch:217 step:4 train loss:0.015512, train acc:99.438, train f1:99.440, train precision:99.477, train recall:99.404, train auc:99.985
fold:0 epoch:217 step:5 train loss:0.016369, train acc:99.402, train f1:99.399, train precision:99.210, train recall:99.588, train auc:99.982
fold:0 epoch:217 step:6 train loss:0.015803, train acc:99.423, train f1:99.422, train precision:99.236, train recall:99.608, train auc:99.984
fold:0 epoch:217 step:7 train loss:0.015355, train acc:99.463, train f1:99.463, train precision:99.251, train recall:99.676, train auc:99.984
fold:0 epoch:217 step:8 train loss:0.017750, train acc:99.335, train f1:99.334, train precision:99.425, train recall:99.243, train auc:99.979
fold:0 epoch:217 step:9 train loss:0.018577, train acc:99.323, train f1:99.323, train precision:99.524, train recall:99.122, train auc:99.977
fold:0 epoch:217        valid loss:0.059612, valid acc:98.570, valid f1:98.575, valid precision:98.239, valid recall:98.913, valid auc:99.822
[1;31mEarlyStopping counter: 7 out of 50[0m
[98.6287057594358, 98.63316844571726, 98.31322174646425, 98.95520438814157, 99.82400836136678]
====================================================================================================
fold:0 epoch:218 step:0 train loss:0.014498, train acc:99.445, train f1:99.448, train precision:99.575, train recall:99.322, train auc:99.986
fold:0 epoch:218 step:1 train loss:0.015533, train acc:99.466, train f1:99.466, train precision:99.264, train recall:99.670, train auc:99.983
fold:0 epoch:218 step:2 train loss:0.015466, train acc:99.454, train f1:99.449, train precision:99.201, train recall:99.697, train auc:99.985
fold:0 epoch:218 step:3 train loss:0.016722, train acc:99.393, train f1:99.393, train precision:99.354, train recall:99.433, train auc:99.981
fold:0 epoch:218 step:4 train loss:0.015933, train acc:99.414, train f1:99.418, train precision:99.605, train recall:99.231, train auc:99.984
fold:0 epoch:218 step:5 train loss:0.013751, train acc:99.496, train f1:99.489, train precision:99.510, train recall:99.467, train auc:99.987
fold:0 epoch:218 step:6 train loss:0.015471, train acc:99.451, train f1:99.452, train precision:99.355, train recall:99.549, train auc:99.985
fold:0 epoch:218 step:7 train loss:0.015545, train acc:99.475, train f1:99.476, train precision:99.391, train recall:99.561, train auc:99.983
fold:0 epoch:218 step:8 train loss:0.014925, train acc:99.445, train f1:99.449, train precision:99.431, train recall:99.468, train auc:99.986
fold:0 epoch:218 step:9 train loss:0.014513, train acc:99.411, train f1:99.414, train precision:99.509, train recall:99.318, train auc:99.987
fold:0 epoch:218        valid loss:0.059851, valid acc:98.608, valid f1:98.613, valid precision:98.257, valid recall:98.971, valid auc:99.821
[1;31mEarlyStopping counter: 8 out of 50[0m
[98.6287057594358, 98.63316844571726, 98.31322174646425, 98.95520438814157, 99.82400836136678]
====================================================================================================
fold:0 epoch:219 step:0 train loss:0.014938, train acc:99.463, train f1:99.466, train precision:99.364, train recall:99.569, train auc:99.985
fold:0 epoch:219 step:1 train loss:0.014329, train acc:99.493, train f1:99.495, train precision:99.393, train recall:99.598, train auc:99.987
fold:0 epoch:219 step:2 train loss:0.014727, train acc:99.472, train f1:99.479, train precision:99.500, train recall:99.458, train auc:99.986
fold:0 epoch:219 step:3 train loss:0.014477, train acc:99.451, train f1:99.449, train precision:99.328, train recall:99.571, train auc:99.987
fold:0 epoch:219 step:4 train loss:0.016505, train acc:99.411, train f1:99.410, train precision:99.431, train recall:99.388, train auc:99.980
fold:0 epoch:219 step:5 train loss:0.016541, train acc:99.405, train f1:99.400, train precision:99.305, train recall:99.495, train auc:99.981
fold:0 epoch:219 step:6 train loss:0.013642, train acc:99.496, train f1:99.495, train precision:99.467, train recall:99.522, train auc:99.988
fold:0 epoch:219 step:7 train loss:0.015874, train acc:99.411, train f1:99.412, train precision:99.500, train recall:99.325, train auc:99.984
fold:0 epoch:219 step:8 train loss:0.015133, train acc:99.448, train f1:99.444, train precision:99.374, train recall:99.515, train auc:99.984
fold:0 epoch:219 step:9 train loss:0.015463, train acc:99.446, train f1:99.450, train precision:99.268, train recall:99.632, train auc:99.984
fold:0 epoch:219        valid loss:0.058893, valid acc:98.608, valid f1:98.613, valid precision:98.252, valid recall:98.976, valid auc:99.825
[1;31mEarlyStopping counter: 9 out of 50[0m
[98.6287057594358, 98.63316844571726, 98.31322174646425, 98.95520438814157, 99.82400836136678]
====================================================================================================
fold:0 epoch:220 step:0 train loss:0.015184, train acc:99.432, train f1:99.433, train precision:99.621, train recall:99.245, train auc:99.986
fold:0 epoch:220 step:1 train loss:0.015182, train acc:99.475, train f1:99.476, train precision:99.524, train recall:99.427, train auc:99.985
fold:0 epoch:220 step:2 train loss:0.014484, train acc:99.487, train f1:99.486, train precision:99.395, train recall:99.577, train auc:99.987
fold:0 epoch:220 step:3 train loss:0.014803, train acc:99.530, train f1:99.528, train precision:99.425, train recall:99.632, train auc:99.984
fold:0 epoch:220 step:4 train loss:0.013813, train acc:99.493, train f1:99.493, train precision:99.353, train recall:99.633, train auc:99.989
fold:0 epoch:220 step:5 train loss:0.013489, train acc:99.518, train f1:99.519, train precision:99.488, train recall:99.549, train auc:99.987
fold:0 epoch:220 step:6 train loss:0.016701, train acc:99.380, train f1:99.385, train precision:99.509, train recall:99.262, train auc:99.977
fold:0 epoch:220 step:7 train loss:0.015527, train acc:99.454, train f1:99.450, train precision:99.465, train recall:99.435, train auc:99.983
fold:0 epoch:220 step:8 train loss:0.018406, train acc:99.286, train f1:99.290, train precision:99.020, train recall:99.562, train auc:99.981
fold:0 epoch:220 step:9 train loss:0.015673, train acc:99.420, train f1:99.418, train precision:99.313, train recall:99.523, train auc:99.984
fold:0 epoch:220        valid loss:0.060512, valid acc:98.569, valid f1:98.575, valid precision:98.126, valid recall:99.028, valid auc:99.824
[1;31mEarlyStopping counter: 10 out of 50[0m
[98.6287057594358, 98.63316844571726, 98.31322174646425, 98.95520438814157, 99.82400836136678]
====================================================================================================
fold:0 epoch:221 step:0 train loss:0.015552, train acc:99.411, train f1:99.416, train precision:99.557, train recall:99.275, train auc:99.985
fold:0 epoch:221 step:1 train loss:0.015912, train acc:99.435, train f1:99.438, train precision:99.592, train recall:99.284, train auc:99.983
fold:0 epoch:221 step:2 train loss:0.013778, train acc:99.484, train f1:99.483, train precision:99.450, train recall:99.517, train auc:99.987
fold:0 epoch:221 step:3 train loss:0.016035, train acc:99.445, train f1:99.446, train precision:99.246, train recall:99.646, train auc:99.984
fold:0 epoch:221 step:4 train loss:0.014493, train acc:99.481, train f1:99.484, train precision:99.339, train recall:99.629, train auc:99.987
fold:0 epoch:221 step:5 train loss:0.015358, train acc:99.448, train f1:99.442, train precision:99.464, train recall:99.421, train auc:99.985
fold:0 epoch:221 step:6 train loss:0.015205, train acc:99.417, train f1:99.415, train precision:99.571, train recall:99.260, train auc:99.985
fold:0 epoch:221 step:7 train loss:0.015554, train acc:99.451, train f1:99.452, train precision:99.452, train recall:99.452, train auc:99.982
fold:0 epoch:221 step:8 train loss:0.016322, train acc:99.371, train f1:99.366, train precision:99.158, train recall:99.574, train auc:99.984
fold:0 epoch:221 step:9 train loss:0.016429, train acc:99.358, train f1:99.365, train precision:99.391, train recall:99.339, train auc:99.984
fold:0 epoch:221        valid loss:0.060188, valid acc:98.574, valid f1:98.580, valid precision:98.171, valid recall:98.992, valid auc:99.823
[1;31mEarlyStopping counter: 11 out of 50[0m
[98.6287057594358, 98.63316844571726, 98.31322174646425, 98.95520438814157, 99.82400836136678]
====================================================================================================
fold:0 epoch:222 step:0 train loss:0.015179, train acc:99.448, train f1:99.451, train precision:99.304, train recall:99.599, train auc:99.985
fold:0 epoch:222 step:1 train loss:0.013882, train acc:99.490, train f1:99.488, train precision:99.528, train recall:99.448, train auc:99.988
fold:0 epoch:222 step:2 train loss:0.013084, train acc:99.527, train f1:99.524, train precision:99.625, train recall:99.423, train auc:99.990
fold:0 epoch:222 step:3 train loss:0.015622, train acc:99.448, train f1:99.447, train precision:99.311, train recall:99.584, train auc:99.981
fold:0 epoch:222 step:4 train loss:0.014030, train acc:99.506, train f1:99.507, train precision:99.326, train recall:99.689, train auc:99.987
fold:0 epoch:222 step:5 train loss:0.016088, train acc:99.371, train f1:99.378, train precision:99.372, train recall:99.384, train auc:99.984
fold:0 epoch:222 step:6 train loss:0.015776, train acc:99.423, train f1:99.419, train precision:99.398, train recall:99.441, train auc:99.984
fold:0 epoch:222 step:7 train loss:0.015346, train acc:99.438, train f1:99.441, train precision:99.483, train recall:99.399, train auc:99.983
fold:0 epoch:222 step:8 train loss:0.017376, train acc:99.365, train f1:99.360, train precision:99.287, train recall:99.434, train auc:99.981
fold:0 epoch:222 step:9 train loss:0.015950, train acc:99.393, train f1:99.402, train precision:99.239, train recall:99.566, train auc:99.983
fold:0 epoch:222        valid loss:0.057470, valid acc:98.647, valid f1:98.651, valid precision:98.387, valid recall:98.916, valid auc:99.824
[1;31mTest score increased (98.628706 --> 98.646990).[0m
[98.64698968264334, 98.6506199854121, 98.3866358369489, 98.91602455269688, 99.82360293509517]
====================================================================================================
fold:0 epoch:223 step:0 train loss:0.012835, train acc:99.588, train f1:99.590, train precision:99.587, train recall:99.593, train auc:99.989
fold:0 epoch:223 step:1 train loss:0.014773, train acc:99.463, train f1:99.461, train precision:99.345, train recall:99.577, train auc:99.987
fold:0 epoch:223 step:2 train loss:0.015852, train acc:99.445, train f1:99.443, train precision:99.346, train recall:99.541, train auc:99.982
fold:0 epoch:223 step:3 train loss:0.015464, train acc:99.411, train f1:99.412, train precision:99.579, train recall:99.246, train auc:99.987
fold:0 epoch:223 step:4 train loss:0.015323, train acc:99.435, train f1:99.437, train precision:99.573, train recall:99.301, train auc:99.984
fold:0 epoch:223 step:5 train loss:0.017932, train acc:99.377, train f1:99.378, train precision:99.178, train recall:99.578, train auc:99.978
fold:0 epoch:223 step:6 train loss:0.014754, train acc:99.472, train f1:99.471, train precision:99.304, train recall:99.639, train auc:99.985
fold:0 epoch:223 step:7 train loss:0.015805, train acc:99.445, train f1:99.446, train precision:99.385, train recall:99.506, train auc:99.984
fold:0 epoch:223 step:8 train loss:0.016545, train acc:99.429, train f1:99.428, train precision:99.565, train recall:99.291, train auc:99.983
fold:0 epoch:223 step:9 train loss:0.015315, train acc:99.499, train f1:99.499, train precision:99.385, train recall:99.613, train auc:99.983
fold:0 epoch:223        valid loss:0.061396, valid acc:98.579, valid f1:98.585, valid precision:98.161, valid recall:99.013, valid auc:99.821
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.64698968264334, 98.6506199854121, 98.3866358369489, 98.91602455269688, 99.82360293509517]
====================================================================================================
fold:0 epoch:224 step:0 train loss:0.015488, train acc:99.445, train f1:99.445, train precision:99.384, train recall:99.506, train auc:99.982
fold:0 epoch:224 step:1 train loss:0.015563, train acc:99.417, train f1:99.418, train precision:99.312, train recall:99.524, train auc:99.983
fold:0 epoch:224 step:2 train loss:0.014303, train acc:99.475, train f1:99.473, train precision:99.370, train recall:99.577, train auc:99.987
fold:0 epoch:224 step:3 train loss:0.016598, train acc:99.393, train f1:99.390, train precision:99.454, train recall:99.326, train auc:99.981
fold:0 epoch:224 step:4 train loss:0.014847, train acc:99.454, train f1:99.457, train precision:99.424, train recall:99.490, train auc:99.985
fold:0 epoch:224 step:5 train loss:0.016022, train acc:99.402, train f1:99.404, train precision:99.290, train recall:99.519, train auc:99.983
fold:0 epoch:224 step:6 train loss:0.014391, train acc:99.481, train f1:99.478, train precision:99.423, train recall:99.533, train auc:99.985
fold:0 epoch:224 step:7 train loss:0.015120, train acc:99.484, train f1:99.484, train precision:99.627, train recall:99.342, train auc:99.984
fold:0 epoch:224 step:8 train loss:0.015027, train acc:99.463, train f1:99.464, train precision:99.506, train recall:99.421, train auc:99.986
fold:0 epoch:224 step:9 train loss:0.016381, train acc:99.402, train f1:99.407, train precision:99.252, train recall:99.564, train auc:99.983
fold:0 epoch:224        valid loss:0.060003, valid acc:98.575, valid f1:98.580, valid precision:98.226, valid recall:98.937, valid auc:99.822
[1;31mEarlyStopping counter: 2 out of 50[0m
[98.64698968264334, 98.6506199854121, 98.3866358369489, 98.91602455269688, 99.82360293509517]
====================================================================================================
fold:0 epoch:225 step:0 train loss:0.013627, train acc:99.503, train f1:99.503, train precision:99.385, train recall:99.622, train auc:99.988
fold:0 epoch:225 step:1 train loss:0.015001, train acc:99.451, train f1:99.451, train precision:99.348, train recall:99.554, train auc:99.985
fold:0 epoch:225 step:2 train loss:0.015529, train acc:99.432, train f1:99.431, train precision:99.394, train recall:99.467, train auc:99.984
fold:0 epoch:225 step:3 train loss:0.013802, train acc:99.481, train f1:99.480, train precision:99.608, train recall:99.352, train auc:99.989
fold:0 epoch:225 step:4 train loss:0.015325, train acc:99.466, train f1:99.464, train precision:99.467, train recall:99.461, train auc:99.982
fold:0 epoch:225 step:5 train loss:0.015431, train acc:99.402, train f1:99.403, train precision:99.294, train recall:99.512, train auc:99.983
fold:0 epoch:225 step:6 train loss:0.014492, train acc:99.460, train f1:99.465, train precision:99.456, train recall:99.474, train auc:99.986
fold:0 epoch:225 step:7 train loss:0.015179, train acc:99.466, train f1:99.468, train precision:99.369, train recall:99.568, train auc:99.983
fold:0 epoch:225 step:8 train loss:0.014324, train acc:99.478, train f1:99.477, train precision:99.383, train recall:99.571, train auc:99.986
fold:0 epoch:225 step:9 train loss:0.017969, train acc:99.305, train f1:99.299, train precision:99.396, train recall:99.203, train auc:99.975
fold:0 epoch:225        valid loss:0.057753, valid acc:98.586, valid f1:98.592, valid precision:98.177, valid recall:99.010, valid auc:99.832
[1;31mEarlyStopping counter: 3 out of 50[0m
[98.64698968264334, 98.6506199854121, 98.3866358369489, 98.91602455269688, 99.82360293509517]
====================================================================================================
fold:0 epoch:226 step:0 train loss:0.014723, train acc:99.481, train f1:99.484, train precision:99.647, train recall:99.321, train auc:99.987
fold:0 epoch:226 step:1 train loss:0.013658, train acc:99.521, train f1:99.515, train precision:99.298, train recall:99.734, train auc:99.987
fold:0 epoch:226 step:2 train loss:0.014602, train acc:99.454, train f1:99.459, train precision:99.348, train recall:99.570, train auc:99.986
fold:0 epoch:226 step:3 train loss:0.017334, train acc:99.350, train f1:99.352, train precision:99.367, train recall:99.337, train auc:99.979
fold:0 epoch:226 step:4 train loss:0.015146, train acc:99.435, train f1:99.431, train precision:99.471, train recall:99.392, train auc:99.984
fold:0 epoch:226 step:5 train loss:0.015250, train acc:99.472, train f1:99.473, train precision:99.500, train recall:99.446, train auc:99.984
fold:0 epoch:226 step:6 train loss:0.015394, train acc:99.426, train f1:99.426, train precision:99.341, train recall:99.511, train auc:99.984
fold:0 epoch:226 step:7 train loss:0.014937, train acc:99.451, train f1:99.455, train precision:99.425, train recall:99.485, train auc:99.985
fold:0 epoch:226 step:8 train loss:0.014849, train acc:99.481, train f1:99.478, train precision:99.301, train recall:99.656, train auc:99.983
fold:0 epoch:226 step:9 train loss:0.016136, train acc:99.446, train f1:99.446, train precision:99.560, train recall:99.332, train auc:99.983
fold:0 epoch:226        valid loss:0.058105, valid acc:98.660, valid f1:98.664, valid precision:98.354, valid recall:98.976, valid auc:99.825
[1;31mTest score increased (98.646990 --> 98.660050).[0m
[98.66004962779157, 98.66427120762381, 98.35440080982168, 98.97610030037873, 99.82539755610682]
====================================================================================================
fold:0 epoch:227 step:0 train loss:0.013770, train acc:99.460, train f1:99.458, train precision:99.510, train recall:99.406, train auc:99.988
fold:0 epoch:227 step:1 train loss:0.014016, train acc:99.503, train f1:99.499, train precision:99.416, train recall:99.581, train auc:99.986
fold:0 epoch:227 step:2 train loss:0.012561, train acc:99.527, train f1:99.527, train precision:99.506, train recall:99.548, train auc:99.990
fold:0 epoch:227 step:3 train loss:0.015629, train acc:99.432, train f1:99.436, train precision:99.509, train recall:99.364, train auc:99.983
fold:0 epoch:227 step:4 train loss:0.014713, train acc:99.445, train f1:99.447, train precision:99.266, train recall:99.629, train auc:99.986
fold:0 epoch:227 step:5 train loss:0.014829, train acc:99.509, train f1:99.510, train precision:99.477, train recall:99.544, train auc:99.984
fold:0 epoch:227 step:6 train loss:0.015394, train acc:99.423, train f1:99.422, train precision:99.309, train recall:99.534, train auc:99.983
fold:0 epoch:227 step:7 train loss:0.014365, train acc:99.472, train f1:99.473, train precision:99.470, train recall:99.476, train auc:99.986
fold:0 epoch:227 step:8 train loss:0.015614, train acc:99.399, train f1:99.399, train precision:99.432, train recall:99.365, train auc:99.984
fold:0 epoch:227 step:9 train loss:0.014702, train acc:99.446, train f1:99.448, train precision:99.387, train recall:99.509, train auc:99.984
fold:0 epoch:227        valid loss:0.057805, valid acc:98.672, valid f1:98.673, valid precision:98.561, valid recall:98.785, valid auc:99.827
[1;31mTest score increased (98.660050 --> 98.671804).[0m
[98.67180357842497, 98.67331098268912, 98.56145105806317, 98.78542510121457, 99.82697051452585]
====================================================================================================
fold:0 epoch:228 step:0 train loss:0.014034, train acc:99.533, train f1:99.531, train precision:99.406, train recall:99.656, train auc:99.987
fold:0 epoch:228 step:1 train loss:0.013197, train acc:99.551, train f1:99.554, train precision:99.581, train recall:99.527, train auc:99.988
fold:0 epoch:228 step:2 train loss:0.015009, train acc:99.454, train f1:99.450, train precision:99.367, train recall:99.532, train auc:99.984
fold:0 epoch:228 step:3 train loss:0.015251, train acc:99.469, train f1:99.470, train precision:99.561, train recall:99.379, train auc:99.984
fold:0 epoch:228 step:4 train loss:0.014593, train acc:99.481, train f1:99.485, train precision:99.521, train recall:99.449, train auc:99.986
fold:0 epoch:228 step:5 train loss:0.014678, train acc:99.466, train f1:99.466, train precision:99.427, train recall:99.506, train auc:99.987
fold:0 epoch:228 step:6 train loss:0.015268, train acc:99.405, train f1:99.409, train precision:99.214, train recall:99.605, train auc:99.986
fold:0 epoch:228 step:7 train loss:0.012932, train acc:99.585, train f1:99.581, train precision:99.496, train recall:99.667, train auc:99.986
fold:0 epoch:228 step:8 train loss:0.014114, train acc:99.478, train f1:99.478, train precision:99.566, train recall:99.390, train auc:99.986
fold:0 epoch:228 step:9 train loss:0.016864, train acc:99.455, train f1:99.456, train precision:99.561, train recall:99.352, train auc:99.979
fold:0 epoch:228        valid loss:0.059475, valid acc:98.618, valid f1:98.622, valid precision:98.348, valid recall:98.898, valid auc:99.825
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.67180357842497, 98.67331098268912, 98.56145105806317, 98.78542510121457, 99.82697051452585]
====================================================================================================
fold:0 epoch:229 step:0 train loss:0.014150, train acc:99.496, train f1:99.498, train precision:99.404, train recall:99.592, train auc:99.987
fold:0 epoch:229 step:1 train loss:0.013937, train acc:99.496, train f1:99.495, train precision:99.334, train recall:99.657, train auc:99.988
fold:0 epoch:229 step:2 train loss:0.016204, train acc:99.442, train f1:99.438, train precision:99.221, train recall:99.655, train auc:99.980
fold:0 epoch:229 step:3 train loss:0.014661, train acc:99.457, train f1:99.456, train precision:99.620, train recall:99.292, train auc:99.986
fold:0 epoch:229 step:4 train loss:0.016729, train acc:99.426, train f1:99.424, train precision:99.607, train recall:99.242, train auc:99.982
fold:0 epoch:229 step:5 train loss:0.015208, train acc:99.442, train f1:99.445, train precision:99.351, train recall:99.538, train auc:99.983
fold:0 epoch:229 step:6 train loss:0.014824, train acc:99.417, train f1:99.419, train precision:99.199, train recall:99.641, train auc:99.988
fold:0 epoch:229 step:7 train loss:0.014877, train acc:99.460, train f1:99.462, train precision:99.314, train recall:99.610, train auc:99.986
fold:0 epoch:229 step:8 train loss:0.015129, train acc:99.457, train f1:99.459, train precision:99.647, train recall:99.273, train auc:99.985
fold:0 epoch:229 step:9 train loss:0.015095, train acc:99.420, train f1:99.415, train precision:99.345, train recall:99.486, train auc:99.985
fold:0 epoch:229        valid loss:0.058402, valid acc:98.623, valid f1:98.628, valid precision:98.308, valid recall:98.950, valid auc:99.832
[1;31mEarlyStopping counter: 2 out of 50[0m
[98.67180357842497, 98.67331098268912, 98.56145105806317, 98.78542510121457, 99.82697051452585]
====================================================================================================
fold:0 epoch:230 step:0 train loss:0.013274, train acc:99.533, train f1:99.528, train precision:99.562, train recall:99.494, train auc:99.988
fold:0 epoch:230 step:1 train loss:0.014459, train acc:99.490, train f1:99.494, train precision:99.539, train recall:99.448, train auc:99.984
fold:0 epoch:230 step:2 train loss:0.014088, train acc:99.460, train f1:99.462, train precision:99.411, train recall:99.513, train auc:99.987
fold:0 epoch:230 step:3 train loss:0.015186, train acc:99.445, train f1:99.448, train precision:99.303, train recall:99.593, train auc:99.985
fold:0 epoch:230 step:4 train loss:0.014313, train acc:99.445, train f1:99.444, train precision:99.426, train recall:99.462, train auc:99.987
fold:0 epoch:230 step:5 train loss:0.016738, train acc:99.411, train f1:99.410, train precision:99.504, train recall:99.316, train auc:99.981
fold:0 epoch:230 step:6 train loss:0.013233, train acc:99.521, train f1:99.524, train precision:99.515, train recall:99.534, train auc:99.988
fold:0 epoch:230 step:7 train loss:0.015482, train acc:99.457, train f1:99.454, train precision:99.308, train recall:99.601, train auc:99.985
fold:0 epoch:230 step:8 train loss:0.015026, train acc:99.466, train f1:99.464, train precision:99.516, train recall:99.412, train auc:99.985
fold:0 epoch:230 step:9 train loss:0.013465, train acc:99.551, train f1:99.551, train precision:99.559, train recall:99.542, train auc:99.987
fold:0 epoch:230        valid loss:0.060119, valid acc:98.573, valid f1:98.579, valid precision:98.109, valid recall:99.054, valid auc:99.828
[1;31mEarlyStopping counter: 3 out of 50[0m
[98.67180357842497, 98.67331098268912, 98.56145105806317, 98.78542510121457, 99.82697051452585]
====================================================================================================
fold:0 epoch:231 step:0 train loss:0.014477, train acc:99.451, train f1:99.453, train precision:99.411, train recall:99.496, train auc:99.986
fold:0 epoch:231 step:1 train loss:0.014868, train acc:99.451, train f1:99.454, train precision:99.429, train recall:99.478, train auc:99.985
fold:0 epoch:231 step:2 train loss:0.012975, train acc:99.554, train f1:99.557, train precision:99.430, train recall:99.684, train auc:99.989
fold:0 epoch:231 step:3 train loss:0.014034, train acc:99.448, train f1:99.447, train precision:99.413, train recall:99.480, train auc:99.988
fold:0 epoch:231 step:4 train loss:0.015520, train acc:99.454, train f1:99.453, train precision:99.432, train recall:99.474, train auc:99.982
fold:0 epoch:231 step:5 train loss:0.014989, train acc:99.435, train f1:99.436, train precision:99.500, train recall:99.372, train auc:99.986
fold:0 epoch:231 step:6 train loss:0.013504, train acc:99.521, train f1:99.520, train precision:99.450, train recall:99.590, train auc:99.987
fold:0 epoch:231 step:7 train loss:0.013334, train acc:99.512, train f1:99.512, train precision:99.452, train recall:99.573, train auc:99.988
fold:0 epoch:231 step:8 train loss:0.014828, train acc:99.478, train f1:99.475, train precision:99.404, train recall:99.545, train auc:99.984
fold:0 epoch:231 step:9 train loss:0.011397, train acc:99.569, train f1:99.566, train precision:99.522, train recall:99.610, train auc:99.993
fold:0 epoch:231        valid loss:0.058651, valid acc:98.620, valid f1:98.623, valid precision:98.358, valid recall:98.890, valid auc:99.828
[1;31mEarlyStopping counter: 4 out of 50[0m
[98.67180357842497, 98.67331098268912, 98.56145105806317, 98.78542510121457, 99.82697051452585]
====================================================================================================
fold:0 epoch:232 step:0 train loss:0.013998, train acc:99.536, train f1:99.536, train precision:99.572, train recall:99.500, train auc:99.986
fold:0 epoch:232 step:1 train loss:0.015166, train acc:99.454, train f1:99.453, train precision:99.481, train recall:99.426, train auc:99.983
fold:0 epoch:232 step:2 train loss:0.014226, train acc:99.500, train f1:99.502, train precision:99.533, train recall:99.472, train auc:99.985
fold:0 epoch:232 step:3 train loss:0.014258, train acc:99.469, train f1:99.472, train precision:99.333, train recall:99.611, train auc:99.987
fold:0 epoch:232 step:4 train loss:0.012874, train acc:99.521, train f1:99.521, train precision:99.379, train recall:99.664, train auc:99.990
fold:0 epoch:232 step:5 train loss:0.013566, train acc:99.515, train f1:99.513, train precision:99.412, train recall:99.613, train auc:99.986
fold:0 epoch:232 step:6 train loss:0.015001, train acc:99.472, train f1:99.464, train precision:99.467, train recall:99.461, train auc:99.984
fold:0 epoch:232 step:7 train loss:0.017029, train acc:99.326, train f1:99.332, train precision:99.558, train recall:99.107, train auc:99.983
fold:0 epoch:232 step:8 train loss:0.013377, train acc:99.521, train f1:99.524, train precision:99.461, train recall:99.587, train auc:99.988
fold:0 epoch:232 step:9 train loss:0.016046, train acc:99.446, train f1:99.438, train precision:99.251, train recall:99.624, train auc:99.984
fold:0 epoch:232        valid loss:0.057925, valid acc:98.644, valid f1:98.648, valid precision:98.374, valid recall:98.924, valid auc:99.831
[1;31mEarlyStopping counter: 5 out of 50[0m
[98.67180357842497, 98.67331098268912, 98.56145105806317, 98.78542510121457, 99.82697051452585]
====================================================================================================
fold:0 epoch:233 step:0 train loss:0.012664, train acc:99.545, train f1:99.545, train precision:99.529, train recall:99.560, train auc:99.986
fold:0 epoch:233 step:1 train loss:0.013519, train acc:99.484, train f1:99.483, train precision:99.529, train recall:99.438, train auc:99.989
fold:0 epoch:233 step:2 train loss:0.014102, train acc:99.472, train f1:99.466, train precision:99.580, train recall:99.353, train auc:99.988
fold:0 epoch:233 step:3 train loss:0.013745, train acc:99.509, train f1:99.511, train precision:99.568, train recall:99.453, train auc:99.986
fold:0 epoch:233 step:4 train loss:0.013939, train acc:99.496, train f1:99.500, train precision:99.287, train recall:99.715, train auc:99.987
fold:0 epoch:233 step:5 train loss:0.015345, train acc:99.481, train f1:99.485, train precision:99.256, train recall:99.714, train auc:99.984
fold:0 epoch:233 step:6 train loss:0.013753, train acc:99.512, train f1:99.509, train precision:99.454, train recall:99.564, train auc:99.988
fold:0 epoch:233 step:7 train loss:0.017036, train acc:99.362, train f1:99.366, train precision:99.665, train recall:99.069, train auc:99.985
fold:0 epoch:233 step:8 train loss:0.013709, train acc:99.493, train f1:99.490, train precision:99.521, train recall:99.460, train auc:99.987
fold:0 epoch:233 step:9 train loss:0.015902, train acc:99.455, train f1:99.457, train precision:99.213, train recall:99.701, train auc:99.984
fold:0 epoch:233        valid loss:0.060244, valid acc:98.637, valid f1:98.642, valid precision:98.248, valid recall:99.039, valid auc:99.827
[1;31mEarlyStopping counter: 6 out of 50[0m
[98.67180357842497, 98.67331098268912, 98.56145105806317, 98.78542510121457, 99.82697051452585]
====================================================================================================
fold:0 epoch:234 step:0 train loss:0.016969, train acc:99.405, train f1:99.401, train precision:99.154, train recall:99.649, train auc:99.983
fold:0 epoch:234 step:1 train loss:0.013309, train acc:99.533, train f1:99.532, train precision:99.535, train recall:99.529, train auc:99.987
fold:0 epoch:234 step:2 train loss:0.015282, train acc:99.472, train f1:99.473, train precision:99.658, train recall:99.289, train auc:99.985
fold:0 epoch:234 step:3 train loss:0.015768, train acc:99.417, train f1:99.416, train precision:99.620, train recall:99.212, train auc:99.985
fold:0 epoch:234 step:4 train loss:0.013614, train acc:99.500, train f1:99.502, train precision:99.411, train recall:99.593, train auc:99.988
fold:0 epoch:234 step:5 train loss:0.016497, train acc:99.390, train f1:99.389, train precision:99.045, train recall:99.736, train auc:99.986
fold:0 epoch:234 step:6 train loss:0.014868, train acc:99.496, train f1:99.500, train precision:99.370, train recall:99.630, train auc:99.985
fold:0 epoch:234 step:7 train loss:0.015632, train acc:99.466, train f1:99.466, train precision:99.646, train recall:99.287, train auc:99.984
fold:0 epoch:234 step:8 train loss:0.015363, train acc:99.435, train f1:99.435, train precision:99.572, train recall:99.299, train auc:99.986
fold:0 epoch:234 step:9 train loss:0.014440, train acc:99.543, train f1:99.543, train precision:99.421, train recall:99.666, train auc:99.980
fold:0 epoch:234        valid loss:0.059238, valid acc:98.665, valid f1:98.670, valid precision:98.327, valid recall:99.015, valid auc:99.827
[1;31mEarlyStopping counter: 7 out of 50[0m
[98.67180357842497, 98.67331098268912, 98.56145105806317, 98.78542510121457, 99.82697051452585]
====================================================================================================
fold:0 epoch:235 step:0 train loss:0.014768, train acc:99.481, train f1:99.480, train precision:99.237, train recall:99.724, train auc:99.988
fold:0 epoch:235 step:1 train loss:0.013520, train acc:99.493, train f1:99.492, train precision:99.444, train recall:99.541, train auc:99.987
fold:0 epoch:235 step:2 train loss:0.013199, train acc:99.542, train f1:99.539, train precision:99.570, train recall:99.509, train auc:99.988
fold:0 epoch:235 step:3 train loss:0.015009, train acc:99.454, train f1:99.455, train precision:99.622, train recall:99.289, train auc:99.988
fold:0 epoch:235 step:4 train loss:0.014284, train acc:99.503, train f1:99.504, train precision:99.555, train recall:99.452, train auc:99.986
fold:0 epoch:235 step:5 train loss:0.012658, train acc:99.515, train f1:99.518, train precision:99.419, train recall:99.618, train auc:99.991
fold:0 epoch:235 step:6 train loss:0.015180, train acc:99.445, train f1:99.447, train precision:99.247, train recall:99.647, train auc:99.986
fold:0 epoch:235 step:7 train loss:0.017093, train acc:99.396, train f1:99.395, train precision:99.250, train recall:99.541, train auc:99.980
fold:0 epoch:235 step:8 train loss:0.015927, train acc:99.420, train f1:99.421, train precision:99.512, train recall:99.330, train auc:99.982
fold:0 epoch:235 step:9 train loss:0.015234, train acc:99.393, train f1:99.392, train precision:99.488, train recall:99.296, train auc:99.986
fold:0 epoch:235        valid loss:0.058670, valid acc:98.657, valid f1:98.661, valid precision:98.367, valid recall:98.958, valid auc:99.828
[1;31mEarlyStopping counter: 8 out of 50[0m
[98.67180357842497, 98.67331098268912, 98.56145105806317, 98.78542510121457, 99.82697051452585]
====================================================================================================
fold:0 epoch:236 step:0 train loss:0.012722, train acc:99.521, train f1:99.523, train precision:99.610, train recall:99.435, train auc:99.990
fold:0 epoch:236 step:1 train loss:0.014153, train acc:99.496, train f1:99.499, train precision:99.357, train recall:99.641, train auc:99.986
fold:0 epoch:236 step:2 train loss:0.014949, train acc:99.463, train f1:99.461, train precision:99.242, train recall:99.681, train auc:99.987
fold:0 epoch:236 step:3 train loss:0.013903, train acc:99.521, train f1:99.523, train precision:99.375, train recall:99.671, train auc:99.985
fold:0 epoch:236 step:4 train loss:0.015792, train acc:99.438, train f1:99.438, train precision:99.657, train recall:99.219, train auc:99.985
fold:0 epoch:236 step:5 train loss:0.016528, train acc:99.423, train f1:99.419, train precision:99.447, train recall:99.392, train auc:99.981
fold:0 epoch:236 step:6 train loss:0.013814, train acc:99.451, train f1:99.455, train precision:99.317, train recall:99.594, train auc:99.988
fold:0 epoch:236 step:7 train loss:0.014256, train acc:99.445, train f1:99.445, train precision:99.263, train recall:99.627, train auc:99.988
fold:0 epoch:236 step:8 train loss:0.015341, train acc:99.448, train f1:99.446, train precision:99.394, train recall:99.498, train auc:99.984
fold:0 epoch:236 step:9 train loss:0.017328, train acc:99.393, train f1:99.395, train precision:99.282, train recall:99.508, train auc:99.979
fold:0 epoch:236        valid loss:0.060471, valid acc:98.600, valid f1:98.606, valid precision:98.200, valid recall:99.015, valid auc:99.823
[1;31mEarlyStopping counter: 9 out of 50[0m
[98.67180357842497, 98.67331098268912, 98.56145105806317, 98.78542510121457, 99.82697051452585]
====================================================================================================
fold:0 epoch:237 step:0 train loss:0.014224, train acc:99.503, train f1:99.507, train precision:99.624, train recall:99.389, train auc:99.988
fold:0 epoch:237 step:1 train loss:0.014646, train acc:99.472, train f1:99.472, train precision:99.548, train recall:99.397, train auc:99.986
fold:0 epoch:237 step:2 train loss:0.012857, train acc:99.551, train f1:99.555, train precision:99.485, train recall:99.624, train auc:99.988
fold:0 epoch:237 step:3 train loss:0.014357, train acc:99.463, train f1:99.463, train precision:99.215, train recall:99.712, train auc:99.989
fold:0 epoch:237 step:4 train loss:0.013583, train acc:99.539, train f1:99.541, train precision:99.532, train recall:99.550, train auc:99.987
fold:0 epoch:237 step:5 train loss:0.013726, train acc:99.490, train f1:99.491, train precision:99.609, train recall:99.373, train auc:99.988
fold:0 epoch:237 step:6 train loss:0.016782, train acc:99.390, train f1:99.388, train precision:99.492, train recall:99.285, train auc:99.980
fold:0 epoch:237 step:7 train loss:0.013924, train acc:99.509, train f1:99.507, train precision:99.492, train recall:99.523, train auc:99.987
fold:0 epoch:237 step:8 train loss:0.015498, train acc:99.390, train f1:99.383, train precision:99.071, train recall:99.697, train auc:99.987
fold:0 epoch:237 step:9 train loss:0.014293, train acc:99.525, train f1:99.521, train precision:99.450, train recall:99.591, train auc:99.983
fold:0 epoch:237        valid loss:0.061123, valid acc:98.593, valid f1:98.600, valid precision:98.147, valid recall:99.057, valid auc:99.825
[1;31mEarlyStopping counter: 10 out of 50[0m
[98.67180357842497, 98.67331098268912, 98.56145105806317, 98.78542510121457, 99.82697051452585]
====================================================================================================
fold:0 epoch:238 step:0 train loss:0.013657, train acc:99.484, train f1:99.483, train precision:99.504, train recall:99.462, train auc:99.988
fold:0 epoch:238 step:1 train loss:0.012395, train acc:99.548, train f1:99.547, train precision:99.632, train recall:99.462, train auc:99.991
fold:0 epoch:238 step:2 train loss:0.014136, train acc:99.481, train f1:99.481, train precision:99.609, train recall:99.354, train auc:99.987
fold:0 epoch:238 step:3 train loss:0.014176, train acc:99.472, train f1:99.473, train precision:99.343, train recall:99.604, train auc:99.987
fold:0 epoch:238 step:4 train loss:0.013872, train acc:99.536, train f1:99.543, train precision:99.375, train recall:99.711, train auc:99.985
fold:0 epoch:238 step:5 train loss:0.015310, train acc:99.399, train f1:99.400, train precision:99.324, train recall:99.475, train auc:99.984
fold:0 epoch:238 step:6 train loss:0.014401, train acc:99.460, train f1:99.460, train precision:99.457, train recall:99.463, train auc:99.986
fold:0 epoch:238 step:7 train loss:0.014353, train acc:99.423, train f1:99.416, train precision:99.524, train recall:99.309, train auc:99.987
fold:0 epoch:238 step:8 train loss:0.011811, train acc:99.564, train f1:99.564, train precision:99.646, train recall:99.483, train auc:99.992
fold:0 epoch:238 step:9 train loss:0.013997, train acc:99.516, train f1:99.514, train precision:99.575, train recall:99.452, train auc:99.985
fold:0 epoch:238        valid loss:0.057836, valid acc:98.654, valid f1:98.657, valid precision:98.377, valid recall:98.940, valid auc:99.828
[1;31mEarlyStopping counter: 11 out of 50[0m
[98.67180357842497, 98.67331098268912, 98.56145105806317, 98.78542510121457, 99.82697051452585]
====================================================================================================
fold:0 epoch:239 step:0 train loss:0.011465, train acc:99.588, train f1:99.589, train precision:99.411, train recall:99.768, train auc:99.992
fold:0 epoch:239 step:1 train loss:0.015795, train acc:99.411, train f1:99.413, train precision:99.174, train recall:99.652, train auc:99.985
fold:0 epoch:239 step:2 train loss:0.014375, train acc:99.484, train f1:99.482, train precision:99.473, train recall:99.491, train auc:99.985
fold:0 epoch:239 step:3 train loss:0.013606, train acc:99.506, train f1:99.509, train precision:99.721, train recall:99.299, train auc:99.989
fold:0 epoch:239 step:4 train loss:0.012691, train acc:99.542, train f1:99.540, train precision:99.631, train recall:99.448, train auc:99.990
fold:0 epoch:239 step:5 train loss:0.014314, train acc:99.466, train f1:99.469, train precision:99.327, train recall:99.611, train auc:99.986
fold:0 epoch:239 step:6 train loss:0.014630, train acc:99.481, train f1:99.481, train precision:99.293, train recall:99.670, train auc:99.986
fold:0 epoch:239 step:7 train loss:0.013464, train acc:99.506, train f1:99.503, train precision:99.527, train recall:99.478, train auc:99.988
fold:0 epoch:239 step:8 train loss:0.013582, train acc:99.539, train f1:99.540, train precision:99.512, train recall:99.567, train auc:99.988
fold:0 epoch:239 step:9 train loss:0.011685, train acc:99.525, train f1:99.525, train precision:99.525, train recall:99.525, train auc:99.992
fold:0 epoch:239        valid loss:0.058556, valid acc:98.638, valid f1:98.643, valid precision:98.293, valid recall:98.994, valid auc:99.828
[1;31mEarlyStopping counter: 12 out of 50[0m
[98.67180357842497, 98.67331098268912, 98.56145105806317, 98.78542510121457, 99.82697051452585]
====================================================================================================
fold:0 epoch:240 step:0 train loss:0.012803, train acc:99.542, train f1:99.540, train precision:99.583, train recall:99.498, train auc:99.989
fold:0 epoch:240 step:1 train loss:0.013706, train acc:99.512, train f1:99.509, train precision:99.418, train recall:99.601, train auc:99.986
fold:0 epoch:240 step:2 train loss:0.012628, train acc:99.557, train f1:99.559, train precision:99.477, train recall:99.641, train auc:99.989
fold:0 epoch:240 step:3 train loss:0.013251, train acc:99.545, train f1:99.549, train precision:99.528, train recall:99.570, train auc:99.988
fold:0 epoch:240 step:4 train loss:0.015725, train acc:99.402, train f1:99.399, train precision:99.290, train recall:99.509, train auc:99.984
fold:0 epoch:240 step:5 train loss:0.014225, train acc:99.496, train f1:99.497, train precision:99.519, train recall:99.476, train auc:99.987
fold:0 epoch:240 step:6 train loss:0.013443, train acc:99.509, train f1:99.511, train precision:99.489, train recall:99.532, train auc:99.988
fold:0 epoch:240 step:7 train loss:0.013182, train acc:99.500, train f1:99.501, train precision:99.507, train recall:99.494, train auc:99.988
fold:0 epoch:240 step:8 train loss:0.012094, train acc:99.533, train f1:99.533, train precision:99.561, train recall:99.506, train auc:99.990
fold:0 epoch:240 step:9 train loss:0.016910, train acc:99.349, train f1:99.341, train precision:99.112, train recall:99.572, train auc:99.978
fold:0 epoch:240        valid loss:0.058964, valid acc:98.614, valid f1:98.621, valid precision:98.178, valid recall:99.068, valid auc:99.831
[1;31mEarlyStopping counter: 13 out of 50[0m
[98.67180357842497, 98.67331098268912, 98.56145105806317, 98.78542510121457, 99.82697051452585]
====================================================================================================
fold:0 epoch:241 step:0 train loss:0.013215, train acc:99.500, train f1:99.495, train precision:99.538, train recall:99.452, train auc:99.989
fold:0 epoch:241 step:1 train loss:0.013367, train acc:99.524, train f1:99.522, train precision:99.577, train recall:99.467, train auc:99.989
fold:0 epoch:241 step:2 train loss:0.012176, train acc:99.536, train f1:99.539, train precision:99.503, train recall:99.575, train auc:99.990
fold:0 epoch:241 step:3 train loss:0.013396, train acc:99.500, train f1:99.503, train precision:99.383, train recall:99.624, train auc:99.988
fold:0 epoch:241 step:4 train loss:0.014767, train acc:99.530, train f1:99.530, train precision:99.408, train recall:99.651, train auc:99.983
fold:0 epoch:241 step:5 train loss:0.013972, train acc:99.490, train f1:99.494, train precision:99.485, train recall:99.503, train auc:99.987
fold:0 epoch:241 step:6 train loss:0.012698, train acc:99.521, train f1:99.519, train precision:99.486, train recall:99.553, train auc:99.988
fold:0 epoch:241 step:7 train loss:0.013559, train acc:99.512, train f1:99.513, train precision:99.561, train recall:99.464, train auc:99.987
fold:0 epoch:241 step:8 train loss:0.012731, train acc:99.554, train f1:99.553, train precision:99.534, train recall:99.571, train auc:99.990
fold:0 epoch:241 step:9 train loss:0.012997, train acc:99.507, train f1:99.509, train precision:99.405, train recall:99.614, train auc:99.990
fold:0 epoch:241        valid loss:0.060295, valid acc:98.661, valid f1:98.666, valid precision:98.354, valid recall:98.979, valid auc:99.830
[1;31mEarlyStopping counter: 14 out of 50[0m
[98.67180357842497, 98.67331098268912, 98.56145105806317, 98.78542510121457, 99.82697051452585]
====================================================================================================
fold:0 epoch:242 step:0 train loss:0.016045, train acc:99.408, train f1:99.410, train precision:99.313, train recall:99.507, train auc:99.983
fold:0 epoch:242 step:1 train loss:0.012641, train acc:99.548, train f1:99.546, train precision:99.454, train recall:99.638, train auc:99.989
fold:0 epoch:242 step:2 train loss:0.014774, train acc:99.417, train f1:99.413, train precision:99.440, train recall:99.385, train auc:99.986
fold:0 epoch:242 step:3 train loss:0.012021, train acc:99.582, train f1:99.583, train precision:99.586, train recall:99.580, train auc:99.991
fold:0 epoch:242 step:4 train loss:0.012168, train acc:99.521, train f1:99.521, train precision:99.396, train recall:99.645, train auc:99.991
fold:0 epoch:242 step:5 train loss:0.014244, train acc:99.484, train f1:99.488, train precision:99.359, train recall:99.618, train auc:99.986
fold:0 epoch:242 step:6 train loss:0.013854, train acc:99.484, train f1:99.484, train precision:99.469, train recall:99.500, train auc:99.987
fold:0 epoch:242 step:7 train loss:0.013230, train acc:99.512, train f1:99.515, train precision:99.545, train recall:99.485, train auc:99.988
fold:0 epoch:242 step:8 train loss:0.013482, train acc:99.533, train f1:99.536, train precision:99.497, train recall:99.575, train auc:99.988
fold:0 epoch:242 step:9 train loss:0.016767, train acc:99.358, train f1:99.345, train precision:98.999, train recall:99.694, train auc:99.984
fold:0 epoch:242        valid loss:0.056931, valid acc:98.698, valid f1:98.702, valid precision:98.421, valid recall:98.984, valid auc:99.835
[1;31mTest score increased (98.671804 --> 98.697923).[0m
[98.69792346872143, 98.70163695320944, 98.4209432786204, 98.98393626746767, 99.83494545566869]
====================================================================================================
fold:0 epoch:243 step:0 train loss:0.014136, train acc:99.506, train f1:99.505, train precision:99.627, train recall:99.384, train auc:99.988
fold:0 epoch:243 step:1 train loss:0.012699, train acc:99.533, train f1:99.536, train precision:99.617, train recall:99.454, train auc:99.990
fold:0 epoch:243 step:2 train loss:0.013522, train acc:99.506, train f1:99.504, train precision:99.316, train recall:99.693, train auc:99.988
fold:0 epoch:243 step:3 train loss:0.013147, train acc:99.484, train f1:99.485, train precision:99.397, train recall:99.573, train auc:99.989
fold:0 epoch:243 step:4 train loss:0.013978, train acc:99.500, train f1:99.499, train precision:99.444, train recall:99.554, train auc:99.987
fold:0 epoch:243 step:5 train loss:0.015659, train acc:99.426, train f1:99.429, train precision:99.471, train recall:99.387, train auc:99.984
fold:0 epoch:243 step:6 train loss:0.012033, train acc:99.570, train f1:99.570, train precision:99.513, train recall:99.628, train auc:99.990
fold:0 epoch:243 step:7 train loss:0.013291, train acc:99.515, train f1:99.516, train precision:99.500, train recall:99.531, train auc:99.988
fold:0 epoch:243 step:8 train loss:0.015671, train acc:99.426, train f1:99.422, train precision:99.312, train recall:99.532, train auc:99.984
fold:0 epoch:243 step:9 train loss:0.012836, train acc:99.516, train f1:99.518, train precision:99.561, train recall:99.474, train auc:99.990
fold:0 epoch:243        valid loss:0.059107, valid acc:98.620, valid f1:98.625, valid precision:98.260, valid recall:98.992, valid auc:99.833
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.69792346872143, 98.70163695320944, 98.4209432786204, 98.98393626746767, 99.83494545566869]
====================================================================================================
fold:0 epoch:244 step:0 train loss:0.013278, train acc:99.557, train f1:99.555, train precision:99.479, train recall:99.632, train auc:99.988
fold:0 epoch:244 step:1 train loss:0.013236, train acc:99.545, train f1:99.541, train precision:99.531, train recall:99.550, train auc:99.988
fold:0 epoch:244 step:2 train loss:0.013611, train acc:99.490, train f1:99.491, train precision:99.494, train recall:99.488, train auc:99.988
fold:0 epoch:244 step:3 train loss:0.012236, train acc:99.524, train f1:99.524, train precision:99.433, train recall:99.615, train auc:99.990
fold:0 epoch:244 step:4 train loss:0.011083, train acc:99.582, train f1:99.583, train precision:99.514, train recall:99.653, train auc:99.992
fold:0 epoch:244 step:5 train loss:0.014846, train acc:99.448, train f1:99.450, train precision:99.556, train recall:99.345, train auc:99.986
fold:0 epoch:244 step:6 train loss:0.015434, train acc:99.442, train f1:99.440, train precision:99.322, train recall:99.559, train auc:99.984
fold:0 epoch:244 step:7 train loss:0.012638, train acc:99.527, train f1:99.531, train precision:99.455, train recall:99.606, train auc:99.989
fold:0 epoch:244 step:8 train loss:0.015240, train acc:99.445, train f1:99.449, train precision:99.455, train recall:99.443, train auc:99.984
fold:0 epoch:244 step:9 train loss:0.015471, train acc:99.428, train f1:99.424, train precision:99.327, train recall:99.521, train auc:99.985
fold:0 epoch:244        valid loss:0.059089, valid acc:98.648, valid f1:98.652, valid precision:98.409, valid recall:98.895, valid auc:99.830
[1;31mEarlyStopping counter: 2 out of 50[0m
[98.69792346872143, 98.70163695320944, 98.4209432786204, 98.98393626746767, 99.83494545566869]
====================================================================================================
fold:0 epoch:245 step:0 train loss:0.014412, train acc:99.417, train f1:99.420, train precision:99.471, train recall:99.369, train auc:99.988
fold:0 epoch:245 step:1 train loss:0.012388, train acc:99.551, train f1:99.552, train precision:99.421, train recall:99.682, train auc:99.990
fold:0 epoch:245 step:2 train loss:0.012332, train acc:99.582, train f1:99.584, train precision:99.593, train recall:99.574, train auc:99.989
fold:0 epoch:245 step:3 train loss:0.013760, train acc:99.515, train f1:99.514, train precision:99.481, train recall:99.548, train auc:99.987
fold:0 epoch:245 step:4 train loss:0.014629, train acc:99.469, train f1:99.467, train precision:99.479, train recall:99.455, train auc:99.986
fold:0 epoch:245 step:5 train loss:0.014609, train acc:99.487, train f1:99.486, train precision:99.541, train recall:99.431, train auc:99.986
fold:0 epoch:245 step:6 train loss:0.012065, train acc:99.521, train f1:99.519, train precision:99.430, train recall:99.607, train auc:99.991
fold:0 epoch:245 step:7 train loss:0.013716, train acc:99.484, train f1:99.484, train precision:99.408, train recall:99.560, train auc:99.988
fold:0 epoch:245 step:8 train loss:0.013676, train acc:99.539, train f1:99.541, train precision:99.399, train recall:99.683, train auc:99.987
fold:0 epoch:245 step:9 train loss:0.014365, train acc:99.525, train f1:99.528, train precision:99.702, train recall:99.354, train auc:99.984
fold:0 epoch:245        valid loss:0.057879, valid acc:98.640, valid f1:98.644, valid precision:98.371, valid recall:98.919, valid auc:99.829
[1;31mEarlyStopping counter: 3 out of 50[0m
[98.69792346872143, 98.70163695320944, 98.4209432786204, 98.98393626746767, 99.83494545566869]
====================================================================================================
fold:0 epoch:246 step:0 train loss:0.013586, train acc:99.481, train f1:99.483, train precision:99.550, train recall:99.416, train auc:99.987
fold:0 epoch:246 step:1 train loss:0.013077, train acc:99.493, train f1:99.494, train precision:99.397, train recall:99.591, train auc:99.989
fold:0 epoch:246 step:2 train loss:0.013264, train acc:99.536, train f1:99.537, train precision:99.434, train recall:99.640, train auc:99.988
fold:0 epoch:246 step:3 train loss:0.013117, train acc:99.512, train f1:99.511, train precision:99.407, train recall:99.614, train auc:99.989
fold:0 epoch:246 step:4 train loss:0.013116, train acc:99.521, train f1:99.520, train precision:99.571, train recall:99.468, train auc:99.989
fold:0 epoch:246 step:5 train loss:0.013246, train acc:99.536, train f1:99.537, train precision:99.719, train recall:99.356, train auc:99.989
fold:0 epoch:246 step:6 train loss:0.013573, train acc:99.490, train f1:99.493, train precision:99.447, train recall:99.538, train auc:99.987
fold:0 epoch:246 step:7 train loss:0.016630, train acc:99.380, train f1:99.380, train precision:99.238, train recall:99.523, train auc:99.983
fold:0 epoch:246 step:8 train loss:0.013742, train acc:99.490, train f1:99.489, train precision:99.419, train recall:99.559, train auc:99.989
fold:0 epoch:246 step:9 train loss:0.013287, train acc:99.499, train f1:99.495, train precision:99.716, train recall:99.274, train auc:99.990
fold:0 epoch:246        valid loss:0.059396, valid acc:98.661, valid f1:98.666, valid precision:98.337, valid recall:98.997, valid auc:99.830
[1;31mEarlyStopping counter: 4 out of 50[0m
[98.69792346872143, 98.70163695320944, 98.4209432786204, 98.98393626746767, 99.83494545566869]
====================================================================================================
fold:0 epoch:247 step:0 train loss:0.013659, train acc:99.487, train f1:99.485, train precision:99.613, train recall:99.357, train auc:99.988
fold:0 epoch:247 step:1 train loss:0.012751, train acc:99.536, train f1:99.540, train precision:99.492, train recall:99.588, train auc:99.989
fold:0 epoch:247 step:2 train loss:0.015478, train acc:99.420, train f1:99.423, train precision:99.140, train recall:99.708, train auc:99.986
fold:0 epoch:247 step:3 train loss:0.013494, train acc:99.515, train f1:99.515, train precision:99.488, train recall:99.542, train auc:99.988
fold:0 epoch:247 step:4 train loss:0.013624, train acc:99.487, train f1:99.486, train precision:99.547, train recall:99.425, train auc:99.987
fold:0 epoch:247 step:5 train loss:0.013231, train acc:99.530, train f1:99.531, train precision:99.665, train recall:99.398, train auc:99.989
fold:0 epoch:247 step:6 train loss:0.015212, train acc:99.460, train f1:99.458, train precision:99.370, train recall:99.547, train auc:99.985
fold:0 epoch:247 step:7 train loss:0.014203, train acc:99.487, train f1:99.486, train precision:99.303, train recall:99.669, train auc:99.988
fold:0 epoch:247 step:8 train loss:0.012088, train acc:99.561, train f1:99.561, train precision:99.536, train recall:99.585, train auc:99.990
fold:0 epoch:247 step:9 train loss:0.016647, train acc:99.340, train f1:99.339, train precision:99.365, train recall:99.312, train auc:99.980
fold:0 epoch:247        valid loss:0.058751, valid acc:98.623, valid f1:98.628, valid precision:98.275, valid recall:98.984, valid auc:99.835
[1;31mEarlyStopping counter: 5 out of 50[0m
[98.69792346872143, 98.70163695320944, 98.4209432786204, 98.98393626746767, 99.83494545566869]
====================================================================================================
fold:0 epoch:248 step:0 train loss:0.012729, train acc:99.548, train f1:99.549, train precision:99.476, train recall:99.621, train auc:99.989
fold:0 epoch:248 step:1 train loss:0.012504, train acc:99.542, train f1:99.541, train precision:99.498, train recall:99.583, train auc:99.989
fold:0 epoch:248 step:2 train loss:0.012982, train acc:99.527, train f1:99.532, train precision:99.421, train recall:99.643, train auc:99.989
fold:0 epoch:248 step:3 train loss:0.012821, train acc:99.521, train f1:99.523, train precision:99.423, train recall:99.623, train auc:99.989
fold:0 epoch:248 step:4 train loss:0.015626, train acc:99.396, train f1:99.395, train precision:99.365, train recall:99.426, train auc:99.984
fold:0 epoch:248 step:5 train loss:0.014388, train acc:99.536, train f1:99.535, train precision:99.511, train recall:99.559, train auc:99.983
fold:0 epoch:248 step:6 train loss:0.013614, train acc:99.487, train f1:99.483, train precision:99.514, train recall:99.452, train auc:99.989
fold:0 epoch:248 step:7 train loss:0.014127, train acc:99.509, train f1:99.509, train precision:99.615, train recall:99.403, train auc:99.987
fold:0 epoch:248 step:8 train loss:0.013309, train acc:99.524, train f1:99.525, train precision:99.421, train recall:99.628, train auc:99.989
fold:0 epoch:248 step:9 train loss:0.013787, train acc:99.464, train f1:99.466, train precision:99.232, train recall:99.702, train auc:99.989
fold:0 epoch:248        valid loss:0.058007, valid acc:98.638, valid f1:98.642, valid precision:98.336, valid recall:98.950, valid auc:99.834
[1;31mEarlyStopping counter: 6 out of 50[0m
[98.69792346872143, 98.70163695320944, 98.4209432786204, 98.98393626746767, 99.83494545566869]
====================================================================================================
fold:0 epoch:249 step:0 train loss:0.014186, train acc:99.509, train f1:99.511, train precision:99.599, train recall:99.424, train auc:99.988
fold:0 epoch:249 step:1 train loss:0.014024, train acc:99.484, train f1:99.480, train precision:99.581, train recall:99.379, train auc:99.988
fold:0 epoch:249 step:2 train loss:0.013070, train acc:99.509, train f1:99.512, train precision:99.569, train recall:99.455, train auc:99.990
fold:0 epoch:249 step:3 train loss:0.012841, train acc:99.567, train f1:99.564, train precision:99.418, train recall:99.711, train auc:99.987
fold:0 epoch:249 step:4 train loss:0.013447, train acc:99.475, train f1:99.475, train precision:99.263, train recall:99.688, train auc:99.989
fold:0 epoch:249 step:5 train loss:0.012907, train acc:99.503, train f1:99.499, train precision:99.460, train recall:99.539, train auc:99.988
fold:0 epoch:249 step:6 train loss:0.014592, train acc:99.466, train f1:99.471, train precision:99.612, train recall:99.330, train auc:99.987
fold:0 epoch:249 step:7 train loss:0.013244, train acc:99.503, train f1:99.500, train precision:99.601, train recall:99.399, train auc:99.988
fold:0 epoch:249 step:8 train loss:0.013794, train acc:99.536, train f1:99.541, train precision:99.439, train recall:99.643, train auc:99.987
fold:0 epoch:249 step:9 train loss:0.014851, train acc:99.446, train f1:99.439, train precision:99.148, train recall:99.732, train auc:99.989
fold:0 epoch:249        valid loss:0.059857, valid acc:98.604, valid f1:98.610, valid precision:98.160, valid recall:99.065, valid auc:99.834
[1;31mEarlyStopping counter: 7 out of 50[0m
[98.69792346872143, 98.70163695320944, 98.4209432786204, 98.98393626746767, 99.83494545566869]
====================================================================================================
fold:0 epoch:250 step:0 train loss:0.013668, train acc:99.457, train f1:99.456, train precision:99.462, train recall:99.450, train auc:99.988
fold:0 epoch:250 step:1 train loss:0.014924, train acc:99.484, train f1:99.482, train precision:99.674, train recall:99.290, train auc:99.986
fold:0 epoch:250 step:2 train loss:0.012912, train acc:99.554, train f1:99.554, train precision:99.603, train recall:99.506, train auc:99.989
fold:0 epoch:250 step:3 train loss:0.013768, train acc:99.533, train f1:99.531, train precision:99.431, train recall:99.632, train auc:99.982
fold:0 epoch:250 step:4 train loss:0.013257, train acc:99.500, train f1:99.500, train precision:99.337, train recall:99.664, train auc:99.989
fold:0 epoch:250 step:5 train loss:0.014469, train acc:99.423, train f1:99.422, train precision:99.286, train recall:99.559, train auc:99.988
fold:0 epoch:250 step:6 train loss:0.013580, train acc:99.509, train f1:99.510, train precision:99.519, train recall:99.501, train auc:99.988
fold:0 epoch:250 step:7 train loss:0.013573, train acc:99.527, train f1:99.530, train precision:99.769, train recall:99.293, train auc:99.990
fold:0 epoch:250 step:8 train loss:0.012740, train acc:99.557, train f1:99.556, train precision:99.626, train recall:99.486, train auc:99.989
fold:0 epoch:250 step:9 train loss:0.014310, train acc:99.516, train f1:99.521, train precision:99.357, train recall:99.686, train auc:99.986
fold:0 epoch:250        valid loss:0.058408, valid acc:98.652, valid f1:98.656, valid precision:98.372, valid recall:98.942, valid auc:99.832
[1;31mEarlyStopping counter: 8 out of 50[0m
[98.69792346872143, 98.70163695320944, 98.4209432786204, 98.98393626746767, 99.83494545566869]
====================================================================================================
fold:0 epoch:251 step:0 train loss:0.014560, train acc:99.503, train f1:99.503, train precision:99.258, train recall:99.750, train auc:99.988
fold:0 epoch:251 step:1 train loss:0.011635, train acc:99.536, train f1:99.538, train precision:99.610, train recall:99.465, train auc:99.992
fold:0 epoch:251 step:2 train loss:0.014820, train acc:99.435, train f1:99.438, train precision:99.647, train recall:99.230, train auc:99.986
fold:0 epoch:251 step:3 train loss:0.014281, train acc:99.472, train f1:99.466, train precision:99.469, train recall:99.463, train auc:99.986
fold:0 epoch:251 step:4 train loss:0.011725, train acc:99.597, train f1:99.596, train precision:99.517, train recall:99.675, train auc:99.991
fold:0 epoch:251 step:5 train loss:0.013703, train acc:99.460, train f1:99.460, train precision:99.257, train recall:99.663, train auc:99.989
fold:0 epoch:251 step:6 train loss:0.016065, train acc:99.417, train f1:99.417, train precision:99.299, train recall:99.536, train auc:99.983
fold:0 epoch:251 step:7 train loss:0.013593, train acc:99.493, train f1:99.495, train precision:99.622, train recall:99.368, train auc:99.989
fold:0 epoch:251 step:8 train loss:0.014245, train acc:99.481, train f1:99.482, train precision:99.543, train recall:99.422, train auc:99.986
fold:0 epoch:251 step:9 train loss:0.012926, train acc:99.534, train f1:99.536, train precision:99.510, train recall:99.562, train auc:99.989
fold:0 epoch:251        valid loss:0.057969, valid acc:98.681, valid f1:98.684, valid precision:98.438, valid recall:98.932, valid auc:99.835
[1;31mEarlyStopping counter: 9 out of 50[0m
[98.69792346872143, 98.70163695320944, 98.4209432786204, 98.98393626746767, 99.83494545566869]
====================================================================================================
fold:0 epoch:252 step:0 train loss:0.013723, train acc:99.457, train f1:99.454, train precision:99.290, train recall:99.619, train auc:99.988
fold:0 epoch:252 step:1 train loss:0.013031, train acc:99.548, train f1:99.545, train precision:99.368, train recall:99.723, train auc:99.989
fold:0 epoch:252 step:2 train loss:0.012828, train acc:99.518, train f1:99.516, train precision:99.583, train recall:99.449, train auc:99.989
fold:0 epoch:252 step:3 train loss:0.013808, train acc:99.506, train f1:99.505, train precision:99.663, train recall:99.347, train auc:99.989
fold:0 epoch:252 step:4 train loss:0.014707, train acc:99.442, train f1:99.442, train precision:99.524, train recall:99.360, train auc:99.987
fold:0 epoch:252 step:5 train loss:0.013816, train acc:99.515, train f1:99.522, train precision:99.382, train recall:99.663, train auc:99.987
fold:0 epoch:252 step:6 train loss:0.015723, train acc:99.399, train f1:99.402, train precision:99.159, train recall:99.647, train auc:99.986
fold:0 epoch:252 step:7 train loss:0.012945, train acc:99.548, train f1:99.552, train precision:99.516, train recall:99.588, train auc:99.989
fold:0 epoch:252 step:8 train loss:0.013997, train acc:99.478, train f1:99.473, train precision:99.500, train recall:99.445, train auc:99.987
fold:0 epoch:252 step:9 train loss:0.012341, train acc:99.507, train f1:99.509, train precision:99.649, train recall:99.369, train auc:99.992
fold:0 epoch:252        valid loss:0.057580, valid acc:98.674, valid f1:98.679, valid precision:98.375, valid recall:98.984, valid auc:99.836
[1;31mEarlyStopping counter: 10 out of 50[0m
[98.69792346872143, 98.70163695320944, 98.4209432786204, 98.98393626746767, 99.83494545566869]
====================================================================================================
fold:0 epoch:253 step:0 train loss:0.012459, train acc:99.527, train f1:99.522, train precision:99.531, train recall:99.513, train auc:99.990
fold:0 epoch:253 step:1 train loss:0.013000, train acc:99.509, train f1:99.513, train precision:99.395, train recall:99.630, train auc:99.988
fold:0 epoch:253 step:2 train loss:0.012662, train acc:99.506, train f1:99.510, train precision:99.347, train recall:99.673, train auc:99.990
fold:0 epoch:253 step:3 train loss:0.011576, train acc:99.582, train f1:99.581, train precision:99.468, train recall:99.694, train auc:99.992
fold:0 epoch:253 step:4 train loss:0.012018, train acc:99.536, train f1:99.537, train precision:99.477, train recall:99.598, train auc:99.990
fold:0 epoch:253 step:5 train loss:0.013821, train acc:99.466, train f1:99.465, train precision:99.639, train recall:99.292, train auc:99.989
fold:0 epoch:253 step:6 train loss:0.012958, train acc:99.500, train f1:99.500, train precision:99.591, train recall:99.410, train auc:99.990
fold:0 epoch:253 step:7 train loss:0.011750, train acc:99.557, train f1:99.557, train precision:99.432, train recall:99.682, train auc:99.992
fold:0 epoch:253 step:8 train loss:0.013885, train acc:99.521, train f1:99.523, train precision:99.405, train recall:99.641, train auc:99.987
fold:0 epoch:253 step:9 train loss:0.015067, train acc:99.437, train f1:99.430, train precision:99.289, train recall:99.572, train auc:99.987
fold:0 epoch:253        valid loss:0.058343, valid acc:98.625, valid f1:98.630, valid precision:98.281, valid recall:98.981, valid auc:99.830
[1;31mEarlyStopping counter: 11 out of 50[0m
[98.69792346872143, 98.70163695320944, 98.4209432786204, 98.98393626746767, 99.83494545566869]
====================================================================================================
fold:0 epoch:254 step:0 train loss:0.012171, train acc:99.567, train f1:99.571, train precision:99.703, train recall:99.439, train auc:99.990
fold:0 epoch:254 step:1 train loss:0.013491, train acc:99.515, train f1:99.512, train precision:99.638, train recall:99.387, train auc:99.988
fold:0 epoch:254 step:2 train loss:0.012768, train acc:99.506, train f1:99.505, train precision:99.505, train recall:99.505, train auc:99.990
fold:0 epoch:254 step:3 train loss:0.012811, train acc:99.527, train f1:99.525, train precision:99.321, train recall:99.730, train auc:99.990
fold:0 epoch:254 step:4 train loss:0.013003, train acc:99.527, train f1:99.523, train precision:99.263, train recall:99.784, train auc:99.990
fold:0 epoch:254 step:5 train loss:0.012770, train acc:99.567, train f1:99.565, train precision:99.534, train recall:99.595, train auc:99.989
fold:0 epoch:254 step:6 train loss:0.014360, train acc:99.463, train f1:99.463, train precision:99.694, train recall:99.232, train auc:99.988
fold:0 epoch:254 step:7 train loss:0.013728, train acc:99.493, train f1:99.496, train precision:99.678, train recall:99.315, train auc:99.989
fold:0 epoch:254 step:8 train loss:0.013041, train acc:99.472, train f1:99.475, train precision:99.327, train recall:99.623, train auc:99.990
fold:0 epoch:254 step:9 train loss:0.016549, train acc:99.411, train f1:99.416, train precision:99.251, train recall:99.581, train auc:99.983
fold:0 epoch:254        valid loss:0.057375, valid acc:98.673, valid f1:98.676, valid precision:98.465, valid recall:98.887, valid auc:99.831
[1;31mEarlyStopping counter: 12 out of 50[0m
[98.69792346872143, 98.70163695320944, 98.4209432786204, 98.98393626746767, 99.83494545566869]
====================================================================================================
fold:0 epoch:255 step:0 train loss:0.012098, train acc:99.564, train f1:99.567, train precision:99.396, train recall:99.739, train auc:99.990
fold:0 epoch:255 step:1 train loss:0.013844, train acc:99.509, train f1:99.509, train precision:99.561, train recall:99.458, train auc:99.986
fold:0 epoch:255 step:2 train loss:0.013387, train acc:99.451, train f1:99.454, train precision:99.738, train recall:99.171, train auc:99.992
fold:0 epoch:255 step:3 train loss:0.012252, train acc:99.551, train f1:99.551, train precision:99.585, train recall:99.518, train auc:99.990
fold:0 epoch:255 step:4 train loss:0.013548, train acc:99.475, train f1:99.477, train precision:99.344, train recall:99.610, train auc:99.989
fold:0 epoch:255 step:5 train loss:0.013445, train acc:99.524, train f1:99.525, train precision:99.283, train recall:99.768, train auc:99.989
fold:0 epoch:255 step:6 train loss:0.014799, train acc:99.454, train f1:99.455, train precision:99.216, train recall:99.695, train auc:99.987
fold:0 epoch:255 step:7 train loss:0.015364, train acc:99.402, train f1:99.399, train precision:99.460, train recall:99.338, train auc:99.985
fold:0 epoch:255 step:8 train loss:0.016339, train acc:99.426, train f1:99.417, train precision:99.515, train recall:99.318, train auc:99.983
fold:0 epoch:255 step:9 train loss:0.015770, train acc:99.464, train f1:99.465, train precision:99.579, train recall:99.352, train auc:99.983
fold:0 epoch:255        valid loss:0.058837, valid acc:98.670, valid f1:98.674, valid precision:98.385, valid recall:98.966, valid auc:99.831
[1;31mEarlyStopping counter: 13 out of 50[0m
[98.69792346872143, 98.70163695320944, 98.4209432786204, 98.98393626746767, 99.83494545566869]
====================================================================================================
fold:0 epoch:256 step:0 train loss:0.012583, train acc:99.548, train f1:99.548, train precision:99.451, train recall:99.646, train auc:99.989
fold:0 epoch:256 step:1 train loss:0.013029, train acc:99.481, train f1:99.485, train precision:99.317, train recall:99.654, train auc:99.989
fold:0 epoch:256 step:2 train loss:0.013271, train acc:99.487, train f1:99.485, train precision:99.290, train recall:99.680, train auc:99.990
fold:0 epoch:256 step:3 train loss:0.012339, train acc:99.506, train f1:99.508, train precision:99.526, train recall:99.490, train auc:99.991
fold:0 epoch:256 step:4 train loss:0.014975, train acc:99.423, train f1:99.417, train precision:99.487, train recall:99.346, train auc:99.984
fold:0 epoch:256 step:5 train loss:0.012196, train acc:99.567, train f1:99.570, train precision:99.612, train recall:99.528, train auc:99.989
fold:0 epoch:256 step:6 train loss:0.014152, train acc:99.518, train f1:99.520, train precision:99.459, train recall:99.580, train auc:99.986
fold:0 epoch:256 step:7 train loss:0.013733, train acc:99.506, train f1:99.505, train precision:99.305, train recall:99.706, train auc:99.989
fold:0 epoch:256 step:8 train loss:0.011942, train acc:99.564, train f1:99.563, train precision:99.518, train recall:99.609, train auc:99.991
fold:0 epoch:256 step:9 train loss:0.015003, train acc:99.420, train f1:99.418, train precision:99.435, train recall:99.400, train auc:99.987
fold:0 epoch:256        valid loss:0.058476, valid acc:98.655, valid f1:98.659, valid precision:98.324, valid recall:98.997, valid auc:99.830
[1;31mEarlyStopping counter: 14 out of 50[0m
[98.69792346872143, 98.70163695320944, 98.4209432786204, 98.98393626746767, 99.83494545566869]
====================================================================================================
fold:0 epoch:257 step:0 train loss:0.012612, train acc:99.518, train f1:99.516, train precision:99.613, train recall:99.418, train auc:99.990
fold:0 epoch:257 step:1 train loss:0.013531, train acc:99.515, train f1:99.512, train precision:99.460, train recall:99.564, train auc:99.988
fold:0 epoch:257 step:2 train loss:0.013804, train acc:99.545, train f1:99.548, train precision:99.425, train recall:99.672, train auc:99.986
fold:0 epoch:257 step:3 train loss:0.012243, train acc:99.515, train f1:99.516, train precision:99.465, train recall:99.568, train auc:99.991
fold:0 epoch:257 step:4 train loss:0.012483, train acc:99.518, train f1:99.522, train precision:99.480, train recall:99.564, train auc:99.988
fold:0 epoch:257 step:5 train loss:0.011533, train acc:99.561, train f1:99.561, train precision:99.476, train recall:99.646, train auc:99.992
fold:0 epoch:257 step:6 train loss:0.011751, train acc:99.567, train f1:99.565, train precision:99.565, train recall:99.565, train auc:99.991
fold:0 epoch:257 step:7 train loss:0.014045, train acc:99.457, train f1:99.460, train precision:99.472, train recall:99.448, train auc:99.987
fold:0 epoch:257 step:8 train loss:0.014708, train acc:99.432, train f1:99.430, train precision:99.442, train recall:99.417, train auc:99.985
fold:0 epoch:257 step:9 train loss:0.013962, train acc:99.464, train f1:99.459, train precision:99.292, train recall:99.627, train auc:99.988
fold:0 epoch:257        valid loss:0.059713, valid acc:98.656, valid f1:98.661, valid precision:98.322, valid recall:99.002, valid auc:99.826
[1;31mEarlyStopping counter: 15 out of 50[0m
[98.69792346872143, 98.70163695320944, 98.4209432786204, 98.98393626746767, 99.83494545566869]
====================================================================================================
fold:0 epoch:258 step:0 train loss:0.011460, train acc:99.600, train f1:99.597, train precision:99.668, train recall:99.527, train auc:99.991
fold:0 epoch:258 step:1 train loss:0.012285, train acc:99.551, train f1:99.548, train precision:99.576, train recall:99.521, train auc:99.990
fold:0 epoch:258 step:2 train loss:0.012193, train acc:99.554, train f1:99.558, train precision:99.649, train recall:99.468, train auc:99.990
fold:0 epoch:258 step:3 train loss:0.012488, train acc:99.619, train f1:99.622, train precision:99.577, train recall:99.667, train auc:99.988
fold:0 epoch:258 step:4 train loss:0.014591, train acc:99.454, train f1:99.455, train precision:99.193, train recall:99.719, train auc:99.988
fold:0 epoch:258 step:5 train loss:0.011962, train acc:99.551, train f1:99.552, train precision:99.495, train recall:99.610, train auc:99.991
fold:0 epoch:258 step:6 train loss:0.013756, train acc:99.524, train f1:99.521, train precision:99.521, train recall:99.521, train auc:99.987
fold:0 epoch:258 step:7 train loss:0.013426, train acc:99.512, train f1:99.512, train precision:99.573, train recall:99.451, train auc:99.986
fold:0 epoch:258 step:8 train loss:0.014073, train acc:99.515, train f1:99.513, train precision:99.626, train recall:99.401, train auc:99.986
fold:0 epoch:258 step:9 train loss:0.014041, train acc:99.525, train f1:99.526, train precision:99.491, train recall:99.561, train auc:99.986
fold:0 epoch:258        valid loss:0.057817, valid acc:98.664, valid f1:98.666, valid precision:98.495, valid recall:98.838, valid auc:99.826
[1;31mEarlyStopping counter: 16 out of 50[0m
[98.69792346872143, 98.70163695320944, 98.4209432786204, 98.98393626746767, 99.83494545566869]
====================================================================================================
fold:0 epoch:259 step:0 train loss:0.012742, train acc:99.503, train f1:99.506, train precision:99.365, train recall:99.648, train auc:99.990
fold:0 epoch:259 step:1 train loss:0.011814, train acc:99.606, train f1:99.610, train precision:99.523, train recall:99.697, train auc:99.991
fold:0 epoch:259 step:2 train loss:0.012250, train acc:99.567, train f1:99.569, train precision:99.538, train recall:99.599, train auc:99.990
fold:0 epoch:259 step:3 train loss:0.011256, train acc:99.597, train f1:99.595, train precision:99.626, train recall:99.565, train auc:99.992
fold:0 epoch:259 step:4 train loss:0.013695, train acc:99.500, train f1:99.498, train precision:99.663, train recall:99.334, train auc:99.989
fold:0 epoch:259 step:5 train loss:0.011995, train acc:99.567, train f1:99.565, train precision:99.535, train recall:99.596, train auc:99.991
fold:0 epoch:259 step:6 train loss:0.013236, train acc:99.509, train f1:99.510, train precision:99.368, train recall:99.653, train auc:99.989
fold:0 epoch:259 step:7 train loss:0.013732, train acc:99.451, train f1:99.449, train precision:99.261, train recall:99.638, train auc:99.989
fold:0 epoch:259 step:8 train loss:0.011646, train acc:99.588, train f1:99.586, train precision:99.534, train recall:99.638, train auc:99.989
fold:0 epoch:259 step:9 train loss:0.012928, train acc:99.499, train f1:99.494, train precision:99.609, train recall:99.380, train auc:99.991
fold:0 epoch:259        valid loss:0.058023, valid acc:98.681, valid f1:98.685, valid precision:98.418, valid recall:98.953, valid auc:99.830
[1;31mEarlyStopping counter: 17 out of 50[0m
[98.69792346872143, 98.70163695320944, 98.4209432786204, 98.98393626746767, 99.83494545566869]
====================================================================================================
fold:0 epoch:260 step:0 train loss:0.011719, train acc:99.585, train f1:99.584, train precision:99.590, train recall:99.578, train auc:99.990
fold:0 epoch:260 step:1 train loss:0.012892, train acc:99.539, train f1:99.536, train precision:99.496, train recall:99.576, train auc:99.989
fold:0 epoch:260 step:2 train loss:0.010607, train acc:99.612, train f1:99.611, train precision:99.505, train recall:99.718, train auc:99.993
fold:0 epoch:260 step:3 train loss:0.011852, train acc:99.542, train f1:99.545, train precision:99.551, train recall:99.539, train auc:99.991
fold:0 epoch:260 step:4 train loss:0.011059, train acc:99.585, train f1:99.586, train precision:99.531, train recall:99.640, train auc:99.992
fold:0 epoch:260 step:5 train loss:0.013658, train acc:99.542, train f1:99.543, train precision:99.525, train recall:99.561, train auc:99.987
fold:0 epoch:260 step:6 train loss:0.012437, train acc:99.561, train f1:99.560, train precision:99.597, train recall:99.524, train auc:99.990
fold:0 epoch:260 step:7 train loss:0.013157, train acc:99.527, train f1:99.530, train precision:99.611, train recall:99.448, train auc:99.989
fold:0 epoch:260 step:8 train loss:0.014094, train acc:99.493, train f1:99.490, train precision:99.277, train recall:99.705, train auc:99.987
fold:0 epoch:260 step:9 train loss:0.011282, train acc:99.578, train f1:99.586, train precision:99.500, train recall:99.672, train auc:99.992
fold:0 epoch:260        valid loss:0.059581, valid acc:98.643, valid f1:98.649, valid precision:98.254, valid recall:99.047, valid auc:99.832
[1;31mEarlyStopping counter: 18 out of 50[0m
[98.69792346872143, 98.70163695320944, 98.4209432786204, 98.98393626746767, 99.83494545566869]
====================================================================================================
fold:0 epoch:261 step:0 train loss:0.013383, train acc:99.490, train f1:99.491, train precision:99.482, train recall:99.500, train auc:99.987
fold:0 epoch:261 step:1 train loss:0.014021, train acc:99.493, train f1:99.492, train precision:99.602, train recall:99.383, train auc:99.987
fold:0 epoch:261 step:2 train loss:0.012669, train acc:99.548, train f1:99.546, train precision:99.582, train recall:99.509, train auc:99.989
fold:0 epoch:261 step:3 train loss:0.012111, train acc:99.585, train f1:99.585, train precision:99.481, train recall:99.688, train auc:99.990
fold:0 epoch:261 step:4 train loss:0.011862, train acc:99.542, train f1:99.545, train precision:99.491, train recall:99.600, train auc:99.991
fold:0 epoch:261 step:5 train loss:0.014194, train acc:99.463, train f1:99.466, train precision:99.182, train recall:99.750, train auc:99.989
fold:0 epoch:261 step:6 train loss:0.015084, train acc:99.478, train f1:99.477, train precision:99.535, train recall:99.420, train auc:99.983
fold:0 epoch:261 step:7 train loss:0.012571, train acc:99.548, train f1:99.546, train precision:99.662, train recall:99.430, train auc:99.990
fold:0 epoch:261 step:8 train loss:0.013721, train acc:99.524, train f1:99.527, train precision:99.708, train recall:99.346, train auc:99.988
fold:0 epoch:261 step:9 train loss:0.013374, train acc:99.543, train f1:99.539, train precision:99.328, train recall:99.751, train auc:99.989
fold:0 epoch:261        valid loss:0.056799, valid acc:98.706, valid f1:98.709, valid precision:98.434, valid recall:98.987, valid auc:99.835
[1;31mTest score increased (98.697923 --> 98.705759).[0m
[98.70575943581036, 98.70938334310087, 98.43376623376624, 98.98654825649731, 99.83549347191214]
====================================================================================================
fold:0 epoch:262 step:0 train loss:0.012538, train acc:99.539, train f1:99.542, train precision:99.442, train recall:99.642, train auc:99.990
fold:0 epoch:262 step:1 train loss:0.011664, train acc:99.588, train f1:99.588, train precision:99.464, train recall:99.713, train auc:99.990
fold:0 epoch:262 step:2 train loss:0.012617, train acc:99.585, train f1:99.584, train precision:99.718, train recall:99.450, train auc:99.988
fold:0 epoch:262 step:3 train loss:0.012784, train acc:99.503, train f1:99.498, train precision:99.477, train recall:99.520, train auc:99.987
fold:0 epoch:262 step:4 train loss:0.011671, train acc:99.551, train f1:99.555, train precision:99.528, train recall:99.582, train auc:99.992
fold:0 epoch:262 step:5 train loss:0.013807, train acc:99.515, train f1:99.512, train precision:99.266, train recall:99.760, train auc:99.988
fold:0 epoch:262 step:6 train loss:0.014080, train acc:99.496, train f1:99.501, train precision:99.456, train recall:99.546, train auc:99.986
fold:0 epoch:262 step:7 train loss:0.013745, train acc:99.515, train f1:99.512, train precision:99.539, train recall:99.484, train auc:99.986
fold:0 epoch:262 step:8 train loss:0.014041, train acc:99.457, train f1:99.458, train precision:99.615, train recall:99.300, train auc:99.988
fold:0 epoch:262 step:9 train loss:0.011393, train acc:99.613, train f1:99.610, train precision:99.592, train recall:99.628, train auc:99.991
fold:0 epoch:262        valid loss:0.057787, valid acc:98.689, valid f1:98.692, valid precision:98.433, valid recall:98.953, valid auc:99.840
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.70575943581036, 98.70938334310087, 98.43376623376624, 98.98654825649731, 99.83549347191214]
====================================================================================================
fold:0 epoch:263 step:0 train loss:0.013938, train acc:99.518, train f1:99.520, train precision:99.388, train recall:99.653, train auc:99.985
fold:0 epoch:263 step:1 train loss:0.011965, train acc:99.551, train f1:99.554, train precision:99.449, train recall:99.660, train auc:99.991
fold:0 epoch:263 step:2 train loss:0.011887, train acc:99.539, train f1:99.542, train precision:99.539, train recall:99.545, train auc:99.991
fold:0 epoch:263 step:3 train loss:0.012460, train acc:99.564, train f1:99.558, train precision:99.648, train recall:99.469, train auc:99.990
fold:0 epoch:263 step:4 train loss:0.011738, train acc:99.588, train f1:99.589, train precision:99.586, train recall:99.592, train auc:99.991
fold:0 epoch:263 step:5 train loss:0.010407, train acc:99.649, train f1:99.647, train precision:99.601, train recall:99.693, train auc:99.992
fold:0 epoch:263 step:6 train loss:0.011265, train acc:99.564, train f1:99.565, train precision:99.453, train recall:99.677, train auc:99.992
fold:0 epoch:263 step:7 train loss:0.014041, train acc:99.496, train f1:99.497, train precision:99.398, train recall:99.598, train auc:99.986
fold:0 epoch:263 step:8 train loss:0.013075, train acc:99.530, train f1:99.529, train precision:99.505, train recall:99.553, train auc:99.988
fold:0 epoch:263 step:9 train loss:0.013200, train acc:99.525, train f1:99.525, train precision:99.630, train recall:99.420, train auc:99.990
fold:0 epoch:263        valid loss:0.057729, valid acc:98.651, valid f1:98.656, valid precision:98.274, valid recall:99.041, valid auc:99.839
[1;31mEarlyStopping counter: 2 out of 50[0m
[98.70575943581036, 98.70938334310087, 98.43376623376624, 98.98654825649731, 99.83549347191214]
====================================================================================================
fold:0 epoch:264 step:0 train loss:0.011340, train acc:99.564, train f1:99.562, train precision:99.596, train recall:99.529, train auc:99.992
fold:0 epoch:264 step:1 train loss:0.011476, train acc:99.573, train f1:99.577, train precision:99.619, train recall:99.535, train auc:99.991
fold:0 epoch:264 step:2 train loss:0.010743, train acc:99.606, train f1:99.605, train precision:99.523, train recall:99.688, train auc:99.992
fold:0 epoch:264 step:3 train loss:0.011268, train acc:99.579, train f1:99.578, train precision:99.468, train recall:99.688, train auc:99.992
fold:0 epoch:264 step:4 train loss:0.012721, train acc:99.554, train f1:99.555, train precision:99.609, train recall:99.500, train auc:99.990
fold:0 epoch:264 step:5 train loss:0.012972, train acc:99.503, train f1:99.502, train precision:99.517, train recall:99.487, train auc:99.989
fold:0 epoch:264 step:6 train loss:0.012689, train acc:99.527, train f1:99.527, train precision:99.373, train recall:99.682, train auc:99.990
fold:0 epoch:264 step:7 train loss:0.014255, train acc:99.463, train f1:99.463, train precision:99.475, train recall:99.451, train auc:99.987
fold:0 epoch:264 step:8 train loss:0.013509, train acc:99.484, train f1:99.483, train precision:99.474, train recall:99.492, train auc:99.987
fold:0 epoch:264 step:9 train loss:0.010978, train acc:99.604, train f1:99.604, train precision:99.508, train recall:99.701, train auc:99.991
fold:0 epoch:264        valid loss:0.055277, valid acc:98.706, valid f1:98.709, valid precision:98.436, valid recall:98.984, valid auc:99.847
[1;31mEarlyStopping counter: 3 out of 50[0m
[98.70575943581036, 98.70938334310087, 98.43376623376624, 98.98654825649731, 99.83549347191214]
====================================================================================================
fold:0 epoch:265 step:0 train loss:0.012513, train acc:99.567, train f1:99.565, train precision:99.534, train recall:99.595, train auc:99.988
fold:0 epoch:265 step:1 train loss:0.012631, train acc:99.548, train f1:99.549, train precision:99.525, train recall:99.573, train auc:99.990
fold:0 epoch:265 step:2 train loss:0.012710, train acc:99.554, train f1:99.553, train precision:99.657, train recall:99.450, train auc:99.990
fold:0 epoch:265 step:3 train loss:0.013216, train acc:99.545, train f1:99.546, train precision:99.603, train recall:99.488, train auc:99.989
fold:0 epoch:265 step:4 train loss:0.012278, train acc:99.536, train f1:99.534, train precision:99.364, train recall:99.705, train auc:99.991
fold:0 epoch:265 step:5 train loss:0.012044, train acc:99.545, train f1:99.548, train precision:99.412, train recall:99.684, train auc:99.991
fold:0 epoch:265 step:6 train loss:0.011802, train acc:99.573, train f1:99.574, train precision:99.580, train recall:99.568, train auc:99.990
fold:0 epoch:265 step:7 train loss:0.013110, train acc:99.527, train f1:99.529, train precision:99.635, train recall:99.424, train auc:99.989
fold:0 epoch:265 step:8 train loss:0.013812, train acc:99.530, train f1:99.527, train precision:99.424, train recall:99.631, train auc:99.986
fold:0 epoch:265 step:9 train loss:0.014517, train acc:99.499, train f1:99.504, train precision:99.565, train recall:99.444, train auc:99.987
fold:0 epoch:265        valid loss:0.060697, valid acc:98.646, valid f1:98.652, valid precision:98.219, valid recall:99.088, valid auc:99.838
[1;31mEarlyStopping counter: 4 out of 50[0m
[98.70575943581036, 98.70938334310087, 98.43376623376624, 98.98654825649731, 99.83549347191214]
====================================================================================================
fold:0 epoch:266 step:0 train loss:0.010950, train acc:99.579, train f1:99.577, train precision:99.357, train recall:99.797, train auc:99.993
fold:0 epoch:266 step:1 train loss:0.014086, train acc:99.509, train f1:99.506, train precision:99.400, train recall:99.613, train auc:99.985
fold:0 epoch:266 step:2 train loss:0.012117, train acc:99.597, train f1:99.595, train precision:99.564, train recall:99.625, train auc:99.990
fold:0 epoch:266 step:3 train loss:0.013311, train acc:99.496, train f1:99.492, train precision:99.679, train recall:99.306, train auc:99.990
fold:0 epoch:266 step:4 train loss:0.013671, train acc:99.521, train f1:99.521, train precision:99.543, train recall:99.500, train auc:99.984
fold:0 epoch:266 step:5 train loss:0.013685, train acc:99.545, train f1:99.545, train precision:99.421, train recall:99.670, train auc:99.988
fold:0 epoch:266 step:6 train loss:0.012699, train acc:99.579, train f1:99.583, train precision:99.415, train recall:99.752, train auc:99.989
fold:0 epoch:266 step:7 train loss:0.011364, train acc:99.619, train f1:99.623, train precision:99.632, train recall:99.614, train auc:99.992
fold:0 epoch:266 step:8 train loss:0.013297, train acc:99.515, train f1:99.517, train precision:99.617, train recall:99.417, train auc:99.989
fold:0 epoch:266 step:9 train loss:0.011343, train acc:99.666, train f1:99.668, train precision:99.546, train recall:99.790, train auc:99.990
fold:0 epoch:266        valid loss:0.057254, valid acc:98.695, valid f1:98.701, valid precision:98.300, valid recall:99.104, valid auc:99.844
[1;31mEarlyStopping counter: 5 out of 50[0m
[98.70575943581036, 98.70938334310087, 98.43376623376624, 98.98654825649731, 99.83549347191214]
====================================================================================================
fold:0 epoch:267 step:0 train loss:0.012920, train acc:99.536, train f1:99.537, train precision:99.464, train recall:99.610, train auc:99.988
fold:0 epoch:267 step:1 train loss:0.012524, train acc:99.557, train f1:99.560, train precision:99.605, train recall:99.514, train auc:99.989
fold:0 epoch:267 step:2 train loss:0.011884, train acc:99.548, train f1:99.545, train precision:99.423, train recall:99.668, train auc:99.990
fold:0 epoch:267 step:3 train loss:0.012788, train acc:99.561, train f1:99.561, train precision:99.604, train recall:99.519, train auc:99.988
fold:0 epoch:267 step:4 train loss:0.014035, train acc:99.500, train f1:99.502, train precision:99.526, train recall:99.478, train auc:99.985
fold:0 epoch:267 step:5 train loss:0.012483, train acc:99.542, train f1:99.540, train precision:99.424, train recall:99.656, train auc:99.991
fold:0 epoch:267 step:6 train loss:0.012267, train acc:99.521, train f1:99.523, train precision:99.454, train recall:99.593, train auc:99.990
fold:0 epoch:267 step:7 train loss:0.012545, train acc:99.524, train f1:99.525, train precision:99.550, train recall:99.501, train auc:99.990
fold:0 epoch:267 step:8 train loss:0.010387, train acc:99.622, train f1:99.619, train precision:99.619, train recall:99.619, train auc:99.993
fold:0 epoch:267 step:9 train loss:0.012809, train acc:99.481, train f1:99.478, train precision:99.504, train recall:99.451, train auc:99.990
fold:0 epoch:267        valid loss:0.055746, valid acc:98.712, valid f1:98.716, valid precision:98.447, valid recall:98.987, valid auc:99.845
[1;31mTest score increased (98.705759 --> 98.712289).[0m
[98.71228940838448, 98.71581140922116, 98.44655150019483, 98.98654825649731, 99.84477140567566]
====================================================================================================
fold:0 epoch:268 step:0 train loss:0.012436, train acc:99.542, train f1:99.539, train precision:99.551, train recall:99.527, train auc:99.989
fold:0 epoch:268 step:1 train loss:0.010554, train acc:99.667, train f1:99.667, train precision:99.652, train recall:99.682, train auc:99.991
fold:0 epoch:268 step:2 train loss:0.013695, train acc:99.500, train f1:99.499, train precision:99.420, train recall:99.578, train auc:99.987
fold:0 epoch:268 step:3 train loss:0.011226, train acc:99.594, train f1:99.592, train precision:99.565, train recall:99.620, train auc:99.990
fold:0 epoch:268 step:4 train loss:0.010847, train acc:99.603, train f1:99.602, train precision:99.651, train recall:99.554, train auc:99.992
fold:0 epoch:268 step:5 train loss:0.012873, train acc:99.542, train f1:99.543, train precision:99.562, train recall:99.525, train auc:99.988
fold:0 epoch:268 step:6 train loss:0.012829, train acc:99.539, train f1:99.542, train precision:99.467, train recall:99.618, train auc:99.989
fold:0 epoch:268 step:7 train loss:0.012151, train acc:99.548, train f1:99.553, train precision:99.474, train recall:99.631, train auc:99.990
fold:0 epoch:268 step:8 train loss:0.011031, train acc:99.637, train f1:99.639, train precision:99.504, train recall:99.775, train auc:99.989
fold:0 epoch:268 step:9 train loss:0.014504, train acc:99.499, train f1:99.490, train precision:99.445, train recall:99.534, train auc:99.988
fold:0 epoch:268        valid loss:0.054632, valid acc:98.720, valid f1:98.724, valid precision:98.462, valid recall:98.987, valid auc:99.849
[1;31mTest score increased (98.712289 --> 98.720125).[0m
[98.72012537547342, 98.7235261937635, 98.46189820468186, 98.98654825649731, 99.84880172146349]
====================================================================================================
fold:0 epoch:269 step:0 train loss:0.010943, train acc:99.597, train f1:99.593, train precision:99.741, train recall:99.446, train auc:99.992
fold:0 epoch:269 step:1 train loss:0.013333, train acc:99.460, train f1:99.459, train precision:99.632, train recall:99.285, train auc:99.989
fold:0 epoch:269 step:2 train loss:0.011942, train acc:99.551, train f1:99.549, train precision:99.589, train recall:99.509, train auc:99.991
fold:0 epoch:269 step:3 train loss:0.012659, train acc:99.536, train f1:99.537, train precision:99.258, train recall:99.817, train auc:99.991
fold:0 epoch:269 step:4 train loss:0.012218, train acc:99.561, train f1:99.561, train precision:99.392, train recall:99.732, train auc:99.990
fold:0 epoch:269 step:5 train loss:0.011872, train acc:99.579, train f1:99.581, train precision:99.508, train recall:99.653, train auc:99.990
fold:0 epoch:269 step:6 train loss:0.013561, train acc:99.512, train f1:99.516, train precision:99.757, train recall:99.275, train auc:99.990
fold:0 epoch:269 step:7 train loss:0.015413, train acc:99.469, train f1:99.470, train precision:99.585, train recall:99.354, train auc:99.985
fold:0 epoch:269 step:8 train loss:0.012646, train acc:99.539, train f1:99.539, train precision:99.294, train recall:99.786, train auc:99.989
fold:0 epoch:269 step:9 train loss:0.013208, train acc:99.507, train f1:99.510, train precision:99.301, train recall:99.719, train auc:99.989
fold:0 epoch:269        valid loss:0.058627, valid acc:98.718, valid f1:98.720, valid precision:98.512, valid recall:98.929, valid auc:99.838
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.72012537547342, 98.7235261937635, 98.46189820468186, 98.98654825649731, 99.84880172146349]
====================================================================================================
fold:0 epoch:270 step:0 train loss:0.012028, train acc:99.539, train f1:99.542, train precision:99.473, train recall:99.612, train auc:99.991
fold:0 epoch:270 step:1 train loss:0.013576, train acc:99.496, train f1:99.495, train precision:99.559, train recall:99.431, train auc:99.987
fold:0 epoch:270 step:2 train loss:0.011121, train acc:99.628, train f1:99.630, train precision:99.715, train recall:99.545, train auc:99.990
fold:0 epoch:270 step:3 train loss:0.011390, train acc:99.588, train f1:99.585, train precision:99.637, train recall:99.533, train auc:99.991
fold:0 epoch:270 step:4 train loss:0.012552, train acc:99.539, train f1:99.542, train precision:99.521, train recall:99.563, train auc:99.990
fold:0 epoch:270 step:5 train loss:0.013021, train acc:99.515, train f1:99.514, train precision:99.371, train recall:99.657, train auc:99.989
fold:0 epoch:270 step:6 train loss:0.012460, train acc:99.551, train f1:99.551, train precision:99.481, train recall:99.621, train auc:99.990
fold:0 epoch:270 step:7 train loss:0.012012, train acc:99.570, train f1:99.568, train precision:99.626, train recall:99.510, train auc:99.991
fold:0 epoch:270 step:8 train loss:0.010916, train acc:99.643, train f1:99.643, train precision:99.646, train recall:99.640, train auc:99.992
fold:0 epoch:270 step:9 train loss:0.013404, train acc:99.560, train f1:99.556, train precision:99.486, train recall:99.627, train auc:99.985
fold:0 epoch:270        valid loss:0.058495, valid acc:98.651, valid f1:98.656, valid precision:98.261, valid recall:99.054, valid auc:99.841
[1;31mEarlyStopping counter: 2 out of 50[0m
[98.72012537547342, 98.7235261937635, 98.46189820468186, 98.98654825649731, 99.84880172146349]
====================================================================================================
fold:0 epoch:271 step:0 train loss:0.012314, train acc:99.539, train f1:99.537, train precision:99.527, train recall:99.546, train auc:99.990
fold:0 epoch:271 step:1 train loss:0.012244, train acc:99.509, train f1:99.511, train precision:99.580, train recall:99.441, train auc:99.991
fold:0 epoch:271 step:2 train loss:0.013676, train acc:99.530, train f1:99.529, train precision:99.456, train recall:99.602, train auc:99.987
fold:0 epoch:271 step:3 train loss:0.012371, train acc:99.561, train f1:99.560, train precision:99.391, train recall:99.731, train auc:99.989
fold:0 epoch:271 step:4 train loss:0.013634, train acc:99.481, train f1:99.483, train precision:99.471, train recall:99.495, train auc:99.988
fold:0 epoch:271 step:5 train loss:0.011509, train acc:99.588, train f1:99.589, train precision:99.713, train recall:99.465, train auc:99.992
fold:0 epoch:271 step:6 train loss:0.012353, train acc:99.570, train f1:99.570, train precision:99.616, train recall:99.525, train auc:99.989
fold:0 epoch:271 step:7 train loss:0.013115, train acc:99.536, train f1:99.535, train precision:99.371, train recall:99.700, train auc:99.990
fold:0 epoch:271 step:8 train loss:0.011211, train acc:99.567, train f1:99.566, train precision:99.408, train recall:99.725, train auc:99.993
fold:0 epoch:271 step:9 train loss:0.013587, train acc:99.464, train f1:99.470, train precision:99.392, train recall:99.547, train auc:99.987
fold:0 epoch:271        valid loss:0.056817, valid acc:98.673, valid f1:98.678, valid precision:98.325, valid recall:99.034, valid auc:99.843
[1;31mEarlyStopping counter: 3 out of 50[0m
[98.72012537547342, 98.7235261937635, 98.46189820468186, 98.98654825649731, 99.84880172146349]
====================================================================================================
fold:0 epoch:272 step:0 train loss:0.011041, train acc:99.625, train f1:99.628, train precision:99.746, train recall:99.511, train auc:99.993
fold:0 epoch:272 step:1 train loss:0.012499, train acc:99.533, train f1:99.533, train precision:99.463, train recall:99.603, train auc:99.990
fold:0 epoch:272 step:2 train loss:0.012110, train acc:99.548, train f1:99.551, train precision:99.520, train recall:99.581, train auc:99.991
fold:0 epoch:272 step:3 train loss:0.013465, train acc:99.500, train f1:99.499, train precision:99.432, train recall:99.566, train auc:99.987
fold:0 epoch:272 step:4 train loss:0.011688, train acc:99.561, train f1:99.559, train precision:99.449, train recall:99.669, train auc:99.990
fold:0 epoch:272 step:5 train loss:0.013104, train acc:99.515, train f1:99.511, train precision:99.465, train recall:99.557, train auc:99.988
fold:0 epoch:272 step:6 train loss:0.012177, train acc:99.564, train f1:99.565, train precision:99.714, train recall:99.417, train auc:99.992
fold:0 epoch:272 step:7 train loss:0.013176, train acc:99.524, train f1:99.522, train precision:99.614, train recall:99.431, train auc:99.989
fold:0 epoch:272 step:8 train loss:0.014197, train acc:99.484, train f1:99.485, train precision:99.313, train recall:99.658, train auc:99.987
fold:0 epoch:272 step:9 train loss:0.015603, train acc:99.437, train f1:99.436, train precision:99.227, train recall:99.647, train auc:99.985
fold:0 epoch:272        valid loss:0.056876, valid acc:98.689, valid f1:98.693, valid precision:98.393, valid recall:98.994, valid auc:99.841
[1;31mEarlyStopping counter: 4 out of 50[0m
[98.72012537547342, 98.7235261937635, 98.46189820468186, 98.98654825649731, 99.84880172146349]
====================================================================================================
fold:0 epoch:273 step:0 train loss:0.012738, train acc:99.567, train f1:99.568, train precision:99.587, train recall:99.550, train auc:99.988
fold:0 epoch:273 step:1 train loss:0.012293, train acc:99.524, train f1:99.523, train precision:99.560, train recall:99.487, train auc:99.990
fold:0 epoch:273 step:2 train loss:0.013027, train acc:99.515, train f1:99.518, train precision:99.599, train recall:99.436, train auc:99.989
fold:0 epoch:273 step:3 train loss:0.013107, train acc:99.515, train f1:99.511, train precision:99.324, train recall:99.698, train auc:99.989
fold:0 epoch:273 step:4 train loss:0.012467, train acc:99.542, train f1:99.543, train precision:99.409, train recall:99.676, train auc:99.988
fold:0 epoch:273 step:5 train loss:0.013147, train acc:99.490, train f1:99.486, train precision:99.587, train recall:99.385, train auc:99.989
fold:0 epoch:273 step:6 train loss:0.012805, train acc:99.512, train f1:99.516, train precision:99.588, train recall:99.444, train auc:99.990
fold:0 epoch:273 step:7 train loss:0.011176, train acc:99.609, train f1:99.608, train precision:99.614, train recall:99.602, train auc:99.991
fold:0 epoch:273 step:8 train loss:0.012071, train acc:99.567, train f1:99.566, train precision:99.554, train recall:99.579, train auc:99.990
fold:0 epoch:273 step:9 train loss:0.014407, train acc:99.534, train f1:99.538, train precision:99.494, train recall:99.581, train auc:99.985
fold:0 epoch:273        valid loss:0.058349, valid acc:98.674, valid f1:98.678, valid precision:98.382, valid recall:98.976, valid auc:99.832
[1;31mEarlyStopping counter: 5 out of 50[0m
[98.72012537547342, 98.7235261937635, 98.46189820468186, 98.98654825649731, 99.84880172146349]
====================================================================================================
fold:0 epoch:274 step:0 train loss:0.012833, train acc:99.551, train f1:99.549, train precision:99.399, train recall:99.699, train auc:99.989
fold:0 epoch:274 step:1 train loss:0.011611, train acc:99.582, train f1:99.582, train precision:99.536, train recall:99.628, train auc:99.991
fold:0 epoch:274 step:2 train loss:0.011796, train acc:99.600, train f1:99.602, train precision:99.647, train recall:99.556, train auc:99.991
fold:0 epoch:274 step:3 train loss:0.012280, train acc:99.542, train f1:99.544, train precision:99.647, train recall:99.441, train auc:99.990
fold:0 epoch:274 step:4 train loss:0.012300, train acc:99.557, train f1:99.554, train precision:99.460, train recall:99.649, train auc:99.990
fold:0 epoch:274 step:5 train loss:0.014222, train acc:99.487, train f1:99.487, train precision:99.335, train recall:99.639, train auc:99.986
fold:0 epoch:274 step:6 train loss:0.011377, train acc:99.564, train f1:99.565, train precision:99.592, train recall:99.538, train auc:99.992
fold:0 epoch:274 step:7 train loss:0.012011, train acc:99.573, train f1:99.576, train precision:99.703, train recall:99.449, train auc:99.991
fold:0 epoch:274 step:8 train loss:0.012421, train acc:99.536, train f1:99.535, train precision:99.383, train recall:99.687, train auc:99.990
fold:0 epoch:274 step:9 train loss:0.013545, train acc:99.525, train f1:99.528, train precision:99.389, train recall:99.667, train auc:99.988
fold:0 epoch:274        valid loss:0.057797, valid acc:98.699, valid f1:98.704, valid precision:98.371, valid recall:99.039, valid auc:99.834
[1;31mEarlyStopping counter: 6 out of 50[0m
[98.72012537547342, 98.7235261937635, 98.46189820468186, 98.98654825649731, 99.84880172146349]
====================================================================================================
fold:0 epoch:275 step:0 train loss:0.011940, train acc:99.564, train f1:99.565, train precision:99.610, train recall:99.520, train auc:99.990
fold:0 epoch:275 step:1 train loss:0.012372, train acc:99.576, train f1:99.575, train precision:99.603, train recall:99.548, train auc:99.988
fold:0 epoch:275 step:2 train loss:0.012320, train acc:99.530, train f1:99.531, train precision:99.446, train recall:99.616, train auc:99.989
fold:0 epoch:275 step:3 train loss:0.011498, train acc:99.594, train f1:99.594, train precision:99.567, train recall:99.622, train auc:99.991
fold:0 epoch:275 step:4 train loss:0.012613, train acc:99.539, train f1:99.542, train precision:99.557, train recall:99.527, train auc:99.989
fold:0 epoch:275 step:5 train loss:0.012238, train acc:99.579, train f1:99.580, train precision:99.544, train recall:99.616, train auc:99.989
fold:0 epoch:275 step:6 train loss:0.011603, train acc:99.573, train f1:99.573, train precision:99.537, train recall:99.610, train auc:99.991
fold:0 epoch:275 step:7 train loss:0.011465, train acc:99.588, train f1:99.587, train precision:99.511, train recall:99.663, train auc:99.991
fold:0 epoch:275 step:8 train loss:0.012127, train acc:99.564, train f1:99.561, train precision:99.485, train recall:99.637, train auc:99.989
fold:0 epoch:275 step:9 train loss:0.015335, train acc:99.490, train f1:99.487, train precision:99.593, train recall:99.382, train auc:99.984
fold:0 epoch:275        valid loss:0.056693, valid acc:98.676, valid f1:98.680, valid precision:98.347, valid recall:99.015, valid auc:99.835
[1;31mEarlyStopping counter: 7 out of 50[0m
[98.72012537547342, 98.7235261937635, 98.46189820468186, 98.98654825649731, 99.84880172146349]
====================================================================================================
fold:0 epoch:276 step:0 train loss:0.012025, train acc:99.551, train f1:99.546, train precision:99.524, train recall:99.567, train auc:99.990
fold:0 epoch:276 step:1 train loss:0.010744, train acc:99.609, train f1:99.609, train precision:99.701, train recall:99.518, train auc:99.992
fold:0 epoch:276 step:2 train loss:0.012932, train acc:99.557, train f1:99.561, train precision:99.564, train recall:99.558, train auc:99.988
fold:0 epoch:276 step:3 train loss:0.010890, train acc:99.609, train f1:99.608, train precision:99.462, train recall:99.755, train auc:99.993
fold:0 epoch:276 step:4 train loss:0.012419, train acc:99.524, train f1:99.525, train precision:99.422, train recall:99.628, train auc:99.991
fold:0 epoch:276 step:5 train loss:0.011872, train acc:99.521, train f1:99.522, train precision:99.488, train recall:99.555, train auc:99.991
fold:0 epoch:276 step:6 train loss:0.010845, train acc:99.591, train f1:99.589, train precision:99.632, train recall:99.546, train auc:99.992
fold:0 epoch:276 step:7 train loss:0.011776, train acc:99.561, train f1:99.564, train precision:99.727, train recall:99.402, train auc:99.991
fold:0 epoch:276 step:8 train loss:0.012950, train acc:99.539, train f1:99.540, train precision:99.459, train recall:99.622, train auc:99.986
fold:0 epoch:276 step:9 train loss:0.012419, train acc:99.604, train f1:99.602, train precision:99.470, train recall:99.734, train auc:99.990
fold:0 epoch:276        valid loss:0.056666, valid acc:98.731, valid f1:98.733, valid precision:98.528, valid recall:98.940, valid auc:99.834
[1;31mTest score increased (98.720125 --> 98.730573).[0m
[98.73057333159201, 98.73322038316174, 98.52776693978412, 98.9395324539637, 99.8336736418122]
====================================================================================================
fold:0 epoch:277 step:0 train loss:0.011394, train acc:99.615, train f1:99.616, train precision:99.562, train recall:99.671, train auc:99.991
fold:0 epoch:277 step:1 train loss:0.011928, train acc:99.551, train f1:99.545, train precision:99.536, train recall:99.554, train auc:99.990
fold:0 epoch:277 step:2 train loss:0.012693, train acc:99.527, train f1:99.527, train precision:99.652, train recall:99.402, train auc:99.991
fold:0 epoch:277 step:3 train loss:0.012360, train acc:99.518, train f1:99.522, train precision:99.546, train recall:99.498, train auc:99.990
fold:0 epoch:277 step:4 train loss:0.011206, train acc:99.573, train f1:99.573, train precision:99.464, train recall:99.683, train auc:99.992
fold:0 epoch:277 step:5 train loss:0.012220, train acc:99.533, train f1:99.532, train precision:99.359, train recall:99.706, train auc:99.990
fold:0 epoch:277 step:6 train loss:0.012386, train acc:99.606, train f1:99.609, train precision:99.606, train recall:99.612, train auc:99.989
fold:0 epoch:277 step:7 train loss:0.013103, train acc:99.557, train f1:99.555, train precision:99.681, train recall:99.430, train auc:99.989
fold:0 epoch:277 step:8 train loss:0.012291, train acc:99.548, train f1:99.550, train precision:99.665, train recall:99.435, train auc:99.990
fold:0 epoch:277 step:9 train loss:0.012900, train acc:99.578, train f1:99.578, train precision:99.543, train recall:99.613, train auc:99.990
fold:0 epoch:277        valid loss:0.057409, valid acc:98.720, valid f1:98.722, valid precision:98.598, valid recall:98.846, valid auc:99.834
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.73057333159201, 98.73322038316174, 98.52776693978412, 98.9395324539637, 99.8336736418122]
====================================================================================================
fold:0 epoch:278 step:0 train loss:0.011681, train acc:99.576, train f1:99.579, train precision:99.425, train recall:99.733, train auc:99.991
fold:0 epoch:278 step:1 train loss:0.012675, train acc:99.545, train f1:99.546, train precision:99.325, train recall:99.768, train auc:99.991
fold:0 epoch:278 step:2 train loss:0.011702, train acc:99.594, train f1:99.593, train precision:99.554, train recall:99.633, train auc:99.990
fold:0 epoch:278 step:3 train loss:0.011603, train acc:99.582, train f1:99.579, train precision:99.686, train recall:99.472, train auc:99.992
fold:0 epoch:278 step:4 train loss:0.011787, train acc:99.561, train f1:99.560, train precision:99.639, train recall:99.481, train auc:99.991
fold:0 epoch:278 step:5 train loss:0.011028, train acc:99.585, train f1:99.583, train precision:99.479, train recall:99.687, train auc:99.992
fold:0 epoch:278 step:6 train loss:0.011214, train acc:99.588, train f1:99.590, train precision:99.496, train recall:99.684, train auc:99.992
fold:0 epoch:278 step:7 train loss:0.011361, train acc:99.557, train f1:99.559, train precision:99.405, train recall:99.714, train auc:99.992
fold:0 epoch:278 step:8 train loss:0.011648, train acc:99.554, train f1:99.555, train precision:99.373, train recall:99.737, train auc:99.992
fold:0 epoch:278 step:9 train loss:0.012937, train acc:99.446, train f1:99.455, train precision:99.809, train recall:99.103, train auc:99.993
fold:0 epoch:278        valid loss:0.057695, valid acc:98.694, valid f1:98.699, valid precision:98.310, valid recall:99.091, valid auc:99.845
[1;31mEarlyStopping counter: 2 out of 50[0m
[98.73057333159201, 98.73322038316174, 98.52776693978412, 98.9395324539637, 99.8336736418122]
====================================================================================================
fold:0 epoch:279 step:0 train loss:0.011521, train acc:99.579, train f1:99.579, train precision:99.622, train recall:99.537, train auc:99.992
fold:0 epoch:279 step:1 train loss:0.011002, train acc:99.579, train f1:99.578, train precision:99.517, train recall:99.639, train auc:99.991
fold:0 epoch:279 step:2 train loss:0.012812, train acc:99.533, train f1:99.533, train precision:99.330, train recall:99.737, train auc:99.991
fold:0 epoch:279 step:3 train loss:0.011443, train acc:99.536, train f1:99.538, train precision:99.502, train recall:99.574, train auc:99.992
fold:0 epoch:279 step:4 train loss:0.011032, train acc:99.591, train f1:99.591, train precision:99.548, train recall:99.634, train auc:99.992
fold:0 epoch:279 step:5 train loss:0.012921, train acc:99.545, train f1:99.547, train precision:99.689, train recall:99.404, train auc:99.989
fold:0 epoch:279 step:6 train loss:0.010781, train acc:99.585, train f1:99.585, train precision:99.603, train recall:99.567, train auc:99.992
fold:0 epoch:279 step:7 train loss:0.012506, train acc:99.570, train f1:99.566, train precision:99.538, train recall:99.594, train auc:99.989
fold:0 epoch:279 step:8 train loss:0.011324, train acc:99.606, train f1:99.606, train precision:99.549, train recall:99.664, train auc:99.991
fold:0 epoch:279 step:9 train loss:0.013999, train acc:99.481, train f1:99.489, train precision:99.342, train recall:99.635, train auc:99.988
fold:0 epoch:279        valid loss:0.058870, valid acc:98.677, valid f1:98.681, valid precision:98.383, valid recall:98.981, valid auc:99.835
[1;31mEarlyStopping counter: 3 out of 50[0m
[98.73057333159201, 98.73322038316174, 98.52776693978412, 98.9395324539637, 99.8336736418122]
====================================================================================================
fold:0 epoch:280 step:0 train loss:0.011943, train acc:99.536, train f1:99.533, train precision:99.453, train recall:99.612, train auc:99.990
fold:0 epoch:280 step:1 train loss:0.011802, train acc:99.533, train f1:99.531, train precision:99.571, train recall:99.491, train auc:99.991
fold:0 epoch:280 step:2 train loss:0.012975, train acc:99.515, train f1:99.519, train precision:99.679, train recall:99.359, train auc:99.991
fold:0 epoch:280 step:3 train loss:0.011634, train acc:99.628, train f1:99.628, train precision:99.622, train recall:99.634, train auc:99.990
fold:0 epoch:280 step:4 train loss:0.014003, train acc:99.503, train f1:99.501, train precision:99.334, train recall:99.669, train auc:99.988
fold:0 epoch:280 step:5 train loss:0.010935, train acc:99.588, train f1:99.591, train precision:99.455, train recall:99.727, train auc:99.993
fold:0 epoch:280 step:6 train loss:0.011767, train acc:99.597, train f1:99.595, train precision:99.558, train recall:99.632, train auc:99.990
fold:0 epoch:280 step:7 train loss:0.012425, train acc:99.521, train f1:99.524, train precision:99.690, train recall:99.358, train auc:99.991
fold:0 epoch:280 step:8 train loss:0.012205, train acc:99.573, train f1:99.573, train precision:99.658, train recall:99.488, train auc:99.990
fold:0 epoch:280 step:9 train loss:0.011429, train acc:99.587, train f1:99.587, train precision:99.508, train recall:99.666, train auc:99.992
fold:0 epoch:280        valid loss:0.057677, valid acc:98.684, valid f1:98.688, valid precision:98.383, valid recall:98.994, valid auc:99.837
[1;31mEarlyStopping counter: 4 out of 50[0m
[98.73057333159201, 98.73322038316174, 98.52776693978412, 98.9395324539637, 99.8336736418122]
====================================================================================================
fold:0 epoch:281 step:0 train loss:0.011707, train acc:99.585, train f1:99.584, train precision:99.542, train recall:99.627, train auc:99.991
fold:0 epoch:281 step:1 train loss:0.012048, train acc:99.561, train f1:99.556, train precision:99.458, train recall:99.654, train auc:99.990
fold:0 epoch:281 step:2 train loss:0.012626, train acc:99.509, train f1:99.512, train precision:99.636, train recall:99.389, train auc:99.991
fold:0 epoch:281 step:3 train loss:0.011944, train acc:99.570, train f1:99.572, train precision:99.672, train recall:99.472, train auc:99.992
fold:0 epoch:281 step:4 train loss:0.013019, train acc:99.542, train f1:99.543, train precision:99.403, train recall:99.683, train auc:99.987
fold:0 epoch:281 step:5 train loss:0.011521, train acc:99.588, train f1:99.589, train precision:99.514, train recall:99.665, train auc:99.990
fold:0 epoch:281 step:6 train loss:0.010879, train acc:99.582, train f1:99.580, train precision:99.436, train recall:99.724, train auc:99.993
fold:0 epoch:281 step:7 train loss:0.011677, train acc:99.551, train f1:99.554, train precision:99.624, train recall:99.485, train auc:99.991
fold:0 epoch:281 step:8 train loss:0.010312, train acc:99.603, train f1:99.602, train precision:99.706, train recall:99.499, train auc:99.993
fold:0 epoch:281 step:9 train loss:0.014937, train acc:99.446, train f1:99.439, train precision:99.430, train recall:99.448, train auc:99.983
fold:0 epoch:281        valid loss:0.057645, valid acc:98.707, valid f1:98.711, valid precision:98.429, valid recall:98.994, valid auc:99.835
[1;31mEarlyStopping counter: 5 out of 50[0m
[98.73057333159201, 98.73322038316174, 98.52776693978412, 98.9395324539637, 99.8336736418122]
====================================================================================================
fold:0 epoch:282 step:0 train loss:0.011226, train acc:99.567, train f1:99.565, train precision:99.534, train recall:99.595, train auc:99.992
fold:0 epoch:282 step:1 train loss:0.011273, train acc:99.567, train f1:99.567, train precision:99.470, train recall:99.665, train auc:99.991
fold:0 epoch:282 step:2 train loss:0.011996, train acc:99.600, train f1:99.599, train precision:99.633, train recall:99.566, train auc:99.988
fold:0 epoch:282 step:3 train loss:0.011634, train acc:99.548, train f1:99.547, train precision:99.651, train recall:99.444, train auc:99.992
fold:0 epoch:282 step:4 train loss:0.010635, train acc:99.619, train f1:99.620, train precision:99.660, train recall:99.581, train auc:99.992
fold:0 epoch:282 step:5 train loss:0.010503, train acc:99.622, train f1:99.623, train precision:99.508, train recall:99.738, train auc:99.993
fold:0 epoch:282 step:6 train loss:0.011545, train acc:99.579, train f1:99.581, train precision:99.461, train recall:99.702, train auc:99.991
fold:0 epoch:282 step:7 train loss:0.012392, train acc:99.579, train f1:99.580, train precision:99.514, train recall:99.647, train auc:99.989
fold:0 epoch:282 step:8 train loss:0.011843, train acc:99.561, train f1:99.560, train precision:99.584, train recall:99.536, train auc:99.992
fold:0 epoch:282 step:9 train loss:0.012975, train acc:99.499, train f1:99.491, train precision:99.429, train recall:99.553, train auc:99.990
fold:0 epoch:282        valid loss:0.056805, valid acc:98.742, valid f1:98.746, valid precision:98.488, valid recall:99.005, valid auc:99.834
[1;31mTest score increased (98.730573 --> 98.742327).[0m
[98.74232728222542, 98.74562009091974, 98.48776178350569, 99.00483217970485, 99.83437632382892]
====================================================================================================
fold:0 epoch:283 step:0 train loss:0.010942, train acc:99.588, train f1:99.587, train precision:99.681, train recall:99.492, train auc:99.993
fold:0 epoch:283 step:1 train loss:0.012538, train acc:99.570, train f1:99.571, train precision:99.671, train recall:99.471, train auc:99.989
fold:0 epoch:283 step:2 train loss:0.012000, train acc:99.490, train f1:99.490, train precision:99.341, train recall:99.639, train auc:99.992
fold:0 epoch:283 step:3 train loss:0.011439, train acc:99.622, train f1:99.622, train precision:99.476, train recall:99.768, train auc:99.990
fold:0 epoch:283 step:4 train loss:0.010809, train acc:99.603, train f1:99.603, train precision:99.573, train recall:99.634, train auc:99.992
fold:0 epoch:283 step:5 train loss:0.012401, train acc:99.564, train f1:99.563, train precision:99.639, train recall:99.487, train auc:99.989
fold:0 epoch:283 step:6 train loss:0.011951, train acc:99.564, train f1:99.564, train precision:99.676, train recall:99.452, train auc:99.991
fold:0 epoch:283 step:7 train loss:0.011252, train acc:99.591, train f1:99.589, train precision:99.528, train recall:99.650, train auc:99.992
fold:0 epoch:283 step:8 train loss:0.011151, train acc:99.615, train f1:99.617, train precision:99.539, train recall:99.696, train auc:99.992
fold:0 epoch:283 step:9 train loss:0.012887, train acc:99.551, train f1:99.556, train precision:99.479, train recall:99.634, train auc:99.989
fold:0 epoch:283        valid loss:0.056424, valid acc:98.740, valid f1:98.742, valid precision:98.551, valid recall:98.934, valid auc:99.839
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.74232728222542, 98.74562009091974, 98.48776178350569, 99.00483217970485, 99.83437632382892]
====================================================================================================
fold:0 epoch:284 step:0 train loss:0.011367, train acc:99.579, train f1:99.579, train precision:99.506, train recall:99.652, train auc:99.992
fold:0 epoch:284 step:1 train loss:0.009571, train acc:99.625, train f1:99.626, train precision:99.714, train recall:99.538, train auc:99.995
fold:0 epoch:284 step:2 train loss:0.011928, train acc:99.561, train f1:99.565, train precision:99.637, train recall:99.492, train auc:99.991
fold:0 epoch:284 step:3 train loss:0.010574, train acc:99.619, train f1:99.619, train precision:99.556, train recall:99.683, train auc:99.993
fold:0 epoch:284 step:4 train loss:0.011832, train acc:99.561, train f1:99.562, train precision:99.490, train recall:99.635, train auc:99.991
fold:0 epoch:284 step:5 train loss:0.012809, train acc:99.545, train f1:99.543, train precision:99.467, train recall:99.620, train auc:99.988
fold:0 epoch:284 step:6 train loss:0.010609, train acc:99.591, train f1:99.591, train precision:99.634, train recall:99.549, train auc:99.993
fold:0 epoch:284 step:7 train loss:0.011342, train acc:99.609, train f1:99.608, train precision:99.608, train recall:99.608, train auc:99.992
fold:0 epoch:284 step:8 train loss:0.012009, train acc:99.594, train f1:99.587, train precision:99.485, train recall:99.689, train auc:99.990
fold:0 epoch:284 step:9 train loss:0.015834, train acc:99.428, train f1:99.437, train precision:99.653, train recall:99.223, train auc:99.985
fold:0 epoch:284        valid loss:0.056472, valid acc:98.737, valid f1:98.741, valid precision:98.440, valid recall:99.044, valid auc:99.841
[1;31mEarlyStopping counter: 2 out of 50[0m
[98.74232728222542, 98.74562009091974, 98.48776178350569, 99.00483217970485, 99.83437632382892]
====================================================================================================
fold:0 epoch:285 step:0 train loss:0.009761, train acc:99.652, train f1:99.651, train precision:99.608, train recall:99.693, train auc:99.994
fold:0 epoch:285 step:1 train loss:0.012470, train acc:99.585, train f1:99.585, train precision:99.373, train recall:99.798, train auc:99.990
fold:0 epoch:285 step:2 train loss:0.010791, train acc:99.652, train f1:99.653, train precision:99.653, train recall:99.653, train auc:99.990
fold:0 epoch:285 step:3 train loss:0.010534, train acc:99.619, train f1:99.619, train precision:99.647, train recall:99.592, train auc:99.993
fold:0 epoch:285 step:4 train loss:0.010839, train acc:99.603, train f1:99.603, train precision:99.682, train recall:99.524, train auc:99.993
fold:0 epoch:285 step:5 train loss:0.012453, train acc:99.512, train f1:99.512, train precision:99.530, train recall:99.494, train auc:99.990
fold:0 epoch:285 step:6 train loss:0.011310, train acc:99.594, train f1:99.594, train precision:99.500, train recall:99.689, train auc:99.991
fold:0 epoch:285 step:7 train loss:0.012791, train acc:99.548, train f1:99.549, train precision:99.494, train recall:99.603, train auc:99.988
fold:0 epoch:285 step:8 train loss:0.013004, train acc:99.496, train f1:99.494, train precision:99.625, train recall:99.363, train auc:99.989
fold:0 epoch:285 step:9 train loss:0.011521, train acc:99.595, train f1:99.599, train precision:99.651, train recall:99.547, train auc:99.989
fold:0 epoch:285        valid loss:0.058652, valid acc:98.674, valid f1:98.679, valid precision:98.362, valid recall:98.997, valid auc:99.831
[1;31mEarlyStopping counter: 3 out of 50[0m
[98.74232728222542, 98.74562009091974, 98.48776178350569, 99.00483217970485, 99.83437632382892]
====================================================================================================
fold:0 epoch:286 step:0 train loss:0.013266, train acc:99.557, train f1:99.558, train precision:99.398, train recall:99.719, train auc:99.986
fold:0 epoch:286 step:1 train loss:0.011829, train acc:99.573, train f1:99.572, train precision:99.481, train recall:99.663, train auc:99.991
fold:0 epoch:286 step:2 train loss:0.011427, train acc:99.570, train f1:99.569, train precision:99.615, train recall:99.523, train auc:99.991
fold:0 epoch:286 step:3 train loss:0.013019, train acc:99.588, train f1:99.589, train precision:99.689, train recall:99.489, train auc:99.989
fold:0 epoch:286 step:4 train loss:0.010081, train acc:99.612, train f1:99.612, train precision:99.676, train recall:99.549, train auc:99.994
fold:0 epoch:286 step:5 train loss:0.011486, train acc:99.582, train f1:99.583, train precision:99.453, train recall:99.714, train auc:99.992
fold:0 epoch:286 step:6 train loss:0.011575, train acc:99.585, train f1:99.585, train precision:99.439, train recall:99.731, train auc:99.991
fold:0 epoch:286 step:7 train loss:0.011174, train acc:99.548, train f1:99.548, train precision:99.584, train recall:99.511, train auc:99.992
fold:0 epoch:286 step:8 train loss:0.012428, train acc:99.518, train f1:99.515, train precision:99.515, train recall:99.515, train auc:99.987
fold:0 epoch:286 step:9 train loss:0.012731, train acc:99.569, train f1:99.575, train precision:99.739, train recall:99.411, train auc:99.990
fold:0 epoch:286        valid loss:0.058034, valid acc:98.740, valid f1:98.743, valid precision:98.455, valid recall:99.034, valid auc:99.833
[1;31mEarlyStopping counter: 4 out of 50[0m
[98.74232728222542, 98.74562009091974, 98.48776178350569, 99.00483217970485, 99.83437632382892]
====================================================================================================
fold:0 epoch:287 step:0 train loss:0.011114, train acc:99.646, train f1:99.647, train precision:99.672, train recall:99.623, train auc:99.990
fold:0 epoch:287 step:1 train loss:0.011677, train acc:99.594, train f1:99.595, train precision:99.441, train recall:99.750, train auc:99.989
fold:0 epoch:287 step:2 train loss:0.012026, train acc:99.564, train f1:99.562, train precision:99.394, train recall:99.730, train auc:99.991
fold:0 epoch:287 step:3 train loss:0.009920, train acc:99.646, train f1:99.645, train precision:99.645, train recall:99.645, train auc:99.993
fold:0 epoch:287 step:4 train loss:0.014245, train acc:99.524, train f1:99.525, train precision:99.646, train recall:99.404, train auc:99.987
fold:0 epoch:287 step:5 train loss:0.009844, train acc:99.673, train f1:99.670, train precision:99.734, train recall:99.605, train auc:99.994
fold:0 epoch:287 step:6 train loss:0.012055, train acc:99.557, train f1:99.560, train precision:99.594, train recall:99.527, train auc:99.990
fold:0 epoch:287 step:7 train loss:0.012469, train acc:99.554, train f1:99.557, train precision:99.407, train recall:99.709, train auc:99.991
fold:0 epoch:287 step:8 train loss:0.012145, train acc:99.554, train f1:99.556, train precision:99.308, train recall:99.805, train auc:99.992
fold:0 epoch:287 step:9 train loss:0.012729, train acc:99.446, train f1:99.441, train precision:99.415, train recall:99.468, train auc:99.990
fold:0 epoch:287        valid loss:0.057432, valid acc:98.706, valid f1:98.711, valid precision:98.281, valid recall:99.146, valid auc:99.841
[1;31mEarlyStopping counter: 5 out of 50[0m
[98.74232728222542, 98.74562009091974, 98.48776178350569, 99.00483217970485, 99.83437632382892]
====================================================================================================
fold:0 epoch:288 step:0 train loss:0.011942, train acc:99.564, train f1:99.567, train precision:99.745, train recall:99.389, train auc:99.993
fold:0 epoch:288 step:1 train loss:0.013178, train acc:99.542, train f1:99.540, train precision:99.693, train recall:99.388, train auc:99.990
fold:0 epoch:288 step:2 train loss:0.011260, train acc:99.606, train f1:99.610, train precision:99.553, train recall:99.667, train auc:99.991
fold:0 epoch:288 step:3 train loss:0.012081, train acc:99.579, train f1:99.579, train precision:99.372, train recall:99.786, train auc:99.991
fold:0 epoch:288 step:4 train loss:0.012625, train acc:99.551, train f1:99.546, train precision:99.298, train recall:99.796, train auc:99.991
fold:0 epoch:288 step:5 train loss:0.012033, train acc:99.594, train f1:99.593, train precision:99.724, train recall:99.462, train auc:99.990
fold:0 epoch:288 step:6 train loss:0.014412, train acc:99.527, train f1:99.529, train precision:99.744, train recall:99.314, train auc:99.989
fold:0 epoch:288 step:7 train loss:0.010417, train acc:99.612, train f1:99.616, train precision:99.686, train recall:99.547, train auc:99.993
fold:0 epoch:288 step:8 train loss:0.011672, train acc:99.564, train f1:99.560, train precision:99.349, train recall:99.772, train auc:99.993
fold:0 epoch:288 step:9 train loss:0.011241, train acc:99.587, train f1:99.586, train precision:99.263, train recall:99.912, train auc:99.992
fold:0 epoch:288        valid loss:0.056423, valid acc:98.745, valid f1:98.749, valid precision:98.410, valid recall:99.091, valid auc:99.840
[1;31mTest score increased (98.742327 --> 98.744939).[0m
[98.74493927125506, 98.74926791175896, 98.40985732814526, 99.09102781768317, 99.8398601021814]
====================================================================================================
fold:0 epoch:289 step:0 train loss:0.011063, train acc:99.646, train f1:99.649, train precision:99.662, train recall:99.637, train auc:99.991
fold:0 epoch:289 step:1 train loss:0.010576, train acc:99.612, train f1:99.610, train precision:99.681, train recall:99.540, train auc:99.992
fold:0 epoch:289 step:2 train loss:0.013010, train acc:99.527, train f1:99.528, train precision:99.683, train recall:99.374, train auc:99.990
fold:0 epoch:289 step:3 train loss:0.011964, train acc:99.628, train f1:99.629, train precision:99.684, train recall:99.574, train auc:99.989
fold:0 epoch:289 step:4 train loss:0.012126, train acc:99.548, train f1:99.545, train precision:99.386, train recall:99.704, train auc:99.991
fold:0 epoch:289 step:5 train loss:0.011016, train acc:99.579, train f1:99.577, train precision:99.486, train recall:99.669, train auc:99.993
fold:0 epoch:289 step:6 train loss:0.011317, train acc:99.576, train f1:99.575, train precision:99.560, train recall:99.591, train auc:99.991
fold:0 epoch:289 step:7 train loss:0.011519, train acc:99.579, train f1:99.578, train precision:99.627, train recall:99.529, train auc:99.990
fold:0 epoch:289 step:8 train loss:0.014418, train acc:99.460, train f1:99.461, train precision:99.561, train recall:99.361, train auc:99.986
fold:0 epoch:289 step:9 train loss:0.011430, train acc:99.595, train f1:99.597, train precision:99.475, train recall:99.720, train auc:99.987
fold:0 epoch:289        valid loss:0.055721, valid acc:98.762, valid f1:98.765, valid precision:98.554, valid recall:98.976, valid auc:99.838
[1;31mTest score increased (98.744939 --> 98.761917).[0m
[98.76191719994776, 98.76456329658299, 98.55392858071731, 98.97610030037873, 99.83813206863988]
====================================================================================================
fold:0 epoch:290 step:0 train loss:0.011571, train acc:99.609, train f1:99.611, train precision:99.611, train recall:99.611, train auc:99.991
fold:0 epoch:290 step:1 train loss:0.012099, train acc:99.570, train f1:99.570, train precision:99.494, train recall:99.646, train auc:99.990
fold:0 epoch:290 step:2 train loss:0.011437, train acc:99.606, train f1:99.606, train precision:99.523, train recall:99.688, train auc:99.991
fold:0 epoch:290 step:3 train loss:0.011008, train acc:99.622, train f1:99.620, train precision:99.577, train recall:99.662, train auc:99.992
fold:0 epoch:290 step:4 train loss:0.012636, train acc:99.551, train f1:99.553, train precision:99.732, train recall:99.374, train auc:99.992
fold:0 epoch:290 step:5 train loss:0.010405, train acc:99.628, train f1:99.627, train precision:99.627, train recall:99.627, train auc:99.993
fold:0 epoch:290 step:6 train loss:0.011369, train acc:99.570, train f1:99.569, train precision:99.512, train recall:99.627, train auc:99.992
fold:0 epoch:290 step:7 train loss:0.011179, train acc:99.579, train f1:99.581, train precision:99.502, train recall:99.660, train auc:99.992
fold:0 epoch:290 step:8 train loss:0.010826, train acc:99.600, train f1:99.601, train precision:99.543, train recall:99.658, train auc:99.992
fold:0 epoch:290 step:9 train loss:0.011421, train acc:99.657, train f1:99.655, train precision:99.541, train recall:99.770, train auc:99.988
fold:0 epoch:290        valid loss:0.056993, valid acc:98.712, valid f1:98.717, valid precision:98.324, valid recall:99.115, valid auc:99.836
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.76191719994776, 98.76456329658299, 98.55392858071731, 98.97610030037873, 99.83813206863988]
====================================================================================================
fold:0 epoch:291 step:0 train loss:0.010431, train acc:99.649, train f1:99.649, train precision:99.707, train recall:99.591, train auc:99.993
fold:0 epoch:291 step:1 train loss:0.010293, train acc:99.628, train f1:99.630, train precision:99.715, train recall:99.546, train auc:99.994
fold:0 epoch:291 step:2 train loss:0.010694, train acc:99.603, train f1:99.605, train precision:99.472, train recall:99.738, train auc:99.993
fold:0 epoch:291 step:3 train loss:0.012409, train acc:99.527, train f1:99.529, train precision:99.338, train recall:99.720, train auc:99.991
fold:0 epoch:291 step:4 train loss:0.009859, train acc:99.652, train f1:99.652, train precision:99.537, train recall:99.768, train auc:99.994
fold:0 epoch:291 step:5 train loss:0.011278, train acc:99.567, train f1:99.567, train precision:99.586, train recall:99.549, train auc:99.992
fold:0 epoch:291 step:6 train loss:0.011536, train acc:99.564, train f1:99.561, train precision:99.692, train recall:99.429, train auc:99.992
fold:0 epoch:291 step:7 train loss:0.011865, train acc:99.597, train f1:99.596, train precision:99.688, train recall:99.505, train auc:99.990
fold:0 epoch:291 step:8 train loss:0.011915, train acc:99.576, train f1:99.574, train precision:99.473, train recall:99.675, train auc:99.990
fold:0 epoch:291 step:9 train loss:0.012000, train acc:99.560, train f1:99.559, train precision:99.453, train recall:99.664, train auc:99.991
fold:0 epoch:291        valid loss:0.056160, valid acc:98.798, valid f1:98.801, valid precision:98.600, valid recall:99.002, valid auc:99.837
[1;31mTest score increased (98.761917 --> 98.798485).[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:292 step:0 train loss:0.009629, train acc:99.649, train f1:99.649, train precision:99.549, train recall:99.750, train auc:99.993
fold:0 epoch:292 step:1 train loss:0.010403, train acc:99.640, train f1:99.641, train precision:99.756, train recall:99.526, train auc:99.989
fold:0 epoch:292 step:2 train loss:0.011793, train acc:99.561, train f1:99.558, train precision:99.583, train recall:99.534, train auc:99.990
fold:0 epoch:292 step:3 train loss:0.012313, train acc:99.594, train f1:99.594, train precision:99.676, train recall:99.512, train auc:99.989
fold:0 epoch:292 step:4 train loss:0.012215, train acc:99.579, train f1:99.580, train precision:99.586, train recall:99.574, train auc:99.990
fold:0 epoch:292 step:5 train loss:0.012214, train acc:99.527, train f1:99.528, train precision:99.331, train recall:99.725, train auc:99.991
fold:0 epoch:292 step:6 train loss:0.010392, train acc:99.612, train f1:99.612, train precision:99.554, train recall:99.670, train auc:99.993
fold:0 epoch:292 step:7 train loss:0.010363, train acc:99.609, train f1:99.611, train precision:99.629, train recall:99.593, train auc:99.993
fold:0 epoch:292 step:8 train loss:0.012334, train acc:99.576, train f1:99.576, train precision:99.628, train recall:99.524, train auc:99.991
fold:0 epoch:292 step:9 train loss:0.011785, train acc:99.551, train f1:99.546, train precision:99.431, train recall:99.662, train auc:99.990
fold:0 epoch:292        valid loss:0.059544, valid acc:98.708, valid f1:98.712, valid precision:98.411, valid recall:99.015, valid auc:99.829
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:293 step:0 train loss:0.011903, train acc:99.585, train f1:99.585, train precision:99.737, train recall:99.433, train auc:99.991
fold:0 epoch:293 step:1 train loss:0.011716, train acc:99.606, train f1:99.605, train precision:99.377, train recall:99.834, train auc:99.993
fold:0 epoch:293 step:2 train loss:0.010498, train acc:99.606, train f1:99.608, train precision:99.478, train recall:99.738, train auc:99.993
fold:0 epoch:293 step:3 train loss:0.010810, train acc:99.597, train f1:99.597, train precision:99.621, train recall:99.572, train auc:99.992
fold:0 epoch:293 step:4 train loss:0.011107, train acc:99.612, train f1:99.613, train precision:99.610, train recall:99.616, train auc:99.992
fold:0 epoch:293 step:5 train loss:0.011029, train acc:99.655, train f1:99.655, train precision:99.707, train recall:99.603, train auc:99.991
fold:0 epoch:293 step:6 train loss:0.012183, train acc:99.557, train f1:99.558, train precision:99.664, train recall:99.451, train auc:99.990
fold:0 epoch:293 step:7 train loss:0.012059, train acc:99.536, train f1:99.532, train precision:99.538, train recall:99.526, train auc:99.991
fold:0 epoch:293 step:8 train loss:0.011641, train acc:99.573, train f1:99.577, train precision:99.463, train recall:99.692, train auc:99.991
fold:0 epoch:293 step:9 train loss:0.014528, train acc:99.428, train f1:99.429, train precision:99.211, train recall:99.648, train auc:99.987
fold:0 epoch:293        valid loss:0.058070, valid acc:98.729, valid f1:98.733, valid precision:98.419, valid recall:99.049, valid auc:99.835
[1;31mEarlyStopping counter: 2 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:294 step:0 train loss:0.010588, train acc:99.600, train f1:99.600, train precision:99.621, train recall:99.578, train auc:99.992
fold:0 epoch:294 step:1 train loss:0.010560, train acc:99.597, train f1:99.597, train precision:99.627, train recall:99.566, train auc:99.993
fold:0 epoch:294 step:2 train loss:0.010407, train acc:99.628, train f1:99.625, train precision:99.521, train recall:99.729, train auc:99.993
fold:0 epoch:294 step:3 train loss:0.012404, train acc:99.512, train f1:99.517, train precision:99.674, train recall:99.361, train auc:99.991
fold:0 epoch:294 step:4 train loss:0.011679, train acc:99.585, train f1:99.584, train precision:99.480, train recall:99.688, train auc:99.989
fold:0 epoch:294 step:5 train loss:0.011109, train acc:99.600, train f1:99.600, train precision:99.500, train recall:99.701, train auc:99.991
fold:0 epoch:294 step:6 train loss:0.011785, train acc:99.588, train f1:99.587, train precision:99.462, train recall:99.712, train auc:99.989
fold:0 epoch:294 step:7 train loss:0.011795, train acc:99.530, train f1:99.531, train precision:99.591, train recall:99.470, train auc:99.991
fold:0 epoch:294 step:8 train loss:0.011908, train acc:99.588, train f1:99.589, train precision:99.713, train recall:99.464, train auc:99.990
fold:0 epoch:294 step:9 train loss:0.012003, train acc:99.648, train f1:99.649, train precision:99.579, train recall:99.719, train auc:99.989
fold:0 epoch:294        valid loss:0.058052, valid acc:98.721, valid f1:98.725, valid precision:98.417, valid recall:99.036, valid auc:99.834
[1;31mEarlyStopping counter: 3 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:295 step:0 train loss:0.011794, train acc:99.536, train f1:99.536, train precision:99.536, train recall:99.536, train auc:99.991
fold:0 epoch:295 step:1 train loss:0.010832, train acc:99.570, train f1:99.570, train precision:99.458, train recall:99.683, train auc:99.993
fold:0 epoch:295 step:2 train loss:0.010626, train acc:99.582, train f1:99.582, train precision:99.403, train recall:99.761, train auc:99.993
fold:0 epoch:295 step:3 train loss:0.010970, train acc:99.625, train f1:99.625, train precision:99.646, train recall:99.604, train auc:99.992
fold:0 epoch:295 step:4 train loss:0.011185, train acc:99.646, train f1:99.646, train precision:99.628, train recall:99.664, train auc:99.991
fold:0 epoch:295 step:5 train loss:0.011766, train acc:99.576, train f1:99.578, train precision:99.550, train recall:99.605, train auc:99.988
fold:0 epoch:295 step:6 train loss:0.010592, train acc:99.637, train f1:99.635, train precision:99.638, train recall:99.632, train auc:99.992
fold:0 epoch:295 step:7 train loss:0.011444, train acc:99.579, train f1:99.581, train precision:99.551, train recall:99.611, train auc:99.992
fold:0 epoch:295 step:8 train loss:0.012071, train acc:99.582, train f1:99.581, train precision:99.487, train recall:99.675, train auc:99.990
fold:0 epoch:295 step:9 train loss:0.011165, train acc:99.604, train f1:99.603, train precision:99.700, train recall:99.507, train auc:99.993
fold:0 epoch:295        valid loss:0.057540, valid acc:98.714, valid f1:98.718, valid precision:98.404, valid recall:99.034, valid auc:99.835
[1;31mEarlyStopping counter: 4 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:296 step:0 train loss:0.010352, train acc:99.625, train f1:99.624, train precision:99.627, train recall:99.621, train auc:99.993
fold:0 epoch:296 step:1 train loss:0.010618, train acc:99.622, train f1:99.621, train precision:99.621, train recall:99.621, train auc:99.992
fold:0 epoch:296 step:2 train loss:0.013099, train acc:99.530, train f1:99.529, train precision:99.523, train recall:99.535, train auc:99.989
fold:0 epoch:296 step:3 train loss:0.010885, train acc:99.603, train f1:99.602, train precision:99.536, train recall:99.670, train auc:99.993
fold:0 epoch:296 step:4 train loss:0.010220, train acc:99.612, train f1:99.611, train precision:99.541, train recall:99.681, train auc:99.993
fold:0 epoch:296 step:5 train loss:0.010708, train acc:99.615, train f1:99.619, train precision:99.577, train recall:99.661, train auc:99.992
fold:0 epoch:296 step:6 train loss:0.009470, train acc:99.631, train f1:99.632, train precision:99.696, train recall:99.568, train auc:99.994
fold:0 epoch:296 step:7 train loss:0.013122, train acc:99.524, train f1:99.524, train precision:99.573, train recall:99.476, train auc:99.989
fold:0 epoch:296 step:8 train loss:0.012335, train acc:99.536, train f1:99.535, train precision:99.389, train recall:99.681, train auc:99.990
fold:0 epoch:296 step:9 train loss:0.012243, train acc:99.569, train f1:99.571, train precision:99.458, train recall:99.684, train auc:99.989
fold:0 epoch:296        valid loss:0.058716, valid acc:98.716, valid f1:98.721, valid precision:98.329, valid recall:99.117, valid auc:99.838
[1;31mEarlyStopping counter: 5 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:297 step:0 train loss:0.011707, train acc:99.570, train f1:99.568, train precision:99.724, train recall:99.413, train auc:99.993
fold:0 epoch:297 step:1 train loss:0.011437, train acc:99.619, train f1:99.621, train precision:99.715, train recall:99.527, train auc:99.990
fold:0 epoch:297 step:2 train loss:0.010832, train acc:99.609, train f1:99.612, train precision:99.503, train recall:99.721, train auc:99.993
fold:0 epoch:297 step:3 train loss:0.012597, train acc:99.591, train f1:99.589, train precision:99.388, train recall:99.791, train auc:99.990
fold:0 epoch:297 step:4 train loss:0.011511, train acc:99.594, train f1:99.595, train precision:99.586, train recall:99.605, train auc:99.991
fold:0 epoch:297 step:5 train loss:0.010625, train acc:99.594, train f1:99.591, train precision:99.557, train recall:99.624, train auc:99.992
fold:0 epoch:297 step:6 train loss:0.010520, train acc:99.609, train f1:99.609, train precision:99.712, train recall:99.505, train auc:99.993
fold:0 epoch:297 step:7 train loss:0.011263, train acc:99.622, train f1:99.619, train precision:99.619, train recall:99.619, train auc:99.991
fold:0 epoch:297 step:8 train loss:0.012600, train acc:99.542, train f1:99.547, train precision:99.522, train recall:99.571, train auc:99.990
fold:0 epoch:297 step:9 train loss:0.011650, train acc:99.631, train f1:99.632, train precision:99.579, train recall:99.684, train auc:99.991
fold:0 epoch:297        valid loss:0.057411, valid acc:98.740, valid f1:98.743, valid precision:98.475, valid recall:99.013, valid auc:99.834
[1;31mEarlyStopping counter: 6 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:298 step:0 train loss:0.011643, train acc:99.588, train f1:99.591, train precision:99.467, train recall:99.715, train auc:99.991
fold:0 epoch:298 step:1 train loss:0.012298, train acc:99.567, train f1:99.568, train precision:99.544, train recall:99.592, train auc:99.988
fold:0 epoch:298 step:2 train loss:0.012696, train acc:99.564, train f1:99.564, train precision:99.670, train recall:99.458, train auc:99.989
fold:0 epoch:298 step:3 train loss:0.010358, train acc:99.649, train f1:99.646, train precision:99.674, train recall:99.618, train auc:99.993
fold:0 epoch:298 step:4 train loss:0.009870, train acc:99.622, train f1:99.619, train precision:99.552, train recall:99.686, train auc:99.994
fold:0 epoch:298 step:5 train loss:0.010101, train acc:99.661, train f1:99.662, train precision:99.562, train recall:99.762, train auc:99.993
fold:0 epoch:298 step:6 train loss:0.012086, train acc:99.551, train f1:99.555, train precision:99.491, train recall:99.618, train auc:99.990
fold:0 epoch:298 step:7 train loss:0.011298, train acc:99.640, train f1:99.639, train precision:99.590, train recall:99.687, train auc:99.990
fold:0 epoch:298 step:8 train loss:0.009977, train acc:99.649, train f1:99.650, train precision:99.714, train recall:99.586, train auc:99.993
fold:0 epoch:298 step:9 train loss:0.010416, train acc:99.692, train f1:99.692, train precision:99.771, train recall:99.613, train auc:99.992
fold:0 epoch:298        valid loss:0.058722, valid acc:98.712, valid f1:98.717, valid precision:98.374, valid recall:99.062, valid auc:99.839
[1;31mEarlyStopping counter: 7 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:299 step:0 train loss:0.010471, train acc:99.643, train f1:99.644, train precision:99.647, train recall:99.641, train auc:99.993
fold:0 epoch:299 step:1 train loss:0.008656, train acc:99.689, train f1:99.687, train precision:99.572, train recall:99.804, train auc:99.996
fold:0 epoch:299 step:2 train loss:0.011275, train acc:99.585, train f1:99.584, train precision:99.475, train recall:99.694, train auc:99.992
fold:0 epoch:299 step:3 train loss:0.010441, train acc:99.640, train f1:99.642, train precision:99.660, train recall:99.623, train auc:99.993
fold:0 epoch:299 step:4 train loss:0.011530, train acc:99.597, train f1:99.597, train precision:99.719, train recall:99.475, train auc:99.992
fold:0 epoch:299 step:5 train loss:0.010065, train acc:99.640, train f1:99.640, train precision:99.646, train recall:99.634, train auc:99.993
fold:0 epoch:299 step:6 train loss:0.010853, train acc:99.600, train f1:99.601, train precision:99.604, train recall:99.597, train auc:99.992
fold:0 epoch:299 step:7 train loss:0.012395, train acc:99.515, train f1:99.514, train precision:99.438, train recall:99.590, train auc:99.990
fold:0 epoch:299 step:8 train loss:0.009754, train acc:99.622, train f1:99.623, train precision:99.587, train recall:99.660, train auc:99.994
fold:0 epoch:299 step:9 train loss:0.012685, train acc:99.534, train f1:99.530, train precision:99.433, train recall:99.627, train auc:99.983
fold:0 epoch:299        valid loss:0.057744, valid acc:98.670, valid f1:98.675, valid precision:98.342, valid recall:99.010, valid auc:99.835
[1;31mEarlyStopping counter: 8 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:300 step:0 train loss:0.009862, train acc:99.655, train f1:99.653, train precision:99.736, train recall:99.571, train auc:99.995
fold:0 epoch:300 step:1 train loss:0.009837, train acc:99.686, train f1:99.686, train precision:99.738, train recall:99.635, train auc:99.994
fold:0 epoch:300 step:2 train loss:0.010134, train acc:99.612, train f1:99.617, train precision:99.596, train recall:99.638, train auc:99.994
fold:0 epoch:300 step:3 train loss:0.010373, train acc:99.655, train f1:99.658, train precision:99.553, train recall:99.764, train auc:99.993
fold:0 epoch:300 step:4 train loss:0.011723, train acc:99.582, train f1:99.578, train precision:99.447, train recall:99.710, train auc:99.990
fold:0 epoch:300 step:5 train loss:0.011496, train acc:99.606, train f1:99.606, train precision:99.615, train recall:99.596, train auc:99.991
fold:0 epoch:300 step:6 train loss:0.011775, train acc:99.557, train f1:99.556, train precision:99.693, train recall:99.418, train auc:99.992
fold:0 epoch:300 step:7 train loss:0.009100, train acc:99.658, train f1:99.658, train precision:99.670, train recall:99.645, train auc:99.995
fold:0 epoch:300 step:8 train loss:0.011404, train acc:99.551, train f1:99.550, train precision:99.486, train recall:99.614, train auc:99.991
fold:0 epoch:300 step:9 train loss:0.012146, train acc:99.560, train f1:99.563, train precision:99.424, train recall:99.703, train auc:99.989
fold:0 epoch:300        valid loss:0.056530, valid acc:98.746, valid f1:98.750, valid precision:98.453, valid recall:99.049, valid auc:99.845
[1;31mEarlyStopping counter: 9 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:301 step:0 train loss:0.010645, train acc:99.615, train f1:99.615, train precision:99.572, train recall:99.657, train auc:99.992
fold:0 epoch:301 step:1 train loss:0.011443, train acc:99.603, train f1:99.605, train precision:99.642, train recall:99.569, train auc:99.991
fold:0 epoch:301 step:2 train loss:0.011382, train acc:99.606, train f1:99.604, train precision:99.650, train recall:99.558, train auc:99.990
fold:0 epoch:301 step:3 train loss:0.009962, train acc:99.634, train f1:99.635, train precision:99.629, train recall:99.641, train auc:99.994
fold:0 epoch:301 step:4 train loss:0.011956, train acc:99.591, train f1:99.590, train precision:99.542, train recall:99.639, train auc:99.990
fold:0 epoch:301 step:5 train loss:0.011035, train acc:99.591, train f1:99.591, train precision:99.421, train recall:99.762, train auc:99.993
fold:0 epoch:301 step:6 train loss:0.010859, train acc:99.597, train f1:99.597, train precision:99.512, train recall:99.682, train auc:99.992
fold:0 epoch:301 step:7 train loss:0.011155, train acc:99.567, train f1:99.568, train precision:99.689, train recall:99.447, train auc:99.993
fold:0 epoch:301 step:8 train loss:0.009616, train acc:99.658, train f1:99.658, train precision:99.670, train recall:99.646, train auc:99.994
fold:0 epoch:301 step:9 train loss:0.011751, train acc:99.622, train f1:99.621, train precision:99.577, train recall:99.665, train auc:99.990
fold:0 epoch:301        valid loss:0.057030, valid acc:98.754, valid f1:98.757, valid precision:98.561, valid recall:98.953, valid auc:99.838
[1;31mEarlyStopping counter: 10 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:302 step:0 train loss:0.011027, train acc:99.609, train f1:99.610, train precision:99.494, train recall:99.725, train auc:99.991
fold:0 epoch:302 step:1 train loss:0.010176, train acc:99.615, train f1:99.614, train precision:99.577, train recall:99.650, train auc:99.993
fold:0 epoch:302 step:2 train loss:0.011545, train acc:99.585, train f1:99.586, train precision:99.713, train recall:99.458, train auc:99.991
fold:0 epoch:302 step:3 train loss:0.011223, train acc:99.628, train f1:99.629, train precision:99.708, train recall:99.550, train auc:99.991
fold:0 epoch:302 step:4 train loss:0.011226, train acc:99.554, train f1:99.555, train precision:99.397, train recall:99.713, train auc:99.993
fold:0 epoch:302 step:5 train loss:0.011428, train acc:99.594, train f1:99.593, train precision:99.413, train recall:99.773, train auc:99.992
fold:0 epoch:302 step:6 train loss:0.011260, train acc:99.594, train f1:99.593, train precision:99.481, train recall:99.706, train auc:99.990
fold:0 epoch:302 step:7 train loss:0.013110, train acc:99.512, train f1:99.510, train precision:99.711, train recall:99.309, train auc:99.990
fold:0 epoch:302 step:8 train loss:0.013153, train acc:99.521, train f1:99.521, train precision:99.658, train recall:99.384, train auc:99.988
fold:0 epoch:302 step:9 train loss:0.010828, train acc:99.560, train f1:99.571, train precision:99.452, train recall:99.691, train auc:99.993
fold:0 epoch:302        valid loss:0.056315, valid acc:98.766, valid f1:98.767, valid precision:98.668, valid recall:98.866, valid auc:99.840
[1;31mEarlyStopping counter: 11 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:303 step:0 train loss:0.012333, train acc:99.573, train f1:99.577, train precision:99.306, train recall:99.848, train auc:99.992
fold:0 epoch:303 step:1 train loss:0.010761, train acc:99.612, train f1:99.614, train precision:99.448, train recall:99.781, train auc:99.993
fold:0 epoch:303 step:2 train loss:0.011739, train acc:99.551, train f1:99.554, train precision:99.623, train recall:99.484, train auc:99.991
fold:0 epoch:303 step:3 train loss:0.012881, train acc:99.545, train f1:99.544, train precision:99.755, train recall:99.335, train auc:99.991
fold:0 epoch:303 step:4 train loss:0.011530, train acc:99.600, train f1:99.599, train precision:99.706, train recall:99.492, train auc:99.992
fold:0 epoch:303 step:5 train loss:0.010646, train acc:99.603, train f1:99.602, train precision:99.553, train recall:99.651, train auc:99.992
fold:0 epoch:303 step:6 train loss:0.010931, train acc:99.615, train f1:99.617, train precision:99.503, train recall:99.732, train auc:99.993
fold:0 epoch:303 step:7 train loss:0.012515, train acc:99.539, train f1:99.534, train precision:99.316, train recall:99.753, train auc:99.990
fold:0 epoch:303 step:8 train loss:0.010154, train acc:99.701, train f1:99.701, train precision:99.677, train recall:99.726, train auc:99.989
fold:0 epoch:303 step:9 train loss:0.008223, train acc:99.727, train f1:99.726, train precision:99.699, train recall:99.752, train auc:99.995
fold:0 epoch:303        valid loss:0.058733, valid acc:98.668, valid f1:98.673, valid precision:98.327, valid recall:99.021, valid auc:99.839
[1;31mEarlyStopping counter: 12 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:304 step:0 train loss:0.012052, train acc:99.564, train f1:99.561, train precision:99.600, train recall:99.521, train auc:99.991
fold:0 epoch:304 step:1 train loss:0.010056, train acc:99.619, train f1:99.616, train precision:99.723, train recall:99.509, train auc:99.994
fold:0 epoch:304 step:2 train loss:0.010976, train acc:99.612, train f1:99.613, train precision:99.671, train recall:99.556, train auc:99.992
fold:0 epoch:304 step:3 train loss:0.012098, train acc:99.567, train f1:99.569, train precision:99.460, train recall:99.678, train auc:99.988
fold:0 epoch:304 step:4 train loss:0.010646, train acc:99.609, train f1:99.608, train precision:99.523, train recall:99.694, train auc:99.993
fold:0 epoch:304 step:5 train loss:0.010763, train acc:99.643, train f1:99.643, train precision:99.574, train recall:99.713, train auc:99.992
fold:0 epoch:304 step:6 train loss:0.012262, train acc:99.557, train f1:99.557, train precision:99.615, train recall:99.499, train auc:99.989
fold:0 epoch:304 step:7 train loss:0.011325, train acc:99.625, train f1:99.624, train precision:99.640, train recall:99.609, train auc:99.989
fold:0 epoch:304 step:8 train loss:0.012367, train acc:99.579, train f1:99.581, train precision:99.636, train recall:99.527, train auc:99.989
fold:0 epoch:304 step:9 train loss:0.011761, train acc:99.595, train f1:99.597, train precision:99.597, train recall:99.597, train auc:99.991
fold:0 epoch:304        valid loss:0.057130, valid acc:98.732, valid f1:98.735, valid precision:98.513, valid recall:98.958, valid auc:99.835
[1;31mEarlyStopping counter: 13 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:305 step:0 train loss:0.012500, train acc:99.570, train f1:99.569, train precision:99.366, train recall:99.773, train auc:99.990
fold:0 epoch:305 step:1 train loss:0.010101, train acc:99.646, train f1:99.646, train precision:99.555, train recall:99.738, train auc:99.992
fold:0 epoch:305 step:2 train loss:0.010355, train acc:99.658, train f1:99.658, train precision:99.658, train recall:99.658, train auc:99.992
fold:0 epoch:305 step:3 train loss:0.011553, train acc:99.585, train f1:99.584, train precision:99.792, train recall:99.377, train auc:99.992
fold:0 epoch:305 step:4 train loss:0.011801, train acc:99.606, train f1:99.607, train precision:99.786, train recall:99.429, train auc:99.991
fold:0 epoch:305 step:5 train loss:0.010557, train acc:99.628, train f1:99.625, train precision:99.534, train recall:99.717, train auc:99.992
fold:0 epoch:305 step:6 train loss:0.012041, train acc:99.554, train f1:99.558, train precision:99.425, train recall:99.690, train auc:99.990
fold:0 epoch:305 step:7 train loss:0.012691, train acc:99.557, train f1:99.556, train precision:99.328, train recall:99.785, train auc:99.991
fold:0 epoch:305 step:8 train loss:0.010846, train acc:99.619, train f1:99.621, train precision:99.581, train recall:99.660, train auc:99.991
fold:0 epoch:305 step:9 train loss:0.010615, train acc:99.631, train f1:99.633, train precision:99.668, train recall:99.598, train auc:99.992
fold:0 epoch:305        valid loss:0.057675, valid acc:98.703, valid f1:98.708, valid precision:98.363, valid recall:99.054, valid auc:99.836
[1;31mEarlyStopping counter: 14 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:306 step:0 train loss:0.011287, train acc:99.591, train f1:99.593, train precision:99.769, train recall:99.418, train auc:99.993
fold:0 epoch:306 step:1 train loss:0.009806, train acc:99.661, train f1:99.658, train precision:99.636, train recall:99.679, train auc:99.994
fold:0 epoch:306 step:2 train loss:0.009779, train acc:99.677, train f1:99.675, train precision:99.601, train recall:99.748, train auc:99.992
fold:0 epoch:306 step:3 train loss:0.009450, train acc:99.658, train f1:99.658, train precision:99.555, train recall:99.762, train auc:99.994
fold:0 epoch:306 step:4 train loss:0.010140, train acc:99.637, train f1:99.641, train precision:99.577, train recall:99.704, train auc:99.993
fold:0 epoch:306 step:5 train loss:0.010822, train acc:99.628, train f1:99.626, train precision:99.681, train recall:99.571, train auc:99.992
fold:0 epoch:306 step:6 train loss:0.011192, train acc:99.606, train f1:99.604, train precision:99.644, train recall:99.564, train auc:99.991
fold:0 epoch:306 step:7 train loss:0.012578, train acc:99.554, train f1:99.558, train precision:99.636, train recall:99.480, train auc:99.989
fold:0 epoch:306 step:8 train loss:0.010888, train acc:99.628, train f1:99.629, train precision:99.520, train recall:99.738, train auc:99.990
fold:0 epoch:306 step:9 train loss:0.010434, train acc:99.657, train f1:99.659, train precision:99.512, train recall:99.808, train auc:99.993
fold:0 epoch:306        valid loss:0.058642, valid acc:98.710, valid f1:98.715, valid precision:98.333, valid recall:99.099, valid auc:99.836
[1;31mEarlyStopping counter: 15 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:307 step:0 train loss:0.010663, train acc:99.628, train f1:99.631, train precision:99.619, train recall:99.643, train auc:99.992
fold:0 epoch:307 step:1 train loss:0.010708, train acc:99.591, train f1:99.592, train precision:99.623, train recall:99.562, train auc:99.993
fold:0 epoch:307 step:2 train loss:0.012018, train acc:99.594, train f1:99.594, train precision:99.664, train recall:99.524, train auc:99.991
fold:0 epoch:307 step:3 train loss:0.010765, train acc:99.631, train f1:99.630, train precision:99.590, train recall:99.670, train auc:99.988
fold:0 epoch:307 step:4 train loss:0.009977, train acc:99.622, train f1:99.620, train precision:99.498, train recall:99.742, train auc:99.994
fold:0 epoch:307 step:5 train loss:0.010475, train acc:99.615, train f1:99.617, train precision:99.641, train recall:99.592, train auc:99.993
fold:0 epoch:307 step:6 train loss:0.010770, train acc:99.603, train f1:99.600, train precision:99.557, train recall:99.643, train auc:99.992
fold:0 epoch:307 step:7 train loss:0.011121, train acc:99.615, train f1:99.615, train precision:99.591, train recall:99.640, train auc:99.991
fold:0 epoch:307 step:8 train loss:0.011285, train acc:99.594, train f1:99.594, train precision:99.525, train recall:99.664, train auc:99.992
fold:0 epoch:307 step:9 train loss:0.010222, train acc:99.683, train f1:99.686, train precision:99.773, train recall:99.599, train auc:99.993
fold:0 epoch:307        valid loss:0.057500, valid acc:98.697, valid f1:98.701, valid precision:98.406, valid recall:98.997, valid auc:99.841
[1;31mEarlyStopping counter: 16 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:308 step:0 train loss:0.009040, train acc:99.664, train f1:99.667, train precision:99.637, train recall:99.697, train auc:99.995
fold:0 epoch:308 step:1 train loss:0.010989, train acc:99.582, train f1:99.581, train precision:99.535, train recall:99.627, train auc:99.992
fold:0 epoch:308 step:2 train loss:0.009754, train acc:99.628, train f1:99.630, train precision:99.569, train recall:99.690, train auc:99.994
fold:0 epoch:308 step:3 train loss:0.009809, train acc:99.673, train f1:99.674, train precision:99.738, train recall:99.610, train auc:99.994
fold:0 epoch:308 step:4 train loss:0.010465, train acc:99.628, train f1:99.628, train precision:99.628, train recall:99.628, train auc:99.992
fold:0 epoch:308 step:5 train loss:0.010041, train acc:99.631, train f1:99.628, train precision:99.612, train recall:99.643, train auc:99.993
fold:0 epoch:308 step:6 train loss:0.010097, train acc:99.643, train f1:99.644, train precision:99.623, train recall:99.665, train auc:99.992
fold:0 epoch:308 step:7 train loss:0.010031, train acc:99.634, train f1:99.634, train precision:99.531, train recall:99.737, train auc:99.994
fold:0 epoch:308 step:8 train loss:0.009656, train acc:99.667, train f1:99.665, train precision:99.558, train recall:99.772, train auc:99.994
fold:0 epoch:308 step:9 train loss:0.009970, train acc:99.569, train f1:99.569, train precision:99.473, train recall:99.665, train auc:99.994
fold:0 epoch:308        valid loss:0.058267, valid acc:98.707, valid f1:98.712, valid precision:98.356, valid recall:99.070, valid auc:99.837
[1;31mEarlyStopping counter: 17 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:309 step:0 train loss:0.012059, train acc:99.564, train f1:99.562, train precision:99.718, train recall:99.407, train auc:99.989
fold:0 epoch:309 step:1 train loss:0.010504, train acc:99.606, train f1:99.607, train precision:99.653, train recall:99.562, train auc:99.991
fold:0 epoch:309 step:2 train loss:0.010495, train acc:99.628, train f1:99.626, train precision:99.534, train recall:99.718, train auc:99.993
fold:0 epoch:309 step:3 train loss:0.009496, train acc:99.652, train f1:99.652, train precision:99.579, train recall:99.725, train auc:99.994
fold:0 epoch:309 step:4 train loss:0.009720, train acc:99.649, train f1:99.647, train precision:99.577, train recall:99.717, train auc:99.994
fold:0 epoch:309 step:5 train loss:0.013208, train acc:99.527, train f1:99.530, train precision:99.563, train recall:99.496, train auc:99.989
fold:0 epoch:309 step:6 train loss:0.010702, train acc:99.597, train f1:99.594, train precision:99.637, train recall:99.551, train auc:99.993
fold:0 epoch:309 step:7 train loss:0.009480, train acc:99.637, train f1:99.638, train precision:99.587, train recall:99.690, train auc:99.995
fold:0 epoch:309 step:8 train loss:0.009469, train acc:99.683, train f1:99.686, train precision:99.758, train recall:99.614, train auc:99.993
fold:0 epoch:309 step:9 train loss:0.012003, train acc:99.587, train f1:99.585, train precision:99.523, train recall:99.647, train auc:99.989
fold:0 epoch:309        valid loss:0.058754, valid acc:98.728, valid f1:98.731, valid precision:98.487, valid recall:98.976, valid auc:99.837
[1;31mEarlyStopping counter: 18 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:310 step:0 train loss:0.009715, train acc:99.698, train f1:99.696, train precision:99.620, train recall:99.773, train auc:99.993
fold:0 epoch:310 step:1 train loss:0.010456, train acc:99.625, train f1:99.626, train precision:99.629, train recall:99.623, train auc:99.992
fold:0 epoch:310 step:2 train loss:0.010500, train acc:99.643, train f1:99.643, train precision:99.670, train recall:99.615, train auc:99.993
fold:0 epoch:310 step:3 train loss:0.010989, train acc:99.576, train f1:99.573, train precision:99.643, train recall:99.503, train auc:99.992
fold:0 epoch:310 step:4 train loss:0.012259, train acc:99.591, train f1:99.589, train precision:99.620, train recall:99.559, train auc:99.986
fold:0 epoch:310 step:5 train loss:0.010949, train acc:99.576, train f1:99.579, train precision:99.570, train recall:99.588, train auc:99.992
fold:0 epoch:310 step:6 train loss:0.011224, train acc:99.625, train f1:99.627, train precision:99.588, train recall:99.667, train auc:99.992
fold:0 epoch:310 step:7 train loss:0.013048, train acc:99.551, train f1:99.547, train precision:99.397, train recall:99.698, train auc:99.989
fold:0 epoch:310 step:8 train loss:0.014150, train acc:99.515, train f1:99.519, train precision:99.576, train recall:99.462, train auc:99.986
fold:0 epoch:310 step:9 train loss:0.009983, train acc:99.701, train f1:99.701, train precision:99.771, train recall:99.631, train auc:99.993
fold:0 epoch:310        valid loss:0.057456, valid acc:98.750, valid f1:98.754, valid precision:98.435, valid recall:99.075, valid auc:99.837
[1;31mEarlyStopping counter: 19 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:311 step:0 train loss:0.010477, train acc:99.628, train f1:99.630, train precision:99.612, train recall:99.648, train auc:99.992
fold:0 epoch:311 step:1 train loss:0.013286, train acc:99.564, train f1:99.559, train precision:99.445, train recall:99.673, train auc:99.984
fold:0 epoch:311 step:2 train loss:0.010823, train acc:99.649, train f1:99.652, train precision:99.746, train recall:99.558, train auc:99.992
fold:0 epoch:311 step:3 train loss:0.009929, train acc:99.655, train f1:99.656, train precision:99.635, train recall:99.677, train auc:99.994
fold:0 epoch:311 step:4 train loss:0.012207, train acc:99.579, train f1:99.579, train precision:99.633, train recall:99.524, train auc:99.989
fold:0 epoch:311 step:5 train loss:0.012350, train acc:99.557, train f1:99.558, train precision:99.513, train recall:99.604, train auc:99.988
fold:0 epoch:311 step:6 train loss:0.011542, train acc:99.570, train f1:99.569, train precision:99.518, train recall:99.621, train auc:99.990
fold:0 epoch:311 step:7 train loss:0.012147, train acc:99.570, train f1:99.568, train precision:99.571, train recall:99.565, train auc:99.989
fold:0 epoch:311 step:8 train loss:0.011213, train acc:99.628, train f1:99.627, train precision:99.481, train recall:99.774, train auc:99.991
fold:0 epoch:311 step:9 train loss:0.009714, train acc:99.692, train f1:99.692, train precision:99.683, train recall:99.701, train auc:99.993
fold:0 epoch:311        valid loss:0.055049, valid acc:98.746, valid f1:98.748, valid precision:98.576, valid recall:98.921, valid auc:99.838
[1;31mEarlyStopping counter: 20 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:312 step:0 train loss:0.011473, train acc:99.579, train f1:99.575, train precision:99.594, train recall:99.557, train auc:99.991
fold:0 epoch:312 step:1 train loss:0.010630, train acc:99.606, train f1:99.607, train precision:99.665, train recall:99.549, train auc:99.993
fold:0 epoch:312 step:2 train loss:0.010148, train acc:99.667, train f1:99.667, train precision:99.701, train recall:99.634, train auc:99.992
fold:0 epoch:312 step:3 train loss:0.009870, train acc:99.655, train f1:99.656, train precision:99.641, train recall:99.671, train auc:99.994
fold:0 epoch:312 step:4 train loss:0.010397, train acc:99.625, train f1:99.625, train precision:99.482, train recall:99.768, train auc:99.991
fold:0 epoch:312 step:5 train loss:0.009486, train acc:99.655, train f1:99.654, train precision:99.614, train recall:99.694, train auc:99.992
fold:0 epoch:312 step:6 train loss:0.010213, train acc:99.628, train f1:99.628, train precision:99.677, train recall:99.580, train auc:99.993
fold:0 epoch:312 step:7 train loss:0.012270, train acc:99.579, train f1:99.579, train precision:99.628, train recall:99.531, train auc:99.989
fold:0 epoch:312 step:8 train loss:0.010907, train acc:99.603, train f1:99.607, train precision:99.631, train recall:99.583, train auc:99.992
fold:0 epoch:312 step:9 train loss:0.014351, train acc:99.472, train f1:99.464, train precision:99.269, train recall:99.660, train auc:99.989
fold:0 epoch:312        valid loss:0.057896, valid acc:98.704, valid f1:98.709, valid precision:98.378, valid recall:99.041, valid auc:99.837
[1;31mEarlyStopping counter: 21 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:313 step:0 train loss:0.010227, train acc:99.619, train f1:99.620, train precision:99.568, train recall:99.671, train auc:99.993
fold:0 epoch:313 step:1 train loss:0.009870, train acc:99.646, train f1:99.643, train precision:99.692, train recall:99.593, train auc:99.994
fold:0 epoch:313 step:2 train loss:0.010211, train acc:99.609, train f1:99.609, train precision:99.725, train recall:99.494, train auc:99.994
fold:0 epoch:313 step:3 train loss:0.011558, train acc:99.576, train f1:99.578, train precision:99.623, train recall:99.532, train auc:99.992
fold:0 epoch:313 step:4 train loss:0.012995, train acc:99.539, train f1:99.543, train precision:99.408, train recall:99.679, train auc:99.988
fold:0 epoch:313 step:5 train loss:0.010376, train acc:99.612, train f1:99.611, train precision:99.541, train recall:99.681, train auc:99.994
fold:0 epoch:313 step:6 train loss:0.010357, train acc:99.649, train f1:99.650, train precision:99.574, train recall:99.726, train auc:99.992
fold:0 epoch:313 step:7 train loss:0.011197, train acc:99.585, train f1:99.584, train precision:99.621, train recall:99.548, train auc:99.992
fold:0 epoch:313 step:8 train loss:0.011473, train acc:99.588, train f1:99.587, train precision:99.602, train recall:99.571, train auc:99.991
fold:0 epoch:313 step:9 train loss:0.012935, train acc:99.464, train f1:99.463, train precision:99.542, train recall:99.385, train auc:99.987
fold:0 epoch:313        valid loss:0.058558, valid acc:98.740, valid f1:98.743, valid precision:98.493, valid recall:98.994, valid auc:99.837
[1;31mEarlyStopping counter: 22 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:314 step:0 train loss:0.011298, train acc:99.600, train f1:99.602, train precision:99.599, train recall:99.605, train auc:99.991
fold:0 epoch:314 step:1 train loss:0.012130, train acc:99.564, train f1:99.565, train precision:99.399, train recall:99.732, train auc:99.991
fold:0 epoch:314 step:2 train loss:0.010903, train acc:99.600, train f1:99.600, train precision:99.408, train recall:99.792, train auc:99.993
fold:0 epoch:314 step:3 train loss:0.010526, train acc:99.600, train f1:99.599, train precision:99.700, train recall:99.499, train auc:99.993
fold:0 epoch:314 step:4 train loss:0.012122, train acc:99.533, train f1:99.535, train precision:99.653, train recall:99.418, train auc:99.990
fold:0 epoch:314 step:5 train loss:0.010992, train acc:99.576, train f1:99.574, train precision:99.528, train recall:99.620, train auc:99.992
fold:0 epoch:314 step:6 train loss:0.011725, train acc:99.591, train f1:99.589, train precision:99.491, train recall:99.687, train auc:99.990
fold:0 epoch:314 step:7 train loss:0.010548, train acc:99.661, train f1:99.661, train precision:99.652, train recall:99.670, train auc:99.992
fold:0 epoch:314 step:8 train loss:0.011167, train acc:99.600, train f1:99.603, train precision:99.576, train recall:99.630, train auc:99.991
fold:0 epoch:314 step:9 train loss:0.012296, train acc:99.551, train f1:99.548, train precision:99.416, train recall:99.680, train auc:99.990
fold:0 epoch:314        valid loss:0.058519, valid acc:98.719, valid f1:98.722, valid precision:98.485, valid recall:98.960, valid auc:99.836
[1;31mEarlyStopping counter: 23 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:315 step:0 train loss:0.011269, train acc:99.622, train f1:99.618, train precision:99.581, train recall:99.655, train auc:99.992
fold:0 epoch:315 step:1 train loss:0.010588, train acc:99.588, train f1:99.589, train precision:99.665, train recall:99.513, train auc:99.993
fold:0 epoch:315 step:2 train loss:0.012371, train acc:99.573, train f1:99.571, train precision:99.504, train recall:99.638, train auc:99.989
fold:0 epoch:315 step:3 train loss:0.010458, train acc:99.643, train f1:99.644, train precision:99.605, train recall:99.684, train auc:99.992
fold:0 epoch:315 step:4 train loss:0.011357, train acc:99.600, train f1:99.603, train precision:99.594, train recall:99.612, train auc:99.990
fold:0 epoch:315 step:5 train loss:0.011329, train acc:99.585, train f1:99.582, train precision:99.527, train recall:99.637, train auc:99.991
fold:0 epoch:315 step:6 train loss:0.010134, train acc:99.658, train f1:99.658, train precision:99.646, train recall:99.670, train auc:99.992
fold:0 epoch:315 step:7 train loss:0.012322, train acc:99.521, train f1:99.523, train precision:99.514, train recall:99.532, train auc:99.989
fold:0 epoch:315 step:8 train loss:0.010744, train acc:99.615, train f1:99.617, train precision:99.745, train recall:99.491, train auc:99.993
fold:0 epoch:315 step:9 train loss:0.011398, train acc:99.595, train f1:99.594, train precision:99.454, train recall:99.735, train auc:99.992
fold:0 epoch:315        valid loss:0.055609, valid acc:98.748, valid f1:98.750, valid precision:98.561, valid recall:98.940, valid auc:99.840
[1;31mEarlyStopping counter: 24 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:316 step:0 train loss:0.010064, train acc:99.609, train f1:99.611, train precision:99.557, train recall:99.666, train auc:99.993
fold:0 epoch:316 step:1 train loss:0.009994, train acc:99.680, train f1:99.679, train precision:99.615, train recall:99.743, train auc:99.992
fold:0 epoch:316 step:2 train loss:0.008189, train acc:99.728, train f1:99.729, train precision:99.701, train recall:99.756, train auc:99.995
fold:0 epoch:316 step:3 train loss:0.010780, train acc:99.652, train f1:99.650, train precision:99.619, train recall:99.681, train auc:99.989
fold:0 epoch:316 step:4 train loss:0.009917, train acc:99.640, train f1:99.640, train precision:99.774, train recall:99.507, train auc:99.994
fold:0 epoch:316 step:5 train loss:0.010223, train acc:99.622, train f1:99.623, train precision:99.623, train recall:99.623, train auc:99.993
fold:0 epoch:316 step:6 train loss:0.011317, train acc:99.557, train f1:99.559, train precision:99.514, train recall:99.605, train auc:99.992
fold:0 epoch:316 step:7 train loss:0.010685, train acc:99.658, train f1:99.662, train precision:99.506, train recall:99.819, train auc:99.992
fold:0 epoch:316 step:8 train loss:0.009727, train acc:99.673, train f1:99.671, train precision:99.601, train recall:99.742, train auc:99.993
fold:0 epoch:316 step:9 train loss:0.008875, train acc:99.666, train f1:99.661, train precision:99.768, train recall:99.555, train auc:99.996
fold:0 epoch:316        valid loss:0.058737, valid acc:98.725, valid f1:98.730, valid precision:98.382, valid recall:99.081, valid auc:99.834
[1;31mEarlyStopping counter: 25 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:317 step:0 train loss:0.011526, train acc:99.606, train f1:99.606, train precision:99.780, train recall:99.432, train auc:99.991
fold:0 epoch:317 step:1 train loss:0.009156, train acc:99.646, train f1:99.646, train precision:99.591, train recall:99.701, train auc:99.995
fold:0 epoch:317 step:2 train loss:0.010216, train acc:99.658, train f1:99.661, train precision:99.511, train recall:99.812, train auc:99.993
fold:0 epoch:317 step:3 train loss:0.010205, train acc:99.652, train f1:99.651, train precision:99.511, train recall:99.792, train auc:99.994
fold:0 epoch:317 step:4 train loss:0.010155, train acc:99.643, train f1:99.644, train precision:99.683, train recall:99.605, train auc:99.993
fold:0 epoch:317 step:5 train loss:0.010864, train acc:99.612, train f1:99.606, train precision:99.640, train recall:99.572, train auc:99.992
fold:0 epoch:317 step:6 train loss:0.010220, train acc:99.631, train f1:99.633, train precision:99.751, train recall:99.516, train auc:99.994
fold:0 epoch:317 step:7 train loss:0.009178, train acc:99.652, train f1:99.650, train precision:99.620, train recall:99.681, train auc:99.994
fold:0 epoch:317 step:8 train loss:0.010433, train acc:99.655, train f1:99.657, train precision:99.606, train recall:99.708, train auc:99.992
fold:0 epoch:317 step:9 train loss:0.011378, train acc:99.578, train f1:99.579, train precision:99.526, train recall:99.631, train auc:99.992
fold:0 epoch:317        valid loss:0.057136, valid acc:98.738, valid f1:98.743, valid precision:98.407, valid recall:99.081, valid auc:99.843
[1;31mEarlyStopping counter: 26 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:318 step:0 train loss:0.009694, train acc:99.649, train f1:99.649, train precision:99.603, train recall:99.694, train auc:99.993
fold:0 epoch:318 step:1 train loss:0.011764, train acc:99.582, train f1:99.579, train precision:99.594, train recall:99.564, train auc:99.991
fold:0 epoch:318 step:2 train loss:0.010810, train acc:99.628, train f1:99.628, train precision:99.683, train recall:99.573, train auc:99.992
fold:0 epoch:318 step:3 train loss:0.010017, train acc:99.637, train f1:99.636, train precision:99.676, train recall:99.597, train auc:99.994
fold:0 epoch:318 step:4 train loss:0.011771, train acc:99.603, train f1:99.600, train precision:99.496, train recall:99.705, train auc:99.989
fold:0 epoch:318 step:5 train loss:0.010953, train acc:99.619, train f1:99.620, train precision:99.587, train recall:99.654, train auc:99.991
fold:0 epoch:318 step:6 train loss:0.011675, train acc:99.609, train f1:99.613, train precision:99.589, train recall:99.637, train auc:99.990
fold:0 epoch:318 step:7 train loss:0.009961, train acc:99.664, train f1:99.664, train precision:99.628, train recall:99.701, train auc:99.992
fold:0 epoch:318 step:8 train loss:0.010118, train acc:99.631, train f1:99.631, train precision:99.683, train recall:99.580, train auc:99.993
fold:0 epoch:318 step:9 train loss:0.010293, train acc:99.639, train f1:99.641, train precision:99.702, train recall:99.579, train auc:99.993
fold:0 epoch:318        valid loss:0.058572, valid acc:98.697, valid f1:98.701, valid precision:98.361, valid recall:99.044, valid auc:99.832
[1;31mEarlyStopping counter: 27 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:319 step:0 train loss:0.011798, train acc:99.570, train f1:99.569, train precision:99.469, train recall:99.670, train auc:99.991
fold:0 epoch:319 step:1 train loss:0.010368, train acc:99.634, train f1:99.636, train precision:99.612, train recall:99.660, train auc:99.992
fold:0 epoch:319 step:2 train loss:0.010690, train acc:99.591, train f1:99.591, train precision:99.555, train recall:99.628, train auc:99.993
fold:0 epoch:319 step:3 train loss:0.010733, train acc:99.637, train f1:99.634, train precision:99.624, train recall:99.643, train auc:99.991
fold:0 epoch:319 step:4 train loss:0.010066, train acc:99.640, train f1:99.637, train precision:99.637, train recall:99.637, train auc:99.993
fold:0 epoch:319 step:5 train loss:0.011043, train acc:99.609, train f1:99.614, train precision:99.770, train recall:99.457, train auc:99.993
fold:0 epoch:319 step:6 train loss:0.011737, train acc:99.594, train f1:99.595, train precision:99.556, train recall:99.634, train auc:99.989
fold:0 epoch:319 step:7 train loss:0.012402, train acc:99.588, train f1:99.591, train precision:99.419, train recall:99.763, train auc:99.990
fold:0 epoch:319 step:8 train loss:0.011449, train acc:99.594, train f1:99.593, train precision:99.474, train recall:99.712, train auc:99.991
fold:0 epoch:319 step:9 train loss:0.010127, train acc:99.666, train f1:99.664, train precision:99.647, train recall:99.682, train auc:99.992
fold:0 epoch:319        valid loss:0.058822, valid acc:98.699, valid f1:98.705, valid precision:98.291, valid recall:99.122, valid auc:99.837
[1;31mEarlyStopping counter: 28 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:320 step:0 train loss:0.011582, train acc:99.548, train f1:99.550, train precision:99.695, train recall:99.404, train auc:99.993
fold:0 epoch:320 step:1 train loss:0.009659, train acc:99.655, train f1:99.655, train precision:99.719, train recall:99.591, train auc:99.994
fold:0 epoch:320 step:2 train loss:0.008906, train acc:99.677, train f1:99.676, train precision:99.603, train recall:99.749, train auc:99.995
fold:0 epoch:320 step:3 train loss:0.010616, train acc:99.619, train f1:99.620, train precision:99.490, train recall:99.751, train auc:99.993
fold:0 epoch:320 step:4 train loss:0.010938, train acc:99.615, train f1:99.617, train precision:99.479, train recall:99.757, train auc:99.991
fold:0 epoch:320 step:5 train loss:0.011648, train acc:99.606, train f1:99.606, train precision:99.670, train recall:99.542, train auc:99.990
fold:0 epoch:320 step:6 train loss:0.010651, train acc:99.631, train f1:99.628, train precision:99.674, train recall:99.582, train auc:99.992
fold:0 epoch:320 step:7 train loss:0.010463, train acc:99.655, train f1:99.654, train precision:99.761, train recall:99.548, train auc:99.992
fold:0 epoch:320 step:8 train loss:0.009440, train acc:99.640, train f1:99.640, train precision:99.671, train recall:99.610, train auc:99.994
fold:0 epoch:320 step:9 train loss:0.010363, train acc:99.692, train f1:99.692, train precision:99.648, train recall:99.736, train auc:99.993
fold:0 epoch:320        valid loss:0.057473, valid acc:98.751, valid f1:98.755, valid precision:98.503, valid recall:99.007, valid auc:99.832
[1;31mEarlyStopping counter: 29 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:321 step:0 train loss:0.009208, train acc:99.655, train f1:99.654, train precision:99.516, train recall:99.791, train auc:99.995
fold:0 epoch:321 step:1 train loss:0.009478, train acc:99.661, train f1:99.662, train precision:99.586, train recall:99.738, train auc:99.993
fold:0 epoch:321 step:2 train loss:0.011930, train acc:99.548, train f1:99.550, train precision:99.677, train recall:99.423, train auc:99.992
fold:0 epoch:321 step:3 train loss:0.010484, train acc:99.615, train f1:99.615, train precision:99.719, train recall:99.512, train auc:99.993
fold:0 epoch:321 step:4 train loss:0.010663, train acc:99.622, train f1:99.625, train precision:99.619, train recall:99.631, train auc:99.992
fold:0 epoch:321 step:5 train loss:0.012119, train acc:99.554, train f1:99.553, train precision:99.389, train recall:99.718, train auc:99.991
fold:0 epoch:321 step:6 train loss:0.010067, train acc:99.603, train f1:99.601, train precision:99.472, train recall:99.729, train auc:99.994
fold:0 epoch:321 step:7 train loss:0.009841, train acc:99.652, train f1:99.651, train precision:99.688, train recall:99.615, train auc:99.994
fold:0 epoch:321 step:8 train loss:0.012229, train acc:99.582, train f1:99.584, train precision:99.744, train recall:99.424, train auc:99.990
fold:0 epoch:321 step:9 train loss:0.009180, train acc:99.683, train f1:99.683, train precision:99.683, train recall:99.683, train auc:99.992
fold:0 epoch:321        valid loss:0.058785, valid acc:98.718, valid f1:98.722, valid precision:98.369, valid recall:99.078, valid auc:99.832
[1;31mEarlyStopping counter: 30 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:322 step:0 train loss:0.009230, train acc:99.667, train f1:99.665, train precision:99.600, train recall:99.729, train auc:99.994
fold:0 epoch:322 step:1 train loss:0.010365, train acc:99.619, train f1:99.622, train precision:99.547, train recall:99.698, train auc:99.993
fold:0 epoch:322 step:2 train loss:0.009435, train acc:99.695, train f1:99.695, train precision:99.689, train recall:99.701, train auc:99.992
fold:0 epoch:322 step:3 train loss:0.009305, train acc:99.686, train f1:99.689, train precision:99.680, train recall:99.698, train auc:99.994
fold:0 epoch:322 step:4 train loss:0.010427, train acc:99.661, train f1:99.660, train precision:99.743, train recall:99.578, train auc:99.992
fold:0 epoch:322 step:5 train loss:0.011475, train acc:99.588, train f1:99.587, train precision:99.615, train recall:99.560, train auc:99.990
fold:0 epoch:322 step:6 train loss:0.011637, train acc:99.564, train f1:99.562, train precision:99.492, train recall:99.632, train auc:99.991
fold:0 epoch:322 step:7 train loss:0.010045, train acc:99.628, train f1:99.627, train precision:99.572, train recall:99.682, train auc:99.993
fold:0 epoch:322 step:8 train loss:0.008156, train acc:99.710, train f1:99.708, train precision:99.675, train recall:99.742, train auc:99.996
fold:0 epoch:322 step:9 train loss:0.013559, train acc:99.481, train f1:99.486, train precision:99.738, train recall:99.235, train auc:99.990
fold:0 epoch:322        valid loss:0.057300, valid acc:98.746, valid f1:98.749, valid precision:98.523, valid recall:98.976, valid auc:99.836
[1;31mEarlyStopping counter: 31 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:323 step:0 train loss:0.011775, train acc:99.591, train f1:99.591, train precision:99.482, train recall:99.701, train auc:99.990
fold:0 epoch:323 step:1 train loss:0.011259, train acc:99.588, train f1:99.588, train precision:99.397, train recall:99.780, train auc:99.992
fold:0 epoch:323 step:2 train loss:0.011233, train acc:99.619, train f1:99.617, train precision:99.511, train recall:99.724, train auc:99.992
fold:0 epoch:323 step:3 train loss:0.011089, train acc:99.622, train f1:99.621, train precision:99.700, train recall:99.542, train auc:99.992
fold:0 epoch:323 step:4 train loss:0.011185, train acc:99.582, train f1:99.586, train precision:99.734, train recall:99.438, train auc:99.992
fold:0 epoch:323 step:5 train loss:0.009863, train acc:99.649, train f1:99.647, train precision:99.699, train recall:99.595, train auc:99.993
fold:0 epoch:323 step:6 train loss:0.011570, train acc:99.588, train f1:99.585, train precision:99.491, train recall:99.680, train auc:99.991
fold:0 epoch:323 step:7 train loss:0.011596, train acc:99.588, train f1:99.591, train precision:99.497, train recall:99.685, train auc:99.991
fold:0 epoch:323 step:8 train loss:0.009721, train acc:99.655, train f1:99.657, train precision:99.581, train recall:99.733, train auc:99.993
fold:0 epoch:323 step:9 train loss:0.014542, train acc:99.420, train f1:99.417, train precision:99.312, train recall:99.523, train auc:99.984
fold:0 epoch:323        valid loss:0.056715, valid acc:98.736, valid f1:98.740, valid precision:98.417, valid recall:99.065, valid auc:99.842
[1;31mEarlyStopping counter: 32 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:324 step:0 train loss:0.009426, train acc:99.664, train f1:99.666, train precision:99.757, train recall:99.576, train auc:99.994
fold:0 epoch:324 step:1 train loss:0.010280, train acc:99.628, train f1:99.627, train precision:99.682, train recall:99.572, train auc:99.993
fold:0 epoch:324 step:2 train loss:0.010232, train acc:99.634, train f1:99.632, train precision:99.638, train recall:99.626, train auc:99.993
fold:0 epoch:324 step:3 train loss:0.010337, train acc:99.606, train f1:99.606, train precision:99.512, train recall:99.700, train auc:99.993
fold:0 epoch:324 step:4 train loss:0.011452, train acc:99.615, train f1:99.616, train precision:99.544, train recall:99.689, train auc:99.990
fold:0 epoch:324 step:5 train loss:0.010450, train acc:99.634, train f1:99.635, train precision:99.574, train recall:99.696, train auc:99.993
fold:0 epoch:324 step:6 train loss:0.010347, train acc:99.643, train f1:99.642, train precision:99.645, train recall:99.639, train auc:99.992
fold:0 epoch:324 step:7 train loss:0.011120, train acc:99.649, train f1:99.649, train precision:99.689, train recall:99.610, train auc:99.991
fold:0 epoch:324 step:8 train loss:0.011343, train acc:99.609, train f1:99.609, train precision:99.567, train recall:99.652, train auc:99.990
fold:0 epoch:324 step:9 train loss:0.010509, train acc:99.604, train f1:99.603, train precision:99.542, train recall:99.665, train auc:99.994
fold:0 epoch:324        valid loss:0.055592, valid acc:98.763, valid f1:98.766, valid precision:98.506, valid recall:99.028, valid auc:99.839
[1;31mEarlyStopping counter: 33 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:325 step:0 train loss:0.009157, train acc:99.689, train f1:99.690, train precision:99.678, train recall:99.702, train auc:99.994
fold:0 epoch:325 step:1 train loss:0.010159, train acc:99.637, train f1:99.636, train precision:99.609, train recall:99.664, train auc:99.994
fold:0 epoch:325 step:2 train loss:0.009787, train acc:99.634, train f1:99.634, train precision:99.628, train recall:99.640, train auc:99.993
fold:0 epoch:325 step:3 train loss:0.008768, train acc:99.673, train f1:99.675, train precision:99.624, train recall:99.727, train auc:99.995
fold:0 epoch:325 step:4 train loss:0.010434, train acc:99.606, train f1:99.605, train precision:99.590, train recall:99.621, train auc:99.992
fold:0 epoch:325 step:5 train loss:0.009939, train acc:99.637, train f1:99.637, train precision:99.634, train recall:99.640, train auc:99.993
fold:0 epoch:325 step:6 train loss:0.010867, train acc:99.631, train f1:99.633, train precision:99.697, train recall:99.570, train auc:99.992
fold:0 epoch:325 step:7 train loss:0.010226, train acc:99.612, train f1:99.611, train precision:99.535, train recall:99.687, train auc:99.994
fold:0 epoch:325 step:8 train loss:0.010492, train acc:99.622, train f1:99.618, train precision:99.545, train recall:99.692, train auc:99.993
fold:0 epoch:325 step:9 train loss:0.009708, train acc:99.639, train f1:99.642, train precision:99.599, train recall:99.686, train auc:99.994
fold:0 epoch:325        valid loss:0.055842, valid acc:98.791, valid f1:98.793, valid precision:98.575, valid recall:99.013, valid auc:99.839
[1;31mEarlyStopping counter: 34 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:326 step:0 train loss:0.009964, train acc:99.637, train f1:99.638, train precision:99.690, train recall:99.587, train auc:99.994
fold:0 epoch:326 step:1 train loss:0.010525, train acc:99.637, train f1:99.640, train precision:99.679, train recall:99.601, train auc:99.992
fold:0 epoch:326 step:2 train loss:0.009195, train acc:99.652, train f1:99.653, train precision:99.563, train recall:99.744, train auc:99.995
fold:0 epoch:326 step:3 train loss:0.009916, train acc:99.677, train f1:99.676, train precision:99.621, train recall:99.731, train auc:99.992
fold:0 epoch:326 step:4 train loss:0.010037, train acc:99.625, train f1:99.623, train precision:99.480, train recall:99.767, train auc:99.993
fold:0 epoch:326 step:5 train loss:0.009642, train acc:99.652, train f1:99.653, train precision:99.635, train recall:99.671, train auc:99.994
fold:0 epoch:326 step:6 train loss:0.011181, train acc:99.594, train f1:99.591, train precision:99.711, train recall:99.472, train auc:99.992
fold:0 epoch:326 step:7 train loss:0.010103, train acc:99.637, train f1:99.635, train precision:99.693, train recall:99.577, train auc:99.993
fold:0 epoch:326 step:8 train loss:0.009034, train acc:99.704, train f1:99.704, train precision:99.713, train recall:99.695, train auc:99.994
fold:0 epoch:326 step:9 train loss:0.010831, train acc:99.622, train f1:99.620, train precision:99.489, train recall:99.752, train auc:99.990
fold:0 epoch:326        valid loss:0.057189, valid acc:98.757, valid f1:98.761, valid precision:98.458, valid recall:99.065, valid auc:99.839
[1;31mEarlyStopping counter: 35 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:327 step:0 train loss:0.008995, train acc:99.667, train f1:99.667, train precision:99.567, train recall:99.768, train auc:99.995
fold:0 epoch:327 step:1 train loss:0.009671, train acc:99.664, train f1:99.666, train precision:99.672, train recall:99.660, train auc:99.994
fold:0 epoch:327 step:2 train loss:0.008845, train acc:99.661, train f1:99.660, train precision:99.578, train recall:99.743, train auc:99.994
fold:0 epoch:327 step:3 train loss:0.009119, train acc:99.661, train f1:99.658, train precision:99.588, train recall:99.729, train auc:99.995
fold:0 epoch:327 step:4 train loss:0.010042, train acc:99.655, train f1:99.655, train precision:99.682, train recall:99.627, train auc:99.992
fold:0 epoch:327 step:5 train loss:0.009853, train acc:99.649, train f1:99.647, train precision:99.717, train recall:99.577, train auc:99.994
fold:0 epoch:327 step:6 train loss:0.011175, train acc:99.585, train f1:99.588, train precision:99.534, train recall:99.642, train auc:99.992
fold:0 epoch:327 step:7 train loss:0.010196, train acc:99.634, train f1:99.636, train precision:99.624, train recall:99.648, train auc:99.992
fold:0 epoch:327 step:8 train loss:0.010519, train acc:99.637, train f1:99.637, train precision:99.513, train recall:99.762, train auc:99.993
fold:0 epoch:327 step:9 train loss:0.011391, train acc:99.622, train f1:99.624, train precision:99.545, train recall:99.702, train auc:99.990
fold:0 epoch:327        valid loss:0.057852, valid acc:98.771, valid f1:98.774, valid precision:98.534, valid recall:99.015, valid auc:99.835
[1;31mEarlyStopping counter: 36 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:328 step:0 train loss:0.009820, train acc:99.628, train f1:99.626, train precision:99.626, train recall:99.626, train auc:99.994
fold:0 epoch:328 step:1 train loss:0.009525, train acc:99.661, train f1:99.657, train precision:99.697, train recall:99.617, train auc:99.993
fold:0 epoch:328 step:2 train loss:0.010207, train acc:99.606, train f1:99.605, train precision:99.681, train recall:99.528, train auc:99.993
fold:0 epoch:328 step:3 train loss:0.009747, train acc:99.655, train f1:99.656, train precision:99.605, train recall:99.708, train auc:99.993
fold:0 epoch:328 step:4 train loss:0.010100, train acc:99.631, train f1:99.631, train precision:99.568, train recall:99.695, train auc:99.992
fold:0 epoch:328 step:5 train loss:0.009716, train acc:99.634, train f1:99.634, train precision:99.537, train recall:99.731, train auc:99.993
fold:0 epoch:328 step:6 train loss:0.010859, train acc:99.615, train f1:99.621, train precision:99.663, train recall:99.579, train auc:99.991
fold:0 epoch:328 step:7 train loss:0.010200, train acc:99.646, train f1:99.646, train precision:99.628, train recall:99.664, train auc:99.993
fold:0 epoch:328 step:8 train loss:0.011641, train acc:99.585, train f1:99.587, train precision:99.569, train recall:99.605, train auc:99.989
fold:0 epoch:328 step:9 train loss:0.013546, train acc:99.525, train f1:99.525, train precision:99.542, train recall:99.507, train auc:99.983
fold:0 epoch:328        valid loss:0.056792, valid acc:98.770, valid f1:98.773, valid precision:98.511, valid recall:99.036, valid auc:99.847
[1;31mEarlyStopping counter: 37 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:329 step:0 train loss:0.009704, train acc:99.643, train f1:99.643, train precision:99.634, train recall:99.652, train auc:99.994
fold:0 epoch:329 step:1 train loss:0.010405, train acc:99.597, train f1:99.596, train precision:99.450, train recall:99.743, train auc:99.993
fold:0 epoch:329 step:2 train loss:0.009809, train acc:99.640, train f1:99.642, train precision:99.648, train recall:99.636, train auc:99.993
fold:0 epoch:329 step:3 train loss:0.010322, train acc:99.643, train f1:99.644, train precision:99.635, train recall:99.653, train auc:99.993
fold:0 epoch:329 step:4 train loss:0.009405, train acc:99.658, train f1:99.658, train precision:99.609, train recall:99.707, train auc:99.995
fold:0 epoch:329 step:5 train loss:0.009393, train acc:99.667, train f1:99.668, train precision:99.683, train recall:99.653, train auc:99.994
fold:0 epoch:329 step:6 train loss:0.010423, train acc:99.628, train f1:99.629, train precision:99.586, train recall:99.671, train auc:99.993
fold:0 epoch:329 step:7 train loss:0.009769, train acc:99.677, train f1:99.675, train precision:99.712, train recall:99.638, train auc:99.993
fold:0 epoch:329 step:8 train loss:0.009952, train acc:99.637, train f1:99.637, train precision:99.616, train recall:99.659, train auc:99.992
fold:0 epoch:329 step:9 train loss:0.012099, train acc:99.604, train f1:99.600, train precision:99.521, train recall:99.680, train auc:99.990
fold:0 epoch:329        valid loss:0.055757, valid acc:98.774, valid f1:98.777, valid precision:98.504, valid recall:99.052, valid auc:99.847
[1;31mEarlyStopping counter: 38 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:330 step:0 train loss:0.009972, train acc:99.628, train f1:99.626, train precision:99.705, train recall:99.547, train auc:99.994
fold:0 epoch:330 step:1 train loss:0.010209, train acc:99.625, train f1:99.625, train precision:99.634, train recall:99.616, train auc:99.993
fold:0 epoch:330 step:2 train loss:0.010092, train acc:99.622, train f1:99.620, train precision:99.486, train recall:99.754, train auc:99.993
fold:0 epoch:330 step:3 train loss:0.009527, train acc:99.649, train f1:99.650, train precision:99.599, train recall:99.702, train auc:99.993
fold:0 epoch:330 step:4 train loss:0.011344, train acc:99.612, train f1:99.614, train precision:99.557, train recall:99.672, train auc:99.991
fold:0 epoch:330 step:5 train loss:0.010055, train acc:99.619, train f1:99.617, train precision:99.571, train recall:99.663, train auc:99.993
fold:0 epoch:330 step:6 train loss:0.011133, train acc:99.622, train f1:99.623, train precision:99.598, train recall:99.647, train auc:99.990
fold:0 epoch:330 step:7 train loss:0.011299, train acc:99.557, train f1:99.556, train precision:99.669, train recall:99.444, train auc:99.992
fold:0 epoch:330 step:8 train loss:0.010089, train acc:99.637, train f1:99.638, train precision:99.599, train recall:99.677, train auc:99.992
fold:0 epoch:330 step:9 train loss:0.010378, train acc:99.648, train f1:99.651, train precision:99.634, train recall:99.668, train auc:99.993
fold:0 epoch:330        valid loss:0.056467, valid acc:98.772, valid f1:98.776, valid precision:98.484, valid recall:99.070, valid auc:99.839
[1;31mEarlyStopping counter: 39 out of 50[0m
[98.7984850463628, 98.80092797747831, 98.60045264171067, 99.00222019067519, 99.83714932354448]
====================================================================================================
fold:0 epoch:331 step:0 train loss:0.010494, train acc:99.594, train f1:99.594, train precision:99.500, train recall:99.688, train auc:99.993
fold:0 epoch:331 step:1 train loss:0.011302, train acc:99.594, train f1:99.592, train precision:99.467, train recall:99.718, train auc:99.991
fold:0 epoch:331 step:2 train loss:0.009980, train acc:99.640, train f1:99.641, train precision:99.671, train recall:99.611, train auc:99.993
fold:0 epoch:331 step:3 train loss:0.009753, train acc:99.612, train f1:99.615, train precision:99.690, train recall:99.539, train auc:99.994
fold:0 epoch:331 step:4 train loss:0.008788, train acc:99.695, train f1:99.693, train precision:99.645, train recall:99.742, train auc:99.994
fold:0 epoch:331 step:5 train loss:0.012050, train acc:99.588, train f1:99.589, train precision:99.555, train recall:99.622, train auc:99.988
fold:0 epoch:331 step:6 train loss:0.010807, train acc:99.625, train f1:99.624, train precision:99.573, train recall:99.676, train auc:99.992
fold:0 epoch:331 step:7 train loss:0.010262, train acc:99.649, train f1:99.650, train precision:99.616, train recall:99.683, train auc:99.992
fold:0 epoch:331 step:8 train loss:0.010885, train acc:99.640, train f1:99.640, train precision:99.658, train recall:99.622, train auc:99.991
fold:0 epoch:331 step:9 train loss:0.010764, train acc:99.648, train f1:99.648, train precision:99.771, train recall:99.525, train auc:99.992
fold:0 epoch:331        valid loss:0.053694, valid acc:98.802, valid f1:98.804, valid precision:98.704, valid recall:98.903, valid auc:99.838
[1;31mTest score increased (98.798485 --> 98.802403).[0m
[98.80240302990727, 98.80360614244523, 98.7044471091184, 98.90296460754865, 99.83839592831265]
====================================================================================================
fold:0 epoch:332 step:0 train loss:0.011009, train acc:99.612, train f1:99.613, train precision:99.574, train recall:99.653, train auc:99.991
fold:0 epoch:332 step:1 train loss:0.010808, train acc:99.628, train f1:99.631, train precision:99.510, train recall:99.752, train auc:99.992
fold:0 epoch:332 step:2 train loss:0.009372, train acc:99.646, train f1:99.649, train precision:99.661, train recall:99.637, train auc:99.995
fold:0 epoch:332 step:3 train loss:0.011580, train acc:99.570, train f1:99.570, train precision:99.519, train recall:99.622, train auc:99.991
fold:0 epoch:332 step:4 train loss:0.010274, train acc:99.606, train f1:99.603, train precision:99.668, train recall:99.539, train auc:99.993
fold:0 epoch:332 step:5 train loss:0.009805, train acc:99.658, train f1:99.656, train precision:99.632, train recall:99.681, train auc:99.993
fold:0 epoch:332 step:6 train loss:0.010901, train acc:99.634, train f1:99.631, train precision:99.527, train recall:99.735, train auc:99.992
fold:0 epoch:332 step:7 train loss:0.009731, train acc:99.677, train f1:99.678, train precision:99.721, train recall:99.636, train auc:99.992
fold:0 epoch:332 step:8 train loss:0.009658, train acc:99.649, train f1:99.648, train precision:99.645, train recall:99.651, train auc:99.994
fold:0 epoch:332 step:9 train loss:0.011410, train acc:99.631, train f1:99.630, train precision:99.577, train recall:99.683, train auc:99.992
fold:0 epoch:332        valid loss:0.056924, valid acc:98.721, valid f1:98.725, valid precision:98.422, valid recall:99.031, valid auc:99.839
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.80240302990727, 98.80360614244523, 98.7044471091184, 98.90296460754865, 99.83839592831265]
====================================================================================================
fold:0 epoch:333 step:0 train loss:0.009737, train acc:99.643, train f1:99.644, train precision:99.683, train recall:99.604, train auc:99.994
fold:0 epoch:333 step:1 train loss:0.010087, train acc:99.670, train f1:99.670, train precision:99.683, train recall:99.658, train auc:99.993
fold:0 epoch:333 step:2 train loss:0.009429, train acc:99.658, train f1:99.656, train precision:99.540, train recall:99.772, train auc:99.995
fold:0 epoch:333 step:3 train loss:0.009738, train acc:99.658, train f1:99.660, train precision:99.666, train recall:99.654, train auc:99.993
fold:0 epoch:333 step:4 train loss:0.012216, train acc:99.561, train f1:99.560, train precision:99.554, train recall:99.566, train auc:99.989
fold:0 epoch:333 step:5 train loss:0.010979, train acc:99.594, train f1:99.595, train precision:99.707, train recall:99.483, train auc:99.992
fold:0 epoch:333 step:6 train loss:0.009701, train acc:99.667, train f1:99.667, train precision:99.554, train recall:99.780, train auc:99.993
fold:0 epoch:333 step:7 train loss:0.008784, train acc:99.707, train f1:99.706, train precision:99.663, train recall:99.748, train auc:99.995
fold:0 epoch:333 step:8 train loss:0.009900, train acc:99.619, train f1:99.619, train precision:99.629, train recall:99.610, train auc:99.994
fold:0 epoch:333 step:9 train loss:0.009480, train acc:99.639, train f1:99.644, train precision:99.739, train recall:99.549, train auc:99.994
fold:0 epoch:333        valid loss:0.056074, valid acc:98.733, valid f1:98.737, valid precision:98.427, valid recall:99.049, valid auc:99.846
[1;31mEarlyStopping counter: 2 out of 50[0m
[98.80240302990727, 98.80360614244523, 98.7044471091184, 98.90296460754865, 99.83839592831265]
====================================================================================================
fold:0 epoch:334 step:0 train loss:0.010662, train acc:99.646, train f1:99.642, train precision:99.568, train recall:99.716, train auc:99.993
fold:0 epoch:334 step:1 train loss:0.010096, train acc:99.649, train f1:99.651, train precision:99.636, train recall:99.666, train auc:99.993
fold:0 epoch:334 step:2 train loss:0.009760, train acc:99.637, train f1:99.637, train precision:99.609, train recall:99.664, train auc:99.994
fold:0 epoch:334 step:3 train loss:0.009072, train acc:99.680, train f1:99.680, train precision:99.695, train recall:99.665, train auc:99.994
fold:0 epoch:334 step:4 train loss:0.010064, train acc:99.631, train f1:99.631, train precision:99.580, train recall:99.683, train auc:99.994
fold:0 epoch:334 step:5 train loss:0.012760, train acc:99.554, train f1:99.557, train precision:99.690, train recall:99.425, train auc:99.989
fold:0 epoch:334 step:6 train loss:0.010372, train acc:99.628, train f1:99.628, train precision:99.501, train recall:99.756, train auc:99.993
fold:0 epoch:334 step:7 train loss:0.011008, train acc:99.600, train f1:99.602, train precision:99.496, train recall:99.708, train auc:99.991
fold:0 epoch:334 step:8 train loss:0.010420, train acc:99.634, train f1:99.632, train precision:99.613, train recall:99.650, train auc:99.993
fold:0 epoch:334 step:9 train loss:0.009902, train acc:99.666, train f1:99.666, train precision:99.736, train recall:99.596, train auc:99.994
fold:0 epoch:334        valid loss:0.056290, valid acc:98.728, valid f1:98.732, valid precision:98.399, valid recall:99.068, valid auc:99.846
[1;31mEarlyStopping counter: 3 out of 50[0m
[98.80240302990727, 98.80360614244523, 98.7044471091184, 98.90296460754865, 99.83839592831265]
====================================================================================================
fold:0 epoch:335 step:0 train loss:0.010195, train acc:99.649, train f1:99.647, train precision:99.632, train recall:99.663, train auc:99.992
fold:0 epoch:335 step:1 train loss:0.009328, train acc:99.661, train f1:99.662, train precision:99.702, train recall:99.623, train auc:99.993
fold:0 epoch:335 step:2 train loss:0.009539, train acc:99.677, train f1:99.676, train precision:99.602, train recall:99.749, train auc:99.994
fold:0 epoch:335 step:3 train loss:0.008973, train acc:99.655, train f1:99.657, train precision:99.593, train recall:99.721, train auc:99.995
fold:0 epoch:335 step:4 train loss:0.009642, train acc:99.646, train f1:99.650, train precision:99.559, train recall:99.740, train auc:99.994
fold:0 epoch:335 step:5 train loss:0.009216, train acc:99.677, train f1:99.679, train precision:99.709, train recall:99.649, train auc:99.995
fold:0 epoch:335 step:6 train loss:0.009028, train acc:99.677, train f1:99.671, train precision:99.641, train recall:99.702, train auc:99.995
fold:0 epoch:335 step:7 train loss:0.010704, train acc:99.625, train f1:99.621, train precision:99.674, train recall:99.569, train auc:99.993
fold:0 epoch:335 step:8 train loss:0.010210, train acc:99.603, train f1:99.605, train precision:99.641, train recall:99.568, train auc:99.993
fold:0 epoch:335 step:9 train loss:0.010886, train acc:99.631, train f1:99.634, train precision:99.617, train recall:99.651, train auc:99.992
fold:0 epoch:335        valid loss:0.056911, valid acc:98.771, valid f1:98.773, valid precision:98.582, valid recall:98.966, valid auc:99.843
[1;31mEarlyStopping counter: 4 out of 50[0m
[98.80240302990727, 98.80360614244523, 98.7044471091184, 98.90296460754865, 99.83839592831265]
====================================================================================================
fold:0 epoch:336 step:0 train loss:0.008931, train acc:99.649, train f1:99.651, train precision:99.466, train recall:99.836, train auc:99.995
fold:0 epoch:336 step:1 train loss:0.007914, train acc:99.704, train f1:99.706, train precision:99.679, train recall:99.733, train auc:99.996
fold:0 epoch:336 step:2 train loss:0.010321, train acc:99.631, train f1:99.634, train precision:99.685, train recall:99.583, train auc:99.991
fold:0 epoch:336 step:3 train loss:0.010145, train acc:99.658, train f1:99.657, train precision:99.718, train recall:99.596, train auc:99.993
fold:0 epoch:336 step:4 train loss:0.010920, train acc:99.619, train f1:99.615, train precision:99.587, train recall:99.642, train auc:99.991
fold:0 epoch:336 step:5 train loss:0.009295, train acc:99.673, train f1:99.675, train precision:99.708, train recall:99.642, train auc:99.994
fold:0 epoch:336 step:6 train loss:0.009950, train acc:99.646, train f1:99.642, train precision:99.569, train recall:99.716, train auc:99.994
fold:0 epoch:336 step:7 train loss:0.010179, train acc:99.634, train f1:99.636, train precision:99.564, train recall:99.709, train auc:99.992
fold:0 epoch:336 step:8 train loss:0.010808, train acc:99.609, train f1:99.607, train precision:99.460, train recall:99.754, train auc:99.992
fold:0 epoch:336 step:9 train loss:0.008069, train acc:99.727, train f1:99.729, train precision:99.720, train recall:99.737, train auc:99.996
fold:0 epoch:336        valid loss:0.055323, valid acc:98.765, valid f1:98.768, valid precision:98.516, valid recall:99.021, valid auc:99.849
[1;31mEarlyStopping counter: 5 out of 50[0m
[98.80240302990727, 98.80360614244523, 98.7044471091184, 98.90296460754865, 99.83839592831265]
====================================================================================================
fold:0 epoch:337 step:0 train loss:0.010977, train acc:99.622, train f1:99.620, train precision:99.785, train recall:99.455, train auc:99.993
fold:0 epoch:337 step:1 train loss:0.010134, train acc:99.634, train f1:99.635, train precision:99.793, train recall:99.478, train auc:99.994
fold:0 epoch:337 step:2 train loss:0.009807, train acc:99.637, train f1:99.637, train precision:99.489, train recall:99.786, train auc:99.994
fold:0 epoch:337 step:3 train loss:0.011664, train acc:99.570, train f1:99.572, train precision:99.364, train recall:99.781, train auc:99.992
fold:0 epoch:337 step:4 train loss:0.010269, train acc:99.573, train f1:99.569, train precision:99.459, train recall:99.680, train auc:99.993
fold:0 epoch:337 step:5 train loss:0.011522, train acc:99.606, train f1:99.605, train precision:99.736, train recall:99.474, train auc:99.991
fold:0 epoch:337 step:6 train loss:0.010757, train acc:99.600, train f1:99.600, train precision:99.755, train recall:99.445, train auc:99.994
fold:0 epoch:337 step:7 train loss:0.010750, train acc:99.597, train f1:99.596, train precision:99.596, train recall:99.596, train auc:99.993
fold:0 epoch:337 step:8 train loss:0.010146, train acc:99.670, train f1:99.672, train precision:99.611, train recall:99.732, train auc:99.993
fold:0 epoch:337 step:9 train loss:0.012717, train acc:99.551, train f1:99.556, train precision:99.392, train recall:99.721, train auc:99.990
fold:0 epoch:337        valid loss:0.059163, valid acc:98.767, valid f1:98.771, valid precision:98.466, valid recall:99.078, valid auc:99.833
[1;31mEarlyStopping counter: 6 out of 50[0m
[98.80240302990727, 98.80360614244523, 98.7044471091184, 98.90296460754865, 99.83839592831265]
====================================================================================================
fold:0 epoch:338 step:0 train loss:0.009650, train acc:99.670, train f1:99.670, train precision:99.651, train recall:99.688, train auc:99.994
fold:0 epoch:338 step:1 train loss:0.010650, train acc:99.649, train f1:99.651, train precision:99.642, train recall:99.660, train auc:99.992
fold:0 epoch:338 step:2 train loss:0.010599, train acc:99.609, train f1:99.610, train precision:99.829, train recall:99.392, train auc:99.994
fold:0 epoch:338 step:3 train loss:0.010621, train acc:99.628, train f1:99.629, train precision:99.653, train recall:99.605, train auc:99.992
fold:0 epoch:338 step:4 train loss:0.010676, train acc:99.600, train f1:99.602, train precision:99.447, train recall:99.756, train auc:99.993
fold:0 epoch:338 step:5 train loss:0.011086, train acc:99.570, train f1:99.570, train precision:99.404, train recall:99.737, train auc:99.993
fold:0 epoch:338 step:6 train loss:0.012772, train acc:99.533, train f1:99.531, train precision:99.503, train recall:99.558, train auc:99.989
fold:0 epoch:338 step:7 train loss:0.011068, train acc:99.625, train f1:99.624, train precision:99.682, train recall:99.566, train auc:99.992
fold:0 epoch:338 step:8 train loss:0.011746, train acc:99.600, train f1:99.600, train precision:99.786, train recall:99.415, train auc:99.991
fold:0 epoch:338 step:9 train loss:0.012061, train acc:99.595, train f1:99.587, train precision:99.497, train recall:99.676, train auc:99.988
fold:0 epoch:338        valid loss:0.055678, valid acc:98.805, valid f1:98.808, valid precision:98.596, valid recall:99.021, valid auc:99.839
[1;31mTest score increased (98.802403 --> 98.805015).[0m
[98.80501501893693, 98.80758454421058, 98.59557867360208, 99.02050411388272, 99.8394003348033]
====================================================================================================
fold:0 epoch:339 step:0 train loss:0.010609, train acc:99.597, train f1:99.595, train precision:99.534, train recall:99.656, train auc:99.993
fold:0 epoch:339 step:1 train loss:0.008809, train acc:99.673, train f1:99.673, train precision:99.615, train recall:99.731, train auc:99.994
fold:0 epoch:339 step:2 train loss:0.010395, train acc:99.591, train f1:99.591, train precision:99.591, train recall:99.591, train auc:99.993
fold:0 epoch:339 step:3 train loss:0.012507, train acc:99.554, train f1:99.559, train precision:99.631, train recall:99.487, train auc:99.989
fold:0 epoch:339 step:4 train loss:0.008660, train acc:99.698, train f1:99.696, train precision:99.662, train recall:99.729, train auc:99.995
fold:0 epoch:339 step:5 train loss:0.010800, train acc:99.631, train f1:99.630, train precision:99.505, train recall:99.755, train auc:99.992
fold:0 epoch:339 step:6 train loss:0.011081, train acc:99.612, train f1:99.610, train precision:99.571, train recall:99.650, train auc:99.992
fold:0 epoch:339 step:7 train loss:0.010618, train acc:99.606, train f1:99.610, train precision:99.625, train recall:99.595, train auc:99.990
fold:0 epoch:339 step:8 train loss:0.009535, train acc:99.664, train f1:99.665, train precision:99.665, train recall:99.665, train auc:99.993
fold:0 epoch:339 step:9 train loss:0.012725, train acc:99.578, train f1:99.582, train precision:99.599, train recall:99.564, train auc:99.988
fold:0 epoch:339        valid loss:0.054572, valid acc:98.804, valid f1:98.806, valid precision:98.603, valid recall:99.010, valid auc:99.841
[1;31mEarlyStopping counter: 1 out of 50[0m
[98.80501501893693, 98.80758454421058, 98.59557867360208, 99.02050411388272, 99.8394003348033]
====================================================================================================
fold:0 epoch:340 step:0 train loss:0.010507, train acc:99.622, train f1:99.624, train precision:99.581, train recall:99.666, train auc:99.993
fold:0 epoch:340 step:1 train loss:0.008333, train acc:99.713, train f1:99.713, train precision:99.694, train recall:99.731, train auc:99.995
fold:0 epoch:340 step:2 train loss:0.010316, train acc:99.686, train f1:99.685, train precision:99.620, train recall:99.749, train auc:99.991
fold:0 epoch:340 step:3 train loss:0.009416, train acc:99.655, train f1:99.656, train precision:99.665, train recall:99.647, train auc:99.994
fold:0 epoch:340 step:4 train loss:0.009726, train acc:99.683, train f1:99.682, train precision:99.706, train recall:99.658, train auc:99.994
fold:0 epoch:340 step:5 train loss:0.010065, train acc:99.625, train f1:99.627, train precision:99.666, train recall:99.587, train auc:99.994
fold:0 epoch:340 step:6 train loss:0.010462, train acc:99.622, train f1:99.624, train precision:99.516, train recall:99.733, train auc:99.993
fold:0 epoch:340 step:7 train loss:0.010332, train acc:99.637, train f1:99.635, train precision:99.601, train recall:99.668, train auc:99.993
fold:0 epoch:340 step:8 train loss:0.009393, train acc:99.664, train f1:99.663, train precision:99.638, train recall:99.687, train auc:99.994
fold:0 epoch:340 step:9 train loss:0.010016, train acc:99.657, train f1:99.656, train precision:99.735, train recall:99.577, train auc:99.994
fold:0 epoch:340        valid loss:0.057903, valid acc:98.727, valid f1:98.730, valid precision:98.455, valid recall:99.007, valid auc:99.837
[1;31mEarlyStopping counter: 2 out of 50[0m
[98.80501501893693, 98.80758454421058, 98.59557867360208, 99.02050411388272, 99.8394003348033]
====================================================================================================
fold:0 epoch:341 step:0 train loss:0.011506, train acc:99.561, train f1:99.555, train precision:99.500, train recall:99.610, train auc:99.991
fold:0 epoch:341 step:1 train loss:0.009732, train acc:99.594, train f1:99.592, train precision:99.559, train recall:99.626, train auc:99.994
fold:0 epoch:341 step:2 train loss:0.010068, train acc:99.652, train f1:99.652, train precision:99.609, train recall:99.695, train auc:99.993
fold:0 epoch:341 step:3 train loss:0.010526, train acc:99.625, train f1:99.624, train precision:99.633, train recall:99.615, train auc:99.992
fold:0 epoch:341 step:4 train loss:0.011094, train acc:99.600, train f1:99.601, train precision:99.610, train recall:99.592, train auc:99.992
fold:0 epoch:341 step:5 train loss:0.008932, train acc:99.698, train f1:99.701, train precision:99.764, train recall:99.637, train auc:99.995
fold:0 epoch:341 step:6 train loss:0.009318, train acc:99.655, train f1:99.655, train precision:99.652, train recall:99.658, train auc:99.995
fold:0 epoch:341 step:7 train loss:0.011060, train acc:99.585, train f1:99.588, train precision:99.546, train recall:99.631, train auc:99.991
fold:0 epoch:341 step:8 train loss:0.010370, train acc:99.649, train f1:99.649, train precision:99.543, train recall:99.756, train auc:99.993
fold:0 epoch:341 step:9 train loss:0.010702, train acc:99.534, train f1:99.536, train precision:99.632, train recall:99.440, train auc:99.993
fold:0 epoch:341        valid loss:0.058348, valid acc:98.718, valid f1:98.722, valid precision:98.384, valid recall:99.062, valid auc:99.844
[1;31mEarlyStopping counter: 3 out of 50[0m
[98.80501501893693, 98.80758454421058, 98.59557867360208, 99.02050411388272, 99.8394003348033]
====================================================================================================
fold:0 epoch:342 step:0 train loss:0.008731, train acc:99.686, train f1:99.686, train precision:99.787, train recall:99.586, train auc:99.994
fold:0 epoch:342 step:1 train loss:0.010614, train acc:99.634, train f1:99.633, train precision:99.652, train recall:99.615, train auc:99.991
fold:0 epoch:342 step:2 train loss:0.010806, train acc:99.585, train f1:99.583, train precision:99.431, train recall:99.736, train auc:99.993
fold:0 epoch:342 step:3 train loss:0.010196, train acc:99.661, train f1:99.663, train precision:99.599, train recall:99.726, train auc:99.993
fold:0 epoch:342 step:4 train loss:0.009680, train acc:99.643, train f1:99.643, train precision:99.628, train recall:99.658, train auc:99.994
fold:0 epoch:342 step:5 train loss:0.009714, train acc:99.637, train f1:99.637, train precision:99.701, train recall:99.573, train auc:99.993
fold:0 epoch:342 step:6 train loss:0.010854, train acc:99.579, train f1:99.576, train precision:99.551, train recall:99.600, train auc:99.993
fold:0 epoch:342 step:7 train loss:0.011600, train acc:99.551, train f1:99.551, train precision:99.573, train recall:99.530, train auc:99.992
fold:0 epoch:342 step:8 train loss:0.010724, train acc:99.603, train f1:99.605, train precision:99.532, train recall:99.677, train auc:99.992
fold:0 epoch:342 step:9 train loss:0.009545, train acc:99.683, train f1:99.686, train precision:99.582, train recall:99.791, train auc:99.994
fold:0 epoch:342        valid loss:0.057544, valid acc:98.731, valid f1:98.733, valid precision:98.528, valid recall:98.940, valid auc:99.834
[1;31mEarlyStopping counter: 4 out of 50[0m
[98.80501501893693, 98.80758454421058, 98.59557867360208, 99.02050411388272, 99.8394003348033]
====================================================================================================
fold:0 epoch:343 step:0 train loss:0.009074, train acc:99.658, train f1:99.658, train precision:99.591, train recall:99.725, train auc:99.995
fold:0 epoch:343 step:1 train loss:0.010018, train acc:99.646, train f1:99.646, train precision:99.688, train recall:99.603, train auc:99.993
fold:0 epoch:343 step:2 train loss:0.009220, train acc:99.680, train f1:99.680, train precision:99.665, train recall:99.695, train auc:99.994
fold:0 epoch:343 step:3 train loss:0.010059, train acc:99.628, train f1:99.627, train precision:99.688, train recall:99.566, train auc:99.992
fold:0 epoch:343 step:4 train loss:0.009919, train acc:99.664, train f1:99.663, train precision:99.590, train recall:99.736, train auc:99.994
fold:0 epoch:343 step:5 train loss:0.011058, train acc:99.631, train f1:99.632, train precision:99.599, train recall:99.666, train auc:99.991
fold:0 epoch:343 step:6 train loss:0.011665, train acc:99.597, train f1:99.598, train precision:99.489, train recall:99.707, train auc:99.990
fold:0 epoch:343 step:7 train loss:0.010352, train acc:99.637, train f1:99.638, train precision:99.684, train recall:99.593, train auc:99.993
fold:0 epoch:343 step:8 train loss:0.009797, train acc:99.652, train f1:99.651, train precision:99.675, train recall:99.626, train auc:99.993
fold:0 epoch:343 step:9 train loss:0.009865, train acc:99.666, train f1:99.666, train precision:99.649, train recall:99.684, train auc:99.993
fold:0 epoch:343        valid loss:0.056002, valid acc:98.776, valid f1:98.779, valid precision:98.549, valid recall:99.010, valid auc:99.844
[1;31mEarlyStopping counter: 5 out of 50[0m
[98.80501501893693, 98.80758454421058, 98.59557867360208, 99.02050411388272, 99.8394003348033]
====================================================================================================
fold:0 epoch:344 step:0 train loss:0.008084, train acc:99.713, train f1:99.713, train precision:99.707, train recall:99.719, train auc:99.996
fold:0 epoch:344 step:1 train loss:0.009074, train acc:99.680, train f1:99.678, train precision:99.620, train recall:99.736, train auc:99.995
fold:0 epoch:344 step:2 train loss:0.010284, train acc:99.637, train f1:99.635, train precision:99.613, train recall:99.656, train auc:99.993
fold:0 epoch:344 step:3 train loss:0.010595, train acc:99.603, train f1:99.602, train precision:99.620, train recall:99.584, train auc:99.991
fold:0 epoch:344 step:4 train loss:0.008754, train acc:99.725, train f1:99.728, train precision:99.770, train recall:99.685, train auc:99.993
fold:0 epoch:344 step:5 train loss:0.009215, train acc:99.689, train f1:99.690, train precision:99.617, train recall:99.762, train auc:99.994
fold:0 epoch:344 step:6 train loss:0.010357, train acc:99.637, train f1:99.634, train precision:99.539, train recall:99.729, train auc:99.992
fold:0 epoch:344 step:7 train loss:0.009453, train acc:99.677, train f1:99.680, train precision:99.722, train recall:99.638, train auc:99.994
fold:0 epoch:344 step:8 train loss:0.010702, train acc:99.619, train f1:99.621, train precision:99.684, train recall:99.558, train auc:99.992
fold:0 epoch:344 step:9 train loss:0.009787, train acc:99.727, train f1:99.722, train precision:99.534, train recall:99.910, train auc:99.993
fold:0 epoch:344        valid loss:0.055145, valid acc:98.766, valid f1:98.768, valid precision:98.562, valid recall:98.976, valid auc:99.843
[1;31mEarlyStopping counter: 6 out of 50[0m
[98.80501501893693, 98.80758454421058, 98.59557867360208, 99.02050411388272, 99.8394003348033]
====================================================================================================
fold:0 epoch:345 step:0 train loss:0.009170, train acc:99.643, train f1:99.642, train precision:99.639, train recall:99.645, train auc:99.994
fold:0 epoch:345 step:1 train loss:0.008983, train acc:99.692, train f1:99.690, train precision:99.779, train recall:99.601, train auc:99.995
fold:0 epoch:345 step:2 train loss:0.010887, train acc:99.588, train f1:99.587, train precision:99.669, train recall:99.504, train auc:99.993
fold:0 epoch:345 step:3 train loss:0.007690, train acc:99.734, train f1:99.735, train precision:99.757, train recall:99.714, train auc:99.997
fold:0 epoch:345 step:4 train loss:0.009798, train acc:99.670, train f1:99.670, train precision:99.567, train recall:99.774, train auc:99.994
fold:0 epoch:345 step:5 train loss:0.008200, train acc:99.710, train f1:99.712, train precision:99.697, train recall:99.727, train auc:99.995
fold:0 epoch:345 step:6 train loss:0.010334, train acc:99.612, train f1:99.614, train precision:99.581, train recall:99.648, train auc:99.991
fold:0 epoch:345 step:7 train loss:0.010987, train acc:99.615, train f1:99.615, train precision:99.622, train recall:99.609, train auc:99.991
fold:0 epoch:345 step:8 train loss:0.010706, train acc:99.609, train f1:99.612, train precision:99.576, train recall:99.648, train auc:99.992
fold:0 epoch:345 step:9 train loss:0.011128, train acc:99.639, train f1:99.632, train precision:99.498, train recall:99.766, train auc:99.993
fold:0 epoch:345        valid loss:0.056885, valid acc:98.751, valid f1:98.755, valid precision:98.486, valid recall:99.026, valid auc:99.839
[1;31mEarlyStopping counter: 7 out of 50[0m
[98.80501501893693, 98.80758454421058, 98.59557867360208, 99.02050411388272, 99.8394003348033]
====================================================================================================
fold:0 epoch:346 step:0 train loss:0.009129, train acc:99.673, train f1:99.672, train precision:99.724, train recall:99.621, train auc:99.995
fold:0 epoch:346 step:1 train loss:0.010892, train acc:99.625, train f1:99.625, train precision:99.701, train recall:99.549, train auc:99.993
fold:0 epoch:346 step:2 train loss:0.009471, train acc:99.658, train f1:99.660, train precision:99.703, train recall:99.618, train auc:99.994
fold:0 epoch:346 step:3 train loss:0.010256, train acc:99.631, train f1:99.631, train precision:99.495, train recall:99.768, train auc:99.992
fold:0 epoch:346 step:4 train loss:0.010208, train acc:99.670, train f1:99.671, train precision:99.616, train recall:99.726, train auc:99.992
fold:0 epoch:346 step:5 train loss:0.008387, train acc:99.719, train f1:99.721, train precision:99.739, train recall:99.703, train auc:99.995
fold:0 epoch:346 step:6 train loss:0.009527, train acc:99.652, train f1:99.651, train precision:99.633, train recall:99.670, train auc:99.994
fold:0 epoch:346 step:7 train loss:0.011288, train acc:99.582, train f1:99.577, train precision:99.611, train recall:99.544, train auc:99.991
fold:0 epoch:346 step:8 train loss:0.010034, train acc:99.634, train f1:99.633, train precision:99.608, train recall:99.657, train auc:99.993
fold:0 epoch:346 step:9 train loss:0.009592, train acc:99.604, train f1:99.609, train precision:99.652, train recall:99.566, train auc:99.994
fold:0 epoch:346        valid loss:0.058261, valid acc:98.729, valid f1:98.734, valid precision:98.387, valid recall:99.083, valid auc:99.841
[1;31mEarlyStopping counter: 8 out of 50[0m
[98.80501501893693, 98.80758454421058, 98.59557867360208, 99.02050411388272, 99.8394003348033]
====================================================================================================
fold:0 epoch:347 step:0 train loss:0.010920, train acc:99.585, train f1:99.585, train precision:99.464, train recall:99.707, train auc:99.992
fold:0 epoch:347 step:1 train loss:0.010547, train acc:99.631, train f1:99.629, train precision:99.535, train recall:99.724, train auc:99.993
fold:0 epoch:347 step:2 train loss:0.009719, train acc:99.640, train f1:99.639, train precision:99.603, train recall:99.676, train auc:99.994
fold:0 epoch:347 step:3 train loss:0.009685, train acc:99.670, train f1:99.671, train precision:99.787, train recall:99.556, train auc:99.995
fold:0 epoch:347 step:4 train loss:0.009336, train acc:99.698, train f1:99.698, train precision:99.804, train recall:99.591, train auc:99.993
fold:0 epoch:347 step:5 train loss:0.009847, train acc:99.646, train f1:99.646, train precision:99.543, train recall:99.750, train auc:99.990
fold:0 epoch:347 step:6 train loss:0.010062, train acc:99.615, train f1:99.612, train precision:99.496, train recall:99.729, train auc:99.993
fold:0 epoch:347 step:7 train loss:0.009909, train acc:99.643, train f1:99.648, train precision:99.574, train recall:99.723, train auc:99.993
fold:0 epoch:347 step:8 train loss:0.010164, train acc:99.615, train f1:99.613, train precision:99.552, train recall:99.674, train auc:99.994
fold:0 epoch:347 step:9 train loss:0.009047, train acc:99.692, train f1:99.697, train precision:99.706, train recall:99.688, train auc:99.995
fold:0 epoch:347        valid loss:0.058314, valid acc:98.736, valid f1:98.740, valid precision:98.437, valid recall:99.044, valid auc:99.837
[1;31mEarlyStopping counter: 9 out of 50[0m
[98.80501501893693, 98.80758454421058, 98.59557867360208, 99.02050411388272, 99.8394003348033]
====================================================================================================
fold:0 epoch:348 step:0 train loss:0.009373, train acc:99.667, train f1:99.666, train precision:99.687, train recall:99.644, train auc:99.994
fold:0 epoch:348 step:1 train loss:0.009749, train acc:99.652, train f1:99.651, train precision:99.706, train recall:99.596, train auc:99.993
fold:0 epoch:348 step:2 train loss:0.009540, train acc:99.670, train f1:99.670, train precision:99.579, train recall:99.762, train auc:99.993
fold:0 epoch:348 step:3 train loss:0.010002, train acc:99.600, train f1:99.603, train precision:99.443, train recall:99.763, train auc:99.993
fold:0 epoch:348 step:4 train loss:0.008687, train acc:99.695, train f1:99.697, train precision:99.631, train recall:99.764, train auc:99.995
fold:0 epoch:348 step:5 train loss:0.010020, train acc:99.619, train f1:99.618, train precision:99.676, train recall:99.560, train auc:99.992
fold:0 epoch:348 step:6 train loss:0.010025, train acc:99.631, train f1:99.631, train precision:99.610, train recall:99.652, train auc:99.993
fold:0 epoch:348 step:7 train loss:0.010096, train acc:99.634, train f1:99.634, train precision:99.689, train recall:99.579, train auc:99.993
fold:0 epoch:348 step:8 train loss:0.009170, train acc:99.664, train f1:99.662, train precision:99.601, train recall:99.724, train auc:99.995
fold:0 epoch:348 step:9 train loss:0.010250, train acc:99.666, train f1:99.667, train precision:99.440, train recall:99.895, train auc:99.994
fold:0 epoch:348        valid loss:0.058328, valid acc:98.779, valid f1:98.781, valid precision:98.595, valid recall:98.968, valid auc:99.829
[1;31mEarlyStopping counter: 10 out of 50[0m
[98.80501501893693, 98.80758454421058, 98.59557867360208, 99.02050411388272, 99.8394003348033]
====================================================================================================
fold:0 epoch:349 step:0 train loss:0.007758, train acc:99.725, train f1:99.725, train precision:99.767, train recall:99.682, train auc:99.996
fold:0 epoch:349 step:1 train loss:0.010325, train acc:99.634, train f1:99.630, train precision:99.710, train recall:99.550, train auc:99.994
fold:0 epoch:349 step:2 train loss:0.010295, train acc:99.646, train f1:99.644, train precision:99.730, train recall:99.559, train auc:99.993
fold:0 epoch:349 step:3 train loss:0.009838, train acc:99.677, train f1:99.675, train precision:99.614, train recall:99.736, train auc:99.993
fold:0 epoch:349 step:4 train loss:0.009120, train acc:99.655, train f1:99.656, train precision:99.587, train recall:99.726, train auc:99.995
fold:0 epoch:349 step:5 train loss:0.009424, train acc:99.689, train f1:99.687, train precision:99.601, train recall:99.772, train auc:99.994
fold:0 epoch:349 step:6 train loss:0.010417, train acc:99.622, train f1:99.623, train precision:99.593, train recall:99.653, train auc:99.993
fold:0 epoch:349 step:7 train loss:0.010398, train acc:99.646, train f1:99.649, train precision:99.800, train recall:99.498, train auc:99.994
fold:0 epoch:349 step:8 train loss:0.009265, train acc:99.643, train f1:99.645, train precision:99.691, train recall:99.600, train auc:99.994
fold:0 epoch:349 step:9 train loss:0.010175, train acc:99.604, train f1:99.610, train precision:99.550, train recall:99.671, train auc:99.993
fold:0 epoch:349        valid loss:0.056178, valid acc:98.801, valid f1:98.802, valid precision:98.697, valid recall:98.908, valid auc:99.844
[1;31mEarlyStopping counter: 11 out of 50[0m
[98.80501501893693, 98.80758454421058, 98.59557867360208, 99.02050411388272, 99.8394003348033]
====================================================================================================
fold:0 epoch:350 step:0 train loss:0.010653, train acc:99.597, train f1:99.596, train precision:99.498, train recall:99.694, train auc:99.993
fold:0 epoch:350 step:1 train loss:0.010032, train acc:99.625, train f1:99.626, train precision:99.489, train recall:99.762, train auc:99.991
fold:0 epoch:350 step:2 train loss:0.009286, train acc:99.631, train f1:99.634, train precision:99.691, train recall:99.577, train auc:99.994
fold:0 epoch:350 step:3 train loss:0.009010, train acc:99.652, train f1:99.650, train precision:99.687, train recall:99.614, train auc:99.995
fold:0 epoch:350 step:4 train loss:0.008872, train acc:99.701, train f1:99.703, train precision:99.776, train recall:99.631, train auc:99.994
fold:0 epoch:350 step:5 train loss:0.009079, train acc:99.667, train f1:99.665, train precision:99.527, train recall:99.803, train auc:99.994
fold:0 epoch:350 step:6 train loss:0.009006, train acc:99.686, train f1:99.687, train precision:99.696, train recall:99.678, train auc:99.994
fold:0 epoch:350 step:7 train loss:0.008573, train acc:99.701, train f1:99.702, train precision:99.702, train recall:99.702, train auc:99.995
fold:0 epoch:350 step:8 train loss:0.010117, train acc:99.673, train f1:99.670, train precision:99.655, train recall:99.685, train auc:99.992
fold:0 epoch:350 step:9 train loss:0.008457, train acc:99.692, train f1:99.694, train precision:99.702, train recall:99.685, train auc:99.994
fold:0 epoch:350        valid loss:0.056552, valid acc:98.788, valid f1:98.790, valid precision:98.618, valid recall:98.963, valid auc:99.839
[1;31mEarlyStopping counter: 12 out of 50[0m
[98.80501501893693, 98.80758454421058, 98.59557867360208, 99.02050411388272, 99.8394003348033]
====================================================================================================
fold:0 epoch:351 step:0 train loss:0.010423, train acc:99.643, train f1:99.644, train precision:99.689, train recall:99.598, train auc:99.991
fold:0 epoch:351 step:1 train loss:0.008737, train acc:99.661, train f1:99.663, train precision:99.690, train recall:99.636, train auc:99.995
fold:0 epoch:351 step:2 train loss:0.010143, train acc:99.628, train f1:99.627, train precision:99.560, train recall:99.694, train auc:99.991
fold:0 epoch:351 step:3 train loss:0.009024, train acc:99.643, train f1:99.642, train precision:99.573, train recall:99.713, train auc:99.994
fold:0 epoch:351 step:4 train loss:0.009728, train acc:99.649, train f1:99.650, train precision:99.640, train recall:99.659, train auc:99.994
fold:0 epoch:351 step:5 train loss:0.009517, train acc:99.646, train f1:99.641, train precision:99.574, train recall:99.709, train auc:99.994
fold:0 epoch:351 step:6 train loss:0.009824, train acc:99.652, train f1:99.655, train precision:99.697, train recall:99.612, train auc:99.994
fold:0 epoch:351 step:7 train loss:0.010228, train acc:99.628, train f1:99.628, train precision:99.689, train recall:99.568, train auc:99.993
fold:0 epoch:351 step:8 train loss:0.009124, train acc:99.652, train f1:99.653, train precision:99.586, train recall:99.720, train auc:99.994
fold:0 epoch:351 step:9 train loss:0.011168, train acc:99.604, train f1:99.601, train precision:99.451, train recall:99.751, train auc:99.994
fold:0 epoch:351        valid loss:0.057151, valid acc:98.767, valid f1:98.770, valid precision:98.529, valid recall:99.013, valid auc:99.841
[1;31mEarlyStopping counter: 13 out of 50[0m
[98.80501501893693, 98.80758454421058, 98.59557867360208, 99.02050411388272, 99.8394003348033]
====================================================================================================
fold:0 epoch:352 step:0 train loss:0.009183, train acc:99.677, train f1:99.676, train precision:99.627, train recall:99.724, train auc:99.994
fold:0 epoch:352 step:1 train loss:0.009476, train acc:99.664, train f1:99.661, train precision:99.778, train recall:99.545, train auc:99.994
fold:0 epoch:352 step:2 train loss:0.008734, train acc:99.683, train f1:99.683, train precision:99.732, train recall:99.635, train auc:99.995
fold:0 epoch:352 step:3 train loss:0.010062, train acc:99.628, train f1:99.627, train precision:99.572, train recall:99.682, train auc:99.993
fold:0 epoch:352 step:4 train loss:0.010116, train acc:99.619, train f1:99.619, train precision:99.603, train recall:99.634, train auc:99.993
fold:0 epoch:352 step:5 train loss:0.009847, train acc:99.649, train f1:99.649, train precision:99.597, train recall:99.701, train auc:99.994
fold:0 epoch:352 step:6 train loss:0.009103, train acc:99.698, train f1:99.697, train precision:99.694, train recall:99.701, train auc:99.994
fold:0 epoch:352 step:7 train loss:0.009457, train acc:99.649, train f1:99.650, train precision:99.713, train recall:99.586, train auc:99.994
fold:0 epoch:352 step:8 train loss:0.010260, train acc:99.667, train f1:99.670, train precision:99.679, train recall:99.661, train auc:99.993
fold:0 epoch:352 step:9 train loss:0.008751, train acc:99.657, train f1:99.659, train precision:99.546, train recall:99.772, train auc:99.995
fold:0 epoch:352        valid loss:0.059995, valid acc:98.766, valid f1:98.770, valid precision:98.453, valid recall:99.088, valid auc:99.837
[1;31mEarlyStopping counter: 14 out of 50[0m
[98.80501501893693, 98.80758454421058, 98.59557867360208, 99.02050411388272, 99.8394003348033]
====================================================================================================
fold:0 epoch:353 step:0 train loss:0.009735, train acc:99.661, train f1:99.663, train precision:99.563, train recall:99.763, train auc:99.994
fold:0 epoch:353 step:1 train loss:0.009180, train acc:99.698, train f1:99.697, train precision:99.676, train recall:99.719, train auc:99.994
fold:0 epoch:353 step:2 train loss:0.010499, train acc:99.579, train f1:99.579, train precision:99.573, train recall:99.585, train auc:99.993
fold:0 epoch:353 step:3 train loss:0.009270, train acc:99.634, train f1:99.634, train precision:99.762, train recall:99.507, train auc:99.995
fold:0 epoch:353 step:4 train loss:0.009870, train acc:99.646, train f1:99.646, train precision:99.652, train recall:99.640, train auc:99.994
fold:0 epoch:353 step:5 train loss:0.010029, train acc:99.628, train f1:99.625, train precision:99.564, train recall:99.687, train auc:99.993
fold:0 epoch:353 step:6 train loss:0.009295, train acc:99.664, train f1:99.668, train precision:99.644, train recall:99.692, train auc:99.994
fold:0 epoch:353 step:7 train loss:0.010202, train acc:99.652, train f1:99.650, train precision:99.613, train recall:99.686, train auc:99.993
fold:0 epoch:353 step:8 train loss:0.010572, train acc:99.612, train f1:99.613, train precision:99.574, train recall:99.653, train auc:99.992
fold:0 epoch:353 step:9 train loss:0.009641, train acc:99.587, train f1:99.582, train precision:99.520, train recall:99.644, train auc:99.995
fold:0 epoch:353        valid loss:0.056572, valid acc:98.759, valid f1:98.763, valid precision:98.458, valid recall:99.070, valid auc:99.844
[1;31mEarlyStopping counter: 15 out of 50[0m
[98.80501501893693, 98.80758454421058, 98.59557867360208, 99.02050411388272, 99.8394003348033]
====================================================================================================
fold:0 epoch:354 step:0 train loss:0.008767, train acc:99.707, train f1:99.708, train precision:99.739, train recall:99.678, train auc:99.994
fold:0 epoch:354 step:1 train loss:0.009207, train acc:99.646, train f1:99.646, train precision:99.689, train recall:99.604, train auc:99.994
fold:0 epoch:354 step:2 train loss:0.008011, train acc:99.719, train f1:99.716, train precision:99.649, train recall:99.784, train auc:99.996
fold:0 epoch:354 step:3 train loss:0.009242, train acc:99.655, train f1:99.656, train precision:99.629, train recall:99.683, train auc:99.995
fold:0 epoch:354 step:4 train loss:0.009109, train acc:99.689, train f1:99.689, train precision:99.634, train recall:99.744, train auc:99.994
fold:0 epoch:354 step:5 train loss:0.010217, train acc:99.615, train f1:99.614, train precision:99.602, train recall:99.626, train auc:99.992
fold:0 epoch:354 step:6 train loss:0.008794, train acc:99.661, train f1:99.659, train precision:99.674, train recall:99.644, train auc:99.995
fold:0 epoch:354 step:7 train loss:0.009466, train acc:99.658, train f1:99.661, train precision:99.637, train recall:99.685, train auc:99.994
fold:0 epoch:354 step:8 train loss:0.008796, train acc:99.716, train f1:99.719, train precision:99.776, train recall:99.661, train auc:99.995
fold:0 epoch:354 step:9 train loss:0.008887, train acc:99.710, train f1:99.707, train precision:99.698, train recall:99.716, train auc:99.994
fold:0 epoch:354        valid loss:0.056640, valid acc:98.775, valid f1:98.779, valid precision:98.484, valid recall:99.075, valid auc:99.845
[1;31mEarlyStopping counter: 16 out of 50[0m
[98.80501501893693, 98.80758454421058, 98.59557867360208, 99.02050411388272, 99.8394003348033]
====================================================================================================
fold:0 epoch:355 step:0 train loss:0.008338, train acc:99.710, train f1:99.712, train precision:99.648, train recall:99.775, train auc:99.996
fold:0 epoch:355 step:1 train loss:0.008499, train acc:99.701, train f1:99.701, train precision:99.707, train recall:99.695, train auc:99.995
fold:0 epoch:355 step:2 train loss:0.009741, train acc:99.670, train f1:99.672, train precision:99.600, train recall:99.745, train auc:99.993
fold:0 epoch:355 step:3 train loss:0.008071, train acc:99.704, train f1:99.702, train precision:99.650, train recall:99.754, train auc:99.996
fold:0 epoch:355 step:4 train loss:0.009063, train acc:99.713, train f1:99.714, train precision:99.708, train recall:99.720, train auc:99.992
fold:0 epoch:355 step:5 train loss:0.009693, train acc:99.670, train f1:99.669, train precision:99.682, train recall:99.657, train auc:99.993
fold:0 epoch:355 step:6 train loss:0.009505, train acc:99.667, train f1:99.669, train precision:99.726, train recall:99.611, train auc:99.994
fold:0 epoch:355 step:7 train loss:0.009218, train acc:99.670, train f1:99.670, train precision:99.633, train recall:99.707, train auc:99.994
fold:0 epoch:355 step:8 train loss:0.010335, train acc:99.643, train f1:99.639, train precision:99.556, train recall:99.722, train auc:99.993
fold:0 epoch:355 step:9 train loss:0.010688, train acc:99.666, train f1:99.672, train precision:99.810, train recall:99.534, train auc:99.991
fold:0 epoch:355        valid loss:0.055745, valid acc:98.788, valid f1:98.791, valid precision:98.547, valid recall:99.036, valid auc:99.846
[1;31mEarlyStopping counter: 17 out of 50[0m
[98.80501501893693, 98.80758454421058, 98.59557867360208, 99.02050411388272, 99.8394003348033]
====================================================================================================
fold:0 epoch:356 step:0 train loss:0.008893, train acc:99.680, train f1:99.679, train precision:99.633, train recall:99.725, train auc:99.995
fold:0 epoch:356 step:1 train loss:0.008758, train acc:99.686, train f1:99.685, train precision:99.694, train recall:99.676, train auc:99.995
fold:0 epoch:356 step:2 train loss:0.008423, train acc:99.680, train f1:99.681, train precision:99.678, train recall:99.684, train auc:99.995
fold:0 epoch:356 step:3 train loss:0.009461, train acc:99.664, train f1:99.665, train precision:99.635, train recall:99.695, train auc:99.994
fold:0 epoch:356 step:4 train loss:0.007647, train acc:99.728, train f1:99.730, train precision:99.715, train recall:99.745, train auc:99.996
fold:0 epoch:356 step:5 train loss:0.008280, train acc:99.673, train f1:99.673, train precision:99.609, train recall:99.737, train auc:99.996
fold:0 epoch:356 step:6 train loss:0.008884, train acc:99.661, train f1:99.663, train precision:99.606, train recall:99.721, train auc:99.994
fold:0 epoch:356 step:7 train loss:0.008784, train acc:99.670, train f1:99.671, train precision:99.695, train recall:99.647, train auc:99.995
fold:0 epoch:356 step:8 train loss:0.009678, train acc:99.664, train f1:99.660, train precision:99.630, train recall:99.691, train auc:99.993
fold:0 epoch:356 step:9 train loss:0.008376, train acc:99.701, train f1:99.700, train precision:99.683, train recall:99.718, train auc:99.995
fold:0 epoch:356        valid loss:0.055942, valid acc:98.774, valid f1:98.777, valid precision:98.539, valid recall:99.015, valid auc:99.850
[1;31mEarlyStopping counter: 18 out of 50[0m
[98.80501501893693, 98.80758454421058, 98.59557867360208, 99.02050411388272, 99.8394003348033]
====================================================================================================
fold:0 epoch:357 step:0 train loss:0.008180, train acc:99.698, train f1:99.697, train precision:99.688, train recall:99.706, train auc:99.996
fold:0 epoch:357 step:1 train loss:0.008478, train acc:99.680, train f1:99.679, train precision:99.713, train recall:99.646, train auc:99.996
fold:0 epoch:357 step:2 train loss:0.008891, train acc:99.695, train f1:99.694, train precision:99.645, train recall:99.742, train auc:99.995
fold:0 epoch:357 step:3 train loss:0.009572, train acc:99.649, train f1:99.649, train precision:99.500, train recall:99.798, train auc:99.993
fold:0 epoch:357 step:4 train loss:0.010138, train acc:99.661, train f1:99.664, train precision:99.697, train recall:99.631, train auc:99.993

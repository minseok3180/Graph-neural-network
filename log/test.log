nohup: ignoring input
running on cuda:0
{'compounds': [661], 'weights': [1.0]}
HetDDI(
  (kg): HGNN(
    (dropout): Dropout(p=0.2, inplace=False)
    (node_embedding): Embedding(98744, 300)
    (gat_layers): ModuleList(
      (0): HetConv(
        (edge_embedding): Embedding(109, 300)
        (bn): Sequential(
          (0): Linear(in_features=300, out_features=300, bias=True)
          (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (leaky_relu): LeakyReLU(negative_slope=0.02)
      )
      (1): HetConv(
        (edge_embedding): Embedding(109, 300)
        (bn): Sequential(
          (0): Linear(in_features=300, out_features=300, bias=True)
          (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (leaky_relu): LeakyReLU(negative_slope=0.02)
      )
      (2): HetConv(
        (edge_embedding): Embedding(109, 300)
        (bn): Sequential(
          (0): Linear(in_features=300, out_features=300, bias=True)
          (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (leaky_relu): LeakyReLU(negative_slope=0.02)
      )
    )
  )
  (kg_fc): Sequential(
    (0): Linear(in_features=300, out_features=300, bias=True)
    (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Dropout(p=0.1, inplace=False)
    (3): ReLU()
    (4): Linear(in_features=300, out_features=300, bias=True)
    (5): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Dropout(p=0.1, inplace=False)
    (7): ReLU()
    (8): Linear(in_features=300, out_features=300, bias=True)
    (9): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Dropout(p=0.1, inplace=False)
    (11): ReLU()
  )
  (mol): Mol(
    (readout): AvgPooling()
    (gnn): HGNN(
      (dropout): Dropout(p=0.2, inplace=False)
      (node_embedding): Embedding(47601, 300)
      (gat_layers): ModuleList(
        (0): HetConv(
          (edge_embedding): Embedding(5, 300)
          (bn): Sequential(
            (0): Linear(in_features=300, out_features=300, bias=True)
            (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (leaky_relu): LeakyReLU(negative_slope=0.02)
        )
        (1): HetConv(
          (edge_embedding): Embedding(5, 300)
          (bn): Sequential(
            (0): Linear(in_features=300, out_features=300, bias=True)
            (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (leaky_relu): LeakyReLU(negative_slope=0.02)
        )
        (2): HetConv(
          (edge_embedding): Embedding(5, 300)
          (bn): Sequential(
            (0): Linear(in_features=300, out_features=300, bias=True)
            (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (leaky_relu): LeakyReLU(negative_slope=0.02)
        )
      )
    )
  )
  (mol_fc): Sequential(
    (0): Linear(in_features=300, out_features=300, bias=True)
    (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Dropout(p=0.1, inplace=False)
    (3): ReLU()
    (4): Linear(in_features=300, out_features=300, bias=True)
    (5): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Dropout(p=0.1, inplace=False)
    (7): ReLU()
    (8): Linear(in_features=300, out_features=300, bias=True)
    (9): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Dropout(p=0.1, inplace=False)
    (11): ReLU()
  )
  (decoder): Mlp(
    (fc_layer): Sequential(
      (0): Linear(in_features=1200, out_features=1200, bias=True)
      (1): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.5, inplace=False)
      (4): Linear(in_features=1200, out_features=1200, bias=True)
      (5): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): ReLU()
      (7): Dropout(p=0.5, inplace=False)
      (8): Linear(in_features=1200, out_features=1200, bias=True)
      (9): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): ReLU()
      (11): Dropout(p=0.5, inplace=False)
    )
    (output_layer): Sequential(
      (0): Linear(in_features=1200, out_features=86, bias=False)
    )
  )
)
self.nodes_fc[0]: tensor([ 0.0613, -0.1034], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0755,  0.0337], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1339, -0.0291], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1192, 0.0285], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0282, -0.0841], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1173, -0.1139], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:0 step:0 train loss:4.547227, train acc:1.819, train f1:0.891, train precision:1.156, train recall:1.209, train kappa:0.091
self.nodes_fc[0]: tensor([ 0.0623, -0.1024], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0745,  0.0347], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1329, -0.0301], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1182, 0.0295], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0292, -0.0851], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1183, -0.1149], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:0 step:1 train loss:4.248007, train acc:11.139, train f1:1.318, train precision:1.744, train recall:1.954, train kappa:2.692
self.nodes_fc[0]: tensor([ 0.0616, -0.1030], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0739,  0.0354], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1321, -0.0304], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1189, 0.0304], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0285, -0.0846], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1182, -0.1159], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:0 step:2 train loss:4.023354, train acc:14.066, train f1:1.705, train precision:1.999, train recall:2.459, train kappa:4.622
self.nodes_fc[0]: tensor([ 0.0615, -0.1038], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0736,  0.0356], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1317, -0.0310], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1191, 0.0314], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0281, -0.0841], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1177, -0.1157], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:0 step:3 train loss:3.910527, train acc:15.689, train f1:2.072, train precision:3.683, train recall:2.954, train kappa:5.999
self.nodes_fc[0]: tensor([ 0.0617, -0.1037], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0733,  0.0358], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1315, -0.0315], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1193, 0.0318], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0277, -0.0839], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1173, -0.1153], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:0 step:4 train loss:3.856748, train acc:16.895, train f1:2.963, train precision:5.206, train recall:3.606, train kappa:6.819
self.nodes_fc[0]: tensor([ 0.0617, -0.1036], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0727,  0.0359], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1315, -0.0319], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1196, 0.0323], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0273, -0.0838], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1170, -0.1150], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:0 step:5 train loss:3.819660, train acc:16.470, train f1:3.463, train precision:5.188, train recall:3.954, train kappa:7.386
self.nodes_fc[0]: tensor([ 0.0617, -0.1038], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0724,  0.0360], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1313, -0.0320], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1201, 0.0322], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0272, -0.0837], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1171, -0.1146], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:0 step:6 train loss:3.730114, train acc:18.039, train f1:4.401, train precision:7.216, train recall:4.781, train kappa:8.750
self.nodes_fc[0]: tensor([ 0.0617, -0.1038], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0723,  0.0358], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1311, -0.0319], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1205, 0.0322], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0272, -0.0836], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1173, -0.1149], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:0 step:7 train loss:3.689654, train acc:18.939, train f1:4.755, train precision:8.337, train recall:5.122, train kappa:9.905
self.nodes_fc[0]: tensor([ 0.0617, -0.1040], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0726,  0.0358], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1309, -0.0318], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1209, 0.0321], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0271, -0.0835], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1178, -0.1154], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:0 step:8 train loss:3.613348, train acc:20.181, train f1:5.228, train precision:10.686, train recall:5.601, train kappa:11.570
self.nodes_fc[0]: tensor([ 0.0616, -0.1037], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0731,  0.0358], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1312, -0.0315], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1212, 0.0319], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0270, -0.0832], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1181, -0.1156], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:0 step:9 train loss:3.552037, train acc:20.709, train f1:5.830, train precision:9.745, train recall:6.049, train kappa:12.422
self.nodes_fc[0]: tensor([ 0.0615, -0.1033], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0737,  0.0362], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1313, -0.0311], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1214, 0.0317], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0270, -0.0828], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1181, -0.1158], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:0 step:10 train loss:3.495945, train acc:21.512, train f1:6.470, train precision:12.920, train recall:6.823, train kappa:13.193
self.nodes_fc[0]: tensor([ 0.0612, -0.1028], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0744,  0.0362], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1314, -0.0307], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1216, 0.0314], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0269, -0.0825], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1180, -0.1161], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:0 step:11 train loss:3.436985, train acc:21.909, train f1:7.522, train precision:12.988, train recall:7.832, train kappa:14.116
self.nodes_fc[0]: tensor([ 0.0607, -0.1023], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0751,  0.0359], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1311, -0.0306], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1217, 0.0311], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0268, -0.0822], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1180, -0.1163], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0607, -0.1023], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0751,  0.0359], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1311, -0.0306], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1217, 0.0311], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0268, -0.0822], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1180, -0.1163], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:0        valid loss:3.349530, valid acc:27.505, valid f1:2.243, valid precision:3.023, valid recall:3.009, valid kappa:11.634
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0607, -0.1023], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0751,  0.0359], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1311, -0.0306], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1217, 0.0311], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0268, -0.0822], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1180, -0.1163], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:1 step:0 train loss:3.336725, train acc:24.393, train f1:8.872, train precision:14.541, train recall:9.128, train kappa:16.857
self.nodes_fc[0]: tensor([ 0.0601, -0.1020], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0759,  0.0355], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1308, -0.0305], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1218, 0.0312], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0268, -0.0819], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1181, -0.1165], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:1 step:1 train loss:3.277366, train acc:25.000, train f1:9.845, train precision:15.982, train recall:10.176, train kappa:17.641
self.nodes_fc[0]: tensor([ 0.0596, -0.1015], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0768,  0.0353], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1304, -0.0306], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1218, 0.0313], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0268, -0.0819], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1182, -0.1167], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:1 step:2 train loss:3.198270, train acc:26.715, train f1:11.502, train precision:15.051, train recall:11.856, train kappa:19.632
self.nodes_fc[0]: tensor([ 0.0594, -0.1009], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0777,  0.0350], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1301, -0.0305], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1220, 0.0315], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0269, -0.0817], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1184, -0.1170], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:1 step:3 train loss:3.139478, train acc:27.557, train f1:11.516, train precision:17.797, train recall:12.311, train kappa:20.511
self.nodes_fc[0]: tensor([ 0.0595, -0.1003], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0786,  0.0347], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1299, -0.0305], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1221, 0.0314], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0269, -0.0816], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1186, -0.1173], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:1 step:4 train loss:3.109574, train acc:27.206, train f1:11.442, train precision:17.292, train recall:12.719, train kappa:20.317
self.nodes_fc[0]: tensor([ 0.0595, -0.0997], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0795,  0.0344], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1295, -0.0304], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1221, 0.0313], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0269, -0.0814], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1188, -0.1176], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:1 step:5 train loss:3.022076, train acc:28.525, train f1:13.144, train precision:21.454, train recall:13.875, train kappa:21.765
self.nodes_fc[0]: tensor([ 0.0593, -0.0993], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0804,  0.0342], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1292, -0.0304], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1223, 0.0312], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0268, -0.0810], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1189, -0.1181], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:1 step:6 train loss:2.965087, train acc:28.699, train f1:13.618, train precision:20.371, train recall:14.414, train kappa:22.175
self.nodes_fc[0]: tensor([ 0.0592, -0.0989], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0814,  0.0340], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1289, -0.0306], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1224, 0.0311], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0268, -0.0806], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1190, -0.1186], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:1 step:7 train loss:2.917075, train acc:29.654, train f1:15.222, train precision:22.688, train recall:15.399, train kappa:23.381
self.nodes_fc[0]: tensor([ 0.0591, -0.0983], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0824,  0.0337], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1286, -0.0307], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1226, 0.0310], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0268, -0.0804], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1192, -0.1190], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:1 step:8 train loss:2.844325, train acc:30.630, train f1:16.807, train precision:27.105, train recall:16.498, train kappa:24.557
self.nodes_fc[0]: tensor([ 0.0592, -0.0979], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0834,  0.0334], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1283, -0.0309], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1227, 0.0310], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0268, -0.0799], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1195, -0.1193], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:1 step:9 train loss:2.790247, train acc:31.995, train f1:18.216, train precision:26.297, train recall:17.705, train kappa:26.199
self.nodes_fc[0]: tensor([ 0.0593, -0.0978], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0844,  0.0333], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1282, -0.0312], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1230, 0.0309], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0268, -0.0794], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1198, -0.1196], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:1 step:10 train loss:2.742069, train acc:32.684, train f1:19.997, train precision:30.728, train recall:19.194, train kappa:27.227
self.nodes_fc[0]: tensor([ 0.0594, -0.0980], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0855,  0.0333], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1279, -0.0315], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1231, 0.0308], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0269, -0.0789], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1201, -0.1200], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:1 step:11 train loss:2.708160, train acc:32.806, train f1:20.965, train precision:31.967, train recall:20.596, train kappa:27.533
self.nodes_fc[0]: tensor([ 0.0595, -0.0983], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0865,  0.0334], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1275, -0.0318], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1233, 0.0307], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0271, -0.0783], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1206, -0.1204], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0595, -0.0983], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0865,  0.0334], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1275, -0.0318], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1233, 0.0307], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0271, -0.0783], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1206, -0.1204], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:1        valid loss:3.960855, valid acc:17.531, valid f1:2.103, valid precision:3.516, valid recall:3.156, valid kappa:5.778
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0595, -0.0983], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0865,  0.0334], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1275, -0.0318], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1233, 0.0307], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0271, -0.0783], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1206, -0.1204], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:2 step:0 train loss:2.627015, train acc:34.439, train f1:22.634, train precision:32.344, train recall:21.732, train kappa:29.383
self.nodes_fc[0]: tensor([ 0.0596, -0.0985], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0876,  0.0336], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1270, -0.0323], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1234, 0.0307], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0272, -0.0778], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1208, -0.1208], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:2 step:1 train loss:2.611991, train acc:34.891, train f1:24.061, train precision:31.662, train recall:23.287, train kappa:30.192
self.nodes_fc[0]: tensor([ 0.0596, -0.0986], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0886,  0.0337], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1264, -0.0328], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1236, 0.0308], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0274, -0.0774], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1208, -0.1212], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:2 step:2 train loss:2.518286, train acc:36.697, train f1:26.335, train precision:37.589, train recall:25.789, train kappa:32.305
self.nodes_fc[0]: tensor([ 0.0594, -0.0986], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0895,  0.0337], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1258, -0.0331], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1238, 0.0309], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0276, -0.0770], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1208, -0.1216], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:2 step:3 train loss:2.473011, train acc:38.080, train f1:27.564, train precision:33.561, train recall:27.137, train kappa:33.900
self.nodes_fc[0]: tensor([ 0.0591, -0.0984], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0905,  0.0335], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1254, -0.0334], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1238, 0.0309], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0277, -0.0765], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1208, -0.1219], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:2 step:4 train loss:2.419837, train acc:39.117, train f1:29.465, train precision:35.788, train recall:29.019, train kappa:35.178
self.nodes_fc[0]: tensor([ 0.0586, -0.0981], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0914,  0.0333], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1252, -0.0337], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1239, 0.0309], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0277, -0.0760], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1207, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:2 step:5 train loss:2.413337, train acc:38.840, train f1:29.393, train precision:37.631, train recall:28.859, train kappa:34.975
self.nodes_fc[0]: tensor([ 0.0583, -0.0977], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0923,  0.0330], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1249, -0.0339], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1239, 0.0310], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0279, -0.0756], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1207, -0.1225], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:2 step:6 train loss:2.378268, train acc:39.627, train f1:30.741, train precision:38.258, train recall:29.905, train kappa:35.876
self.nodes_fc[0]: tensor([ 0.0579, -0.0972], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0932,  0.0329], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1246, -0.0340], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1238, 0.0311], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0280, -0.0752], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1205, -0.1228], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:2 step:7 train loss:2.314687, train acc:40.750, train f1:32.290, train precision:38.684, train recall:31.326, train kappa:37.175
self.nodes_fc[0]: tensor([ 0.0577, -0.0968], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0942,  0.0330], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1244, -0.0340], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1237, 0.0312], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0282, -0.0750], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1202, -0.1231], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:2 step:8 train loss:2.282286, train acc:41.245, train f1:33.521, train precision:40.786, train recall:32.704, train kappa:37.791
self.nodes_fc[0]: tensor([ 0.0575, -0.0967], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0951,  0.0333], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1243, -0.0340], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1235, 0.0311], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0284, -0.0750], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1200, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:2 step:9 train loss:2.251963, train acc:42.221, train f1:34.660, train precision:42.355, train recall:33.997, train kappa:38.883
self.nodes_fc[0]: tensor([ 0.0574, -0.0966], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0959,  0.0334], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1241, -0.0340], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1232, 0.0310], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0286, -0.0749], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1195, -0.1237], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:2 step:10 train loss:2.191212, train acc:43.402, train f1:36.252, train precision:44.302, train recall:35.677, train kappa:40.160
self.nodes_fc[0]: tensor([ 0.0573, -0.0965], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0966,  0.0335], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1240, -0.0341], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1228, 0.0311], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0288, -0.0747], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1191, -0.1240], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:2 step:11 train loss:2.137023, train acc:44.677, train f1:36.953, train precision:43.267, train recall:36.393, train kappa:41.604
self.nodes_fc[0]: tensor([ 0.0570, -0.0964], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0974,  0.0336], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1239, -0.0341], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1226, 0.0311], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0290, -0.0745], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1188, -0.1243], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0570, -0.0964], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0974,  0.0336], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1239, -0.0341], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1226, 0.0311], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0290, -0.0745], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1188, -0.1243], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:2        valid loss:3.569792, valid acc:15.380, valid f1:6.876, valid precision:11.316, valid recall:11.102, valid kappa:6.946
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0570, -0.0964], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0974,  0.0336], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1239, -0.0341], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1226, 0.0311], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0290, -0.0745], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1188, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:3 step:0 train loss:2.112752, train acc:45.303, train f1:38.380, train precision:44.508, train recall:37.810, train kappa:42.324
self.nodes_fc[0]: tensor([ 0.0571, -0.0967], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0981,  0.0337], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1239, -0.0340], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1222, 0.0311], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0292, -0.0742], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1184, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:3 step:1 train loss:2.083754, train acc:46.039, train f1:39.833, train precision:45.140, train recall:39.325, train kappa:43.159
self.nodes_fc[0]: tensor([ 0.0571, -0.0971], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0988,  0.0336], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1239, -0.0339], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1219, 0.0310], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0293, -0.0739], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1179, -0.1250], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:3 step:2 train loss:2.042493, train acc:46.829, train f1:40.710, train precision:47.921, train recall:39.794, train kappa:43.963
self.nodes_fc[0]: tensor([ 0.0569, -0.0976], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.0996,  0.0335], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1239, -0.0337], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1217, 0.0308], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0293, -0.0736], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1174, -0.1252], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:3 step:3 train loss:2.009937, train acc:47.733, train f1:42.229, train precision:48.798, train recall:41.369, train kappa:44.956
self.nodes_fc[0]: tensor([ 0.0566, -0.0981], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1003,  0.0335], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1241, -0.0336], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1215, 0.0306], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0293, -0.0732], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1170, -0.1255], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:3 step:4 train loss:1.967625, train acc:48.752, train f1:43.228, train precision:50.440, train recall:42.395, train kappa:46.025
self.nodes_fc[0]: tensor([ 0.0562, -0.0985], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1010,  0.0335], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1243, -0.0335], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1212, 0.0304], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0294, -0.0729], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1167, -0.1257], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:3 step:5 train loss:1.939884, train acc:49.796, train f1:44.190, train precision:50.463, train recall:43.254, train kappa:47.146
self.nodes_fc[0]: tensor([ 0.0559, -0.0987], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1017,  0.0335], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1244, -0.0335], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1209, 0.0302], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0296, -0.0727], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1162, -0.1260], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:3 step:6 train loss:1.909958, train acc:50.204, train f1:45.004, train precision:52.325, train recall:43.917, train kappa:47.630
self.nodes_fc[0]: tensor([ 0.0558, -0.0990], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1023,  0.0336], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1244, -0.0335], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1206, 0.0301], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0298, -0.0724], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1157, -0.1262], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:3 step:7 train loss:1.892111, train acc:50.394, train f1:45.539, train precision:53.114, train recall:44.632, train kappa:47.862
self.nodes_fc[0]: tensor([ 0.0556, -0.0993], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1028,  0.0336], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1243, -0.0334], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1204, 0.0300], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0300, -0.0720], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1153, -0.1264], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:3 step:8 train loss:1.854349, train acc:51.498, train f1:46.738, train precision:52.223, train recall:45.664, train kappa:49.031
self.nodes_fc[0]: tensor([ 0.0555, -0.0995], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1034,  0.0337], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1242, -0.0334], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1200, 0.0299], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0302, -0.0717], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1149, -0.1265], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:3 step:9 train loss:1.825811, train acc:52.097, train f1:47.744, train precision:52.815, train recall:46.771, train kappa:49.667
self.nodes_fc[0]: tensor([ 0.0551, -0.0997], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1039,  0.0339], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1242, -0.0332], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1197, 0.0299], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0302, -0.0713], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1144, -0.1264], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:3 step:10 train loss:1.786218, train acc:53.491, train f1:50.168, train precision:55.315, train recall:49.213, train kappa:51.246
self.nodes_fc[0]: tensor([ 0.0549, -0.0997], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1044,  0.0341], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1240, -0.0331], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1193, 0.0300], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0302, -0.0709], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1141, -0.1264], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:3 step:11 train loss:1.753427, train acc:54.319, train f1:50.087, train precision:54.909, train recall:48.719, train kappa:52.059
self.nodes_fc[0]: tensor([ 0.0548, -0.0994], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1051,  0.0343], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1238, -0.0329], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1190, 0.0301], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0302, -0.0705], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1137, -0.1263], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0548, -0.0994], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1051,  0.0343], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1238, -0.0329], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1190, 0.0301], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0302, -0.0705], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1137, -0.1263], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:3        valid loss:2.957134, valid acc:23.489, valid f1:17.110, valid precision:24.476, valid recall:25.906, valid kappa:18.066
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0548, -0.0994], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1051,  0.0343], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1238, -0.0329], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1190, 0.0301], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0302, -0.0705], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1137, -0.1263], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:4 step:0 train loss:1.757852, train acc:54.337, train f1:50.923, train precision:55.970, train recall:50.146, train kappa:52.153
self.nodes_fc[0]: tensor([ 0.0546, -0.0992], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1058,  0.0346], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1235, -0.0328], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1189, 0.0302], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0302, -0.0703], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1135, -0.1263], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:4 step:1 train loss:1.722814, train acc:54.974, train f1:51.379, train precision:56.433, train recall:50.309, train kappa:52.770
self.nodes_fc[0]: tensor([ 0.0543, -0.0989], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1064,  0.0348], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1233, -0.0328], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1190, 0.0304], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0302, -0.0701], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1133, -0.1263], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:4 step:2 train loss:1.686479, train acc:55.948, train f1:51.983, train precision:57.610, train recall:50.630, train kappa:53.773
self.nodes_fc[0]: tensor([ 0.0540, -0.0985], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1071,  0.0349], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1230, -0.0328], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1191, 0.0307], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0301, -0.0698], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1130, -0.1263], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:4 step:3 train loss:1.686916, train acc:55.823, train f1:52.024, train precision:56.613, train recall:51.153, train kappa:53.659
self.nodes_fc[0]: tensor([ 0.0535, -0.0982], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1077,  0.0351], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1228, -0.0328], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1192, 0.0309], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0301, -0.0694], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1128, -0.1263], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:4 step:4 train loss:1.656716, train acc:56.470, train f1:53.322, train precision:59.365, train recall:52.047, train kappa:54.371
self.nodes_fc[0]: tensor([ 0.0530, -0.0980], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1082,  0.0353], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1225, -0.0328], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1193, 0.0312], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0301, -0.0689], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1127, -0.1262], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:4 step:5 train loss:1.644925, train acc:57.135, train f1:53.645, train precision:59.197, train recall:52.383, train kappa:55.099
self.nodes_fc[0]: tensor([ 0.0524, -0.0979], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1086,  0.0354], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1224, -0.0328], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1194, 0.0315], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0301, -0.0686], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1125, -0.1263], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:4 step:6 train loss:1.615373, train acc:57.455, train f1:54.908, train precision:61.652, train recall:53.607, train kappa:55.468
self.nodes_fc[0]: tensor([ 0.0520, -0.0980], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1091,  0.0355], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1222, -0.0327], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1196, 0.0318], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0302, -0.0683], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1124, -0.1263], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:4 step:7 train loss:1.614482, train acc:57.474, train f1:54.727, train precision:58.936, train recall:53.553, train kappa:55.500
self.nodes_fc[0]: tensor([ 0.0517, -0.0981], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1097,  0.0355], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1220, -0.0328], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1199, 0.0322], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0303, -0.0682], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1123, -0.1264], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:4 step:8 train loss:1.588305, train acc:58.725, train f1:55.838, train precision:60.880, train recall:54.641, train kappa:56.845
self.nodes_fc[0]: tensor([ 0.0515, -0.0982], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1102,  0.0356], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1219, -0.0329], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1201, 0.0325], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0304, -0.0681], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1122, -0.1264], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:4 step:9 train loss:1.565873, train acc:59.097, train f1:56.509, train precision:61.150, train recall:55.515, train kappa:57.270
self.nodes_fc[0]: tensor([ 0.0513, -0.0983], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1108,  0.0357], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1217, -0.0329], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1204, 0.0329], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0305, -0.0680], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1121, -0.1264], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:4 step:10 train loss:1.543493, train acc:59.366, train f1:56.505, train precision:60.491, train recall:55.653, train kappa:57.507
self.nodes_fc[0]: tensor([ 0.0512, -0.0984], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1113,  0.0358], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1215, -0.0330], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1208, 0.0334], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0307, -0.0680], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1120, -0.1264], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:4 step:11 train loss:1.561084, train acc:59.454, train f1:57.578, train precision:62.044, train recall:56.562, train kappa:57.609
self.nodes_fc[0]: tensor([ 0.0512, -0.0985], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1117,  0.0360], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1215, -0.0332], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1211, 0.0337], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0308, -0.0682], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1119, -0.1264], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0512, -0.0985], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1117,  0.0360], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1215, -0.0332], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1211, 0.0337], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0308, -0.0682], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1119, -0.1264], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:4        valid loss:2.239862, valid acc:38.095, valid f1:26.630, valid precision:32.511, valid recall:42.403, valid kappa:32.904
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0512, -0.0985], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1117,  0.0360], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1215, -0.0332], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1211, 0.0337], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0308, -0.0682], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1119, -0.1264], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:5 step:0 train loss:1.520990, train acc:60.269, train f1:57.148, train precision:62.017, train recall:55.916, train kappa:58.416
self.nodes_fc[0]: tensor([ 0.0513, -0.0984], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1122,  0.0362], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1215, -0.0334], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1213, 0.0339], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0310, -0.0684], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1119, -0.1264], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:5 step:1 train loss:1.500258, train acc:60.815, train f1:58.065, train precision:63.145, train recall:56.614, train kappa:59.002
self.nodes_fc[0]: tensor([ 0.0514, -0.0983], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1127,  0.0364], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1215, -0.0335], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1214, 0.0340], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0312, -0.0684], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1117, -0.1263], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:5 step:2 train loss:1.493274, train acc:61.108, train f1:58.381, train precision:62.542, train recall:57.069, train kappa:59.369
self.nodes_fc[0]: tensor([ 0.0515, -0.0984], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1132,  0.0365], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1214, -0.0335], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1215, 0.0342], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0314, -0.0683], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1116, -0.1263], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:5 step:3 train loss:1.476463, train acc:61.517, train f1:58.930, train precision:64.155, train recall:58.056, train kappa:59.787
self.nodes_fc[0]: tensor([ 0.0514, -0.0984], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1137,  0.0365], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1213, -0.0332], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1217, 0.0343], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0316, -0.0682], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1114, -0.1262], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:5 step:4 train loss:1.446290, train acc:62.119, train f1:59.111, train precision:62.863, train recall:58.578, train kappa:60.430
self.nodes_fc[0]: tensor([ 0.0514, -0.0986], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1141,  0.0366], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1212, -0.0328], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1217, 0.0345], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0317, -0.0681], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1112, -0.1261], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:5 step:5 train loss:1.425797, train acc:62.750, train f1:60.124, train precision:63.450, train recall:59.474, train kappa:61.130
self.nodes_fc[0]: tensor([ 0.0513, -0.0988], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1146,  0.0366], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1212, -0.0324], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1218, 0.0348], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0319, -0.0680], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1111, -0.1261], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:5 step:6 train loss:1.422692, train acc:62.704, train f1:59.966, train precision:64.196, train recall:58.512, train kappa:61.003
self.nodes_fc[0]: tensor([ 0.0511, -0.0991], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1151,  0.0367], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1210, -0.0320], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1218, 0.0350], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0320, -0.0680], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1109, -0.1260], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:5 step:7 train loss:1.404764, train acc:63.138, train f1:60.495, train precision:64.722, train recall:59.090, train kappa:61.477
self.nodes_fc[0]: tensor([ 0.0508, -0.0994], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1155,  0.0368], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1206, -0.0317], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1219, 0.0352], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0321, -0.0679], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1107, -0.1259], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:5 step:8 train loss:1.421639, train acc:62.741, train f1:60.536, train precision:66.277, train recall:59.037, train kappa:61.038
self.nodes_fc[0]: tensor([ 0.0506, -0.0997], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1159,  0.0368], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1203, -0.0315], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1221, 0.0356], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0322, -0.0680], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1106, -0.1257], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:5 step:9 train loss:1.367992, train acc:63.867, train f1:61.417, train precision:66.433, train recall:60.072, train kappa:62.258
self.nodes_fc[0]: tensor([ 0.0504, -0.1000], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1162,  0.0368], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1199, -0.0313], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1222, 0.0360], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0323, -0.0681], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1105, -0.1256], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:5 step:10 train loss:1.380504, train acc:63.931, train f1:61.431, train precision:64.989, train recall:60.601, train kappa:62.355
self.nodes_fc[0]: tensor([ 0.0502, -0.1003], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1164,  0.0368], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1196, -0.0308], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1221, 0.0364], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0324, -0.0681], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1104, -0.1254], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:5 step:11 train loss:1.363764, train acc:64.637, train f1:62.252, train precision:65.714, train recall:61.910, train kappa:63.085
self.nodes_fc[0]: tensor([ 0.0500, -0.1005], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1167,  0.0368], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1194, -0.0305], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1220, 0.0367], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0325, -0.0680], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1103, -0.1252], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0500, -0.1005], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1167,  0.0368], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1194, -0.0305], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1220, 0.0367], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0325, -0.0680], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1103, -0.1252], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:5        valid loss:1.660167, valid acc:53.737, valid f1:35.624, valid precision:36.917, valid recall:55.118, valid kappa:48.915
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0500, -0.1005], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1167,  0.0368], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1194, -0.0305], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1220, 0.0367], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0325, -0.0680], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1103, -0.1252], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:6 step:0 train loss:1.356024, train acc:64.432, train f1:62.245, train precision:67.679, train recall:61.498, train kappa:62.898
self.nodes_fc[0]: tensor([ 0.0499, -0.1006], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1169,  0.0367], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1192, -0.0302], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1221, 0.0371], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0326, -0.0679], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1102, -0.1250], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:6 step:1 train loss:1.327584, train acc:65.552, train f1:63.272, train precision:68.571, train recall:62.161, train kappa:64.035
self.nodes_fc[0]: tensor([ 0.0498, -0.1005], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1172,  0.0367], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1192, -0.0299], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1223, 0.0373], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0326, -0.0678], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1100, -0.1249], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:6 step:2 train loss:1.340354, train acc:65.045, train f1:62.599, train precision:68.117, train recall:61.569, train kappa:63.534
self.nodes_fc[0]: tensor([ 0.0497, -0.1004], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1175,  0.0367], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1193, -0.0297], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1225, 0.0375], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0327, -0.0676], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1100, -0.1248], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:6 step:3 train loss:1.313174, train acc:65.796, train f1:63.108, train precision:68.544, train recall:62.311, train kappa:64.305
self.nodes_fc[0]: tensor([ 0.0496, -0.1004], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1177,  0.0368], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1194, -0.0295], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1226, 0.0378], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0328, -0.0675], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1099, -0.1247], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:6 step:4 train loss:1.297264, train acc:65.894, train f1:63.115, train precision:67.049, train recall:61.856, train kappa:64.394
self.nodes_fc[0]: tensor([ 0.0495, -0.1004], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1180,  0.0368], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1196, -0.0293], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1227, 0.0382], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0328, -0.0674], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1099, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:6 step:5 train loss:1.286220, train acc:66.095, train f1:63.941, train precision:67.898, train recall:62.638, train kappa:64.611
self.nodes_fc[0]: tensor([ 0.0495, -0.1005], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1182,  0.0368], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1198, -0.0292], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1229, 0.0386], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0329, -0.0672], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1098, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:6 step:6 train loss:1.307073, train acc:65.964, train f1:63.423, train precision:67.379, train recall:61.994, train kappa:64.459
self.nodes_fc[0]: tensor([ 0.0494, -0.1006], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1185,  0.0368], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1201, -0.0291], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1233, 0.0390], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0330, -0.0672], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1098, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:6 step:7 train loss:1.281939, train acc:66.586, train f1:64.320, train precision:69.210, train recall:63.130, train kappa:65.112
self.nodes_fc[0]: tensor([ 0.0493, -0.1006], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1187,  0.0368], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1204, -0.0290], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1236, 0.0395], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0330, -0.0671], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1097, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:6 step:8 train loss:1.276759, train acc:66.711, train f1:64.462, train precision:68.835, train recall:63.497, train kappa:65.263
self.nodes_fc[0]: tensor([ 0.0492, -0.1005], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1189,  0.0367], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1205, -0.0290], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1239, 0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0330, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1097, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:6 step:9 train loss:1.258398, train acc:66.791, train f1:64.153, train precision:68.352, train recall:63.624, train kappa:65.354
self.nodes_fc[0]: tensor([ 0.0490, -0.1004], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1191,  0.0366], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1206, -0.0290], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1242, 0.0404], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0331, -0.0669], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1096, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:6 step:10 train loss:1.248394, train acc:67.377, train f1:65.003, train precision:68.395, train recall:64.395, train kappa:65.991
self.nodes_fc[0]: tensor([ 0.0489, -0.1004], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1193,  0.0365], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1208, -0.0289], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1244, 0.0408], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0332, -0.0669], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1096, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:6 step:11 train loss:1.263679, train acc:67.069, train f1:64.849, train precision:69.351, train recall:64.701, train kappa:65.697
self.nodes_fc[0]: tensor([ 0.0487, -0.1005], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1195,  0.0365], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1209, -0.0291], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1247, 0.0413], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0332, -0.0670], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1096, -0.1247], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0487, -0.1005], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1195,  0.0365], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1209, -0.0291], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1247, 0.0413], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0332, -0.0670], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1096, -0.1247], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:6        valid loss:1.134611, valid acc:68.888, valid f1:46.283, valid precision:44.231, valid recall:63.953, valid kappa:65.253
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0487, -0.1005], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1195,  0.0365], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1209, -0.0291], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1247, 0.0413], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0332, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1096, -0.1247], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:7 step:0 train loss:1.232219, train acc:67.465, train f1:65.162, train precision:69.087, train recall:64.566, train kappa:66.089
self.nodes_fc[0]: tensor([ 0.0486, -0.1006], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1196,  0.0366], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1211, -0.0292], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1250, 0.0417], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0332, -0.0671], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1096, -0.1247], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:7 step:1 train loss:1.236541, train acc:67.532, train f1:65.272, train precision:70.172, train recall:64.215, train kappa:66.112
self.nodes_fc[0]: tensor([ 0.0485, -0.1007], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1197,  0.0367], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1213, -0.0293], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1251, 0.0422], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0332, -0.0671], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1096, -0.1247], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:7 step:2 train loss:1.225383, train acc:67.697, train f1:65.492, train precision:70.284, train recall:64.132, train kappa:66.260
self.nodes_fc[0]: tensor([ 0.0484, -0.1007], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1198,  0.0367], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1214, -0.0293], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1252, 0.0426], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0331, -0.0671], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1096, -0.1247], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:7 step:3 train loss:1.211768, train acc:68.054, train f1:65.794, train precision:71.270, train recall:64.885, train kappa:66.672
self.nodes_fc[0]: tensor([ 0.0484, -0.1007], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1199,  0.0368], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1214, -0.0293], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1253, 0.0428], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0330, -0.0671], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1096, -0.1248], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:7 step:4 train loss:1.199064, train acc:68.307, train f1:66.205, train precision:69.607, train recall:65.592, train kappa:66.979
self.nodes_fc[0]: tensor([ 0.0482, -0.1008], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1200,  0.0370], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1214, -0.0292], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1253, 0.0431], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0329, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1096, -0.1248], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:7 step:5 train loss:1.194768, train acc:68.332, train f1:65.786, train precision:69.908, train recall:65.030, train kappa:66.985
self.nodes_fc[0]: tensor([ 0.0480, -0.1010], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1201,  0.0370], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1213, -0.0291], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1253, 0.0433], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0328, -0.0669], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1095, -0.1249], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:7 step:6 train loss:1.188672, train acc:68.140, train f1:66.072, train precision:70.631, train recall:65.382, train kappa:66.808
self.nodes_fc[0]: tensor([ 0.0478, -0.1013], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1202,  0.0372], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1212, -0.0289], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1254, 0.0434], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0328, -0.0667], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1094, -0.1250], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:7 step:7 train loss:1.185124, train acc:68.665, train f1:66.168, train precision:70.172, train recall:65.648, train kappa:67.361
self.nodes_fc[0]: tensor([ 0.0477, -0.1015], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1202,  0.0373], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1212, -0.0288], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1253, 0.0436], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0328, -0.0664], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1094, -0.1251], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:7 step:8 train loss:1.184462, train acc:68.524, train f1:65.922, train precision:69.176, train recall:65.451, train kappa:67.193
self.nodes_fc[0]: tensor([ 0.0476, -0.1017], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1202,  0.0376], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1212, -0.0288], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1252, 0.0439], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0328, -0.0663], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1093, -0.1253], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:7 step:9 train loss:1.175491, train acc:68.884, train f1:66.545, train precision:71.218, train recall:65.845, train kappa:67.567
self.nodes_fc[0]: tensor([ 0.0476, -0.1018], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1202,  0.0379], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1212, -0.0289], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1250, 0.0442], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0329, -0.0662], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1092, -0.1254], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:7 step:10 train loss:1.170553, train acc:68.945, train f1:66.816, train precision:70.744, train recall:65.844, train kappa:67.598
self.nodes_fc[0]: tensor([ 0.0477, -0.1018], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1200,  0.0381], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1213, -0.0290], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1249, 0.0445], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0329, -0.0662], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1090, -0.1255], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:7 step:11 train loss:1.134879, train acc:70.061, train f1:67.177, train precision:71.715, train recall:66.334, train kappa:68.754
self.nodes_fc[0]: tensor([ 0.0477, -0.1017], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1199,  0.0384], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1212, -0.0292], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1249, 0.0446], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0329, -0.0661], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1088, -0.1255], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0477, -0.1017], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1199,  0.0384], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1212, -0.0292], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1249, 0.0446], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0329, -0.0661], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1088, -0.1255], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:7        valid loss:1.000165, valid acc:72.767, valid f1:50.128, valid precision:47.665, valid recall:66.226, valid kappa:69.494
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0477, -0.1017], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1199,  0.0384], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1212, -0.0292], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1249, 0.0446], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0329, -0.0661], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1088, -0.1255], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:8 step:0 train loss:1.124955, train acc:70.395, train f1:68.178, train precision:73.930, train recall:66.969, train kappa:69.141
self.nodes_fc[0]: tensor([ 0.0477, -0.1016], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1199,  0.0386], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1211, -0.0295], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1249, 0.0446], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0329, -0.0660], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1085, -0.1256], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:8 step:1 train loss:1.132413, train acc:69.958, train f1:67.852, train precision:72.330, train recall:66.847, train kappa:68.662
self.nodes_fc[0]: tensor([ 0.0477, -0.1015], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1200,  0.0389], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1210, -0.0297], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1248, 0.0447], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0330, -0.0659], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1083, -0.1258], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:8 step:2 train loss:1.126742, train acc:70.020, train f1:67.783, train precision:71.426, train recall:67.297, train kappa:68.775
self.nodes_fc[0]: tensor([ 0.0477, -0.1014], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1202,  0.0391], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1210, -0.0298], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1247, 0.0448], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0330, -0.0658], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1081, -0.1259], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:8 step:3 train loss:1.119449, train acc:70.453, train f1:67.677, train precision:71.524, train recall:67.260, train kappa:69.212
self.nodes_fc[0]: tensor([ 0.0477, -0.1014], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1205,  0.0392], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1210, -0.0300], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1247, 0.0449], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0330, -0.0656], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1079, -0.1259], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:8 step:4 train loss:1.114431, train acc:70.074, train f1:67.729, train precision:70.906, train recall:67.574, train kappa:68.800
self.nodes_fc[0]: tensor([ 0.0477, -0.1014], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1207,  0.0394], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1210, -0.0301], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1246, 0.0450], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0330, -0.0655], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1077, -0.1260], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:8 step:5 train loss:1.114763, train acc:70.132, train f1:67.719, train precision:71.957, train recall:67.087, train kappa:68.888
self.nodes_fc[0]: tensor([ 0.0477, -0.1013], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1208,  0.0395], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1210, -0.0303], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1244, 0.0452], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0331, -0.0653], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1074, -0.1261], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:8 step:6 train loss:1.109091, train acc:70.508, train f1:68.081, train precision:71.539, train recall:67.550, train kappa:69.241
self.nodes_fc[0]: tensor([ 0.0477, -0.1012], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1209,  0.0395], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1209, -0.0305], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1242, 0.0455], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0332, -0.0650], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1071, -0.1261], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:8 step:7 train loss:1.108418, train acc:70.245, train f1:68.251, train precision:72.757, train recall:67.130, train kappa:68.933
self.nodes_fc[0]: tensor([ 0.0479, -0.1009], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1210,  0.0396], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1208, -0.0308], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1239, 0.0457], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0333, -0.0647], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1069, -0.1262], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:8 step:8 train loss:1.106490, train acc:70.572, train f1:68.397, train precision:73.034, train recall:67.015, train kappa:69.322
self.nodes_fc[0]: tensor([ 0.0480, -0.1008], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1210,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1207, -0.0310], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1237, 0.0458], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0334, -0.0645], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1067, -0.1262], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:8 step:9 train loss:1.098660, train acc:70.740, train f1:68.468, train precision:73.201, train recall:67.464, train kappa:69.502
self.nodes_fc[0]: tensor([ 0.0482, -0.1008], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1209,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1208, -0.0314], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1235, 0.0459], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0335, -0.0642], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1065, -0.1262], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:8 step:10 train loss:1.097337, train acc:70.801, train f1:68.833, train precision:73.717, train recall:67.961, train kappa:69.597
self.nodes_fc[0]: tensor([ 0.0482, -0.1007], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1210,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1207, -0.0315], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1231, 0.0460], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0335, -0.0641], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1063, -0.1262], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:8 step:11 train loss:1.116365, train acc:70.263, train f1:68.666, train precision:71.851, train recall:68.051, train kappa:69.030
self.nodes_fc[0]: tensor([ 0.0483, -0.1007], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1210,  0.0396], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1207, -0.0316], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1227, 0.0460], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0335, -0.0640], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1062, -0.1262], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0483, -0.1007], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1210,  0.0396], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1207, -0.0316], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1227, 0.0460], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0335, -0.0640], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1062, -0.1262], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:8        valid loss:0.959835, valid acc:74.184, valid f1:50.520, valid precision:46.984, valid recall:67.301, valid kappa:71.127
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0483, -0.1007], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1210,  0.0396], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1207, -0.0316], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1227, 0.0460], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0335, -0.0640], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1062, -0.1262], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:9 step:0 train loss:1.087529, train acc:71.155, train f1:69.098, train precision:72.411, train recall:68.385, train kappa:69.951
self.nodes_fc[0]: tensor([ 0.0484, -0.1007], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1211,  0.0395], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1208, -0.0317], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1222, 0.0459], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0335, -0.0641], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1060, -0.1262], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:9 step:1 train loss:1.065180, train acc:71.756, train f1:69.609, train precision:72.614, train recall:69.335, train kappa:70.591
self.nodes_fc[0]: tensor([ 0.0484, -0.1008], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1213,  0.0395], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1210, -0.0317], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1218, 0.0458], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0336, -0.0641], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1058, -0.1261], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:9 step:2 train loss:1.072592, train acc:71.487, train f1:69.431, train precision:72.770, train recall:69.235, train kappa:70.312
self.nodes_fc[0]: tensor([ 0.0485, -0.1008], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1214,  0.0394], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1211, -0.0318], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1215, 0.0456], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0337, -0.0642], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1057, -0.1260], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:9 step:3 train loss:1.063308, train acc:71.768, train f1:69.463, train precision:73.198, train recall:68.774, train kappa:70.588
self.nodes_fc[0]: tensor([ 0.0485, -0.1009], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1216,  0.0394], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1212, -0.0318], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1212, 0.0454], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0338, -0.0642], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1055, -0.1259], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:9 step:4 train loss:1.049498, train acc:71.780, train f1:69.818, train precision:73.602, train recall:69.221, train kappa:70.621
self.nodes_fc[0]: tensor([ 0.0484, -0.1009], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1217,  0.0393], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1213, -0.0319], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1210, 0.0452], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0339, -0.0642], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1054, -0.1258], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:9 step:5 train loss:1.053068, train acc:71.674, train f1:69.503, train precision:73.212, train recall:68.711, train kappa:70.449
self.nodes_fc[0]: tensor([ 0.0484, -0.1008], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1219,  0.0393], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1212, -0.0320], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1209, 0.0452], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0340, -0.0643], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1052, -0.1257], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:9 step:6 train loss:1.048940, train acc:72.058, train f1:70.294, train precision:74.628, train recall:69.209, train kappa:70.881
self.nodes_fc[0]: tensor([ 0.0483, -0.1009], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1220,  0.0393], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1212, -0.0320], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1207, 0.0455], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0341, -0.0645], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1051, -0.1257], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:9 step:7 train loss:1.041506, train acc:71.741, train f1:69.599, train precision:73.410, train recall:68.483, train kappa:70.545
self.nodes_fc[0]: tensor([ 0.0482, -0.1010], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1223,  0.0393], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1212, -0.0321], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1206, 0.0458], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0342, -0.0647], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1050, -0.1256], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:9 step:8 train loss:1.045385, train acc:72.009, train f1:69.607, train precision:73.141, train recall:69.096, train kappa:70.851
self.nodes_fc[0]: tensor([ 0.0481, -0.1012], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1224,  0.0393], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1212, -0.0320], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1205, 0.0461], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0343, -0.0649], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1049, -0.1256], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:9 step:9 train loss:1.028441, train acc:72.208, train f1:70.096, train precision:73.179, train recall:69.539, train kappa:71.061
self.nodes_fc[0]: tensor([ 0.0480, -0.1016], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1226,  0.0394], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1214, -0.0318], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1204, 0.0465], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0344, -0.0651], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1048, -0.1255], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:9 step:10 train loss:1.027768, train acc:72.211, train f1:70.313, train precision:73.826, train recall:69.927, train kappa:71.063
self.nodes_fc[0]: tensor([ 0.0479, -0.1020], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1227,  0.0394], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1215, -0.0317], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1204, 0.0470], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0344, -0.0653], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1047, -0.1254], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:9 step:11 train loss:0.989583, train acc:73.101, train f1:70.449, train precision:74.463, train recall:69.925, train kappa:71.968
self.nodes_fc[0]: tensor([ 0.0478, -0.1024], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1227,  0.0394], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1216, -0.0318], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1205, 0.0475], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0346, -0.0656], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1046, -0.1254], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0478, -0.1024], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1227,  0.0394], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1216, -0.0318], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1205, 0.0475], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0346, -0.0656], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1046, -0.1254], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:9        valid loss:0.899065, valid acc:75.650, valid f1:51.678, valid precision:49.891, valid recall:67.740, valid kappa:72.684
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0478, -0.1024], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1227,  0.0394], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1216, -0.0318], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1205, 0.0475], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0346, -0.0656], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1046, -0.1254], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:10 step:0 train loss:1.013975, train acc:72.723, train f1:70.868, train precision:74.668, train recall:70.122, train kappa:71.573
self.nodes_fc[0]: tensor([ 0.0478, -0.1027], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1227,  0.0394], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1216, -0.0318], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1208, 0.0478], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0659], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1044, -0.1253], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:10 step:1 train loss:1.006831, train acc:72.830, train f1:70.779, train precision:75.033, train recall:69.854, train kappa:71.691
self.nodes_fc[0]: tensor([ 0.0478, -0.1028], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1226,  0.0393], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1215, -0.0318], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1211, 0.0480], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0662], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1043, -0.1253], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:10 step:2 train loss:1.008060, train acc:72.946, train f1:70.952, train precision:75.285, train recall:70.315, train kappa:71.816
self.nodes_fc[0]: tensor([ 0.0478, -0.1030], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1225,  0.0392], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1216, -0.0318], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1211, 0.0482], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0349, -0.0664], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1042, -0.1252], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:10 step:3 train loss:0.991698, train acc:73.029, train f1:70.670, train precision:73.897, train recall:69.898, train kappa:71.891
self.nodes_fc[0]: tensor([ 0.0478, -0.1030], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1224,  0.0392], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1217, -0.0319], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1211, 0.0485], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0349, -0.0666], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1040, -0.1251], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:10 step:4 train loss:1.000691, train acc:73.062, train f1:70.923, train precision:74.962, train recall:70.183, train kappa:71.935
self.nodes_fc[0]: tensor([ 0.0478, -0.1031], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1224,  0.0392], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1219, -0.0320], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1212, 0.0488], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0349, -0.0668], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1039, -0.1250], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:10 step:5 train loss:0.998389, train acc:73.068, train f1:71.005, train precision:75.167, train recall:70.596, train kappa:71.973
self.nodes_fc[0]: tensor([ 0.0479, -0.1032], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1223,  0.0392], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1220, -0.0321], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1212, 0.0490], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0349, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1038, -0.1248], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:10 step:6 train loss:0.993183, train acc:73.215, train f1:71.132, train precision:75.202, train recall:70.618, train kappa:72.099
self.nodes_fc[0]: tensor([ 0.0478, -0.1033], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1223,  0.0392], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1222, -0.0320], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1213, 0.0493], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0349, -0.0671], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1037, -0.1247], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:10 step:7 train loss:0.979435, train acc:73.355, train f1:71.076, train precision:74.972, train recall:70.758, train kappa:72.253
self.nodes_fc[0]: tensor([ 0.0477, -0.1034], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1222,  0.0391], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1223, -0.0318], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1212, 0.0496], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0672], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1036, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:10 step:8 train loss:0.994835, train acc:73.154, train f1:71.458, train precision:75.198, train recall:71.148, train kappa:72.062
self.nodes_fc[0]: tensor([ 0.0477, -0.1035], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1222,  0.0390], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1223, -0.0318], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1212, 0.0500], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0672], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1035, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:10 step:9 train loss:0.974021, train acc:73.618, train f1:71.422, train precision:74.298, train recall:71.013, train kappa:72.531
self.nodes_fc[0]: tensor([ 0.0476, -0.1033], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1222,  0.0389], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1223, -0.0316], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1211, 0.0504], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0672], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1034, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:10 step:10 train loss:0.976438, train acc:73.740, train f1:71.753, train precision:74.334, train recall:71.145, train kappa:72.666
self.nodes_fc[0]: tensor([ 0.0477, -0.1032], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1222,  0.0388], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1222, -0.0315], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1211, 0.0506], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0673], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1033, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:10 step:11 train loss:0.989605, train acc:73.333, train f1:71.558, train precision:74.666, train recall:71.157, train kappa:72.244
self.nodes_fc[0]: tensor([ 0.0478, -0.1032], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1222,  0.0387], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1221, -0.0313], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1213, 0.0506], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0673], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1033, -0.1245], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0478, -0.1032], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1222,  0.0387], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1221, -0.0313], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1213, 0.0506], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0673], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1033, -0.1245], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:10        valid loss:0.874813, valid acc:76.482, valid f1:52.462, valid precision:50.245, valid recall:68.508, valid kappa:73.620
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0478, -0.1032], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1222,  0.0387], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1221, -0.0313], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1213, 0.0506], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0673], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1033, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:11 step:0 train loss:0.964628, train acc:73.502, train f1:71.649, train precision:76.170, train recall:70.958, train kappa:72.423
self.nodes_fc[0]: tensor([ 0.0479, -0.1032], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1222,  0.0387], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1219, -0.0310], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1212, 0.0506], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0674], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1033, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:11 step:1 train loss:0.971346, train acc:73.758, train f1:71.852, train precision:76.179, train recall:71.197, train kappa:72.689
self.nodes_fc[0]: tensor([ 0.0481, -0.1031], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1222,  0.0387], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1218, -0.0307], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1212, 0.0504], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0675], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1033, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:11 step:2 train loss:0.949882, train acc:74.576, train f1:72.670, train precision:76.016, train recall:72.093, train kappa:73.536
self.nodes_fc[0]: tensor([ 0.0481, -0.1029], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1221,  0.0388], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1218, -0.0304], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1212, 0.0501], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0676], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1032, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:11 step:3 train loss:0.964354, train acc:73.825, train f1:71.979, train precision:76.891, train recall:71.565, train kappa:72.739
self.nodes_fc[0]: tensor([ 0.0482, -0.1027], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1222,  0.0388], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1217, -0.0301], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1212, 0.0498], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0346, -0.0676], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1032, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:11 step:4 train loss:0.947399, train acc:74.387, train f1:72.366, train precision:76.147, train recall:71.720, train kappa:73.328
self.nodes_fc[0]: tensor([ 0.0482, -0.1024], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1222,  0.0388], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1213, -0.0298], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1211, 0.0495], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0677], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1031, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:11 step:5 train loss:0.956383, train acc:74.002, train f1:72.078, train precision:75.678, train recall:71.181, train kappa:72.904
self.nodes_fc[0]: tensor([ 0.0482, -0.1021], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1221,  0.0388], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1209, -0.0297], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1210, 0.0492], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0677], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1031, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:11 step:6 train loss:0.942990, train acc:74.506, train f1:72.480, train precision:75.527, train recall:71.838, train kappa:73.436
self.nodes_fc[0]: tensor([ 0.0481, -0.1017], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1221,  0.0388], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1204, -0.0294], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1209, 0.0488], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0676], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1031, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:11 step:7 train loss:0.937694, train acc:74.521, train f1:72.165, train precision:74.958, train recall:71.788, train kappa:73.470
self.nodes_fc[0]: tensor([ 0.0481, -0.1014], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1221,  0.0388], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1200, -0.0293], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1207, 0.0486], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0346, -0.0677], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1030, -0.1247], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:11 step:8 train loss:0.942963, train acc:74.152, train f1:72.203, train precision:76.072, train recall:71.648, train kappa:73.076
self.nodes_fc[0]: tensor([ 0.0480, -0.1011], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1222,  0.0389], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1198, -0.0294], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1205, 0.0484], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0346, -0.0676], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1030, -0.1247], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:11 step:9 train loss:0.951399, train acc:74.207, train f1:72.251, train precision:75.039, train recall:71.962, train kappa:73.181
self.nodes_fc[0]: tensor([ 0.0479, -0.1010], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1223,  0.0388], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1195, -0.0294], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1203, 0.0482], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0346, -0.0676], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1030, -0.1248], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:11 step:10 train loss:0.929002, train acc:74.973, train f1:72.935, train precision:76.499, train recall:72.679, train kappa:73.934
self.nodes_fc[0]: tensor([ 0.0479, -0.1009], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1224,  0.0388], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1193, -0.0293], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1201, 0.0481], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0346, -0.0676], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1029, -0.1248], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:11 step:11 train loss:0.950169, train acc:73.854, train f1:71.939, train precision:75.714, train recall:71.547, train kappa:72.790
self.nodes_fc[0]: tensor([ 0.0479, -0.1009], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1224,  0.0387], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1190, -0.0291], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1201, 0.0480], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0346, -0.0677], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1029, -0.1248], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0479, -0.1009], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1224,  0.0387], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1190, -0.0291], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1201, 0.0480], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0346, -0.0677], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1029, -0.1248], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:11        valid loss:0.845780, valid acc:76.999, valid f1:52.995, valid precision:51.153, valid recall:68.761, valid kappa:74.185
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0479, -0.1009], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1224,  0.0387], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1190, -0.0291], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1201, 0.0480], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0346, -0.0677], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1029, -0.1248], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:12 step:0 train loss:0.944731, train acc:74.405, train f1:72.725, train precision:76.740, train recall:72.449, train kappa:73.362
self.nodes_fc[0]: tensor([ 0.0479, -0.1008], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1225,  0.0386], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1187, -0.0292], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1200, 0.0479], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0678], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1028, -0.1248], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:12 step:1 train loss:0.913987, train acc:75.076, train f1:73.067, train precision:76.661, train recall:72.561, train kappa:74.040
self.nodes_fc[0]: tensor([ 0.0480, -0.1007], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1226,  0.0386], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1185, -0.0294], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1200, 0.0477], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0679], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1028, -0.1248], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:12 step:2 train loss:0.912739, train acc:75.027, train f1:73.020, train precision:76.034, train recall:72.515, train kappa:74.003
self.nodes_fc[0]: tensor([ 0.0481, -0.1006], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1227,  0.0386], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1183, -0.0297], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1199, 0.0474], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0680], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1028, -0.1248], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:12 step:3 train loss:0.920228, train acc:74.820, train f1:72.954, train precision:76.882, train recall:72.063, train kappa:73.771
self.nodes_fc[0]: tensor([ 0.0483, -0.1005], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1229,  0.0386], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1181, -0.0300], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1199, 0.0472], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0681], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1027, -0.1247], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:12 step:4 train loss:0.915612, train acc:74.936, train f1:73.029, train precision:76.361, train recall:72.196, train kappa:73.882
self.nodes_fc[0]: tensor([ 0.0483, -0.1004], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1229,  0.0386], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1179, -0.0304], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1200, 0.0470], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0683], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1027, -0.1247], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:12 step:5 train loss:0.936539, train acc:74.341, train f1:72.564, train precision:76.347, train recall:71.705, train kappa:73.246
self.nodes_fc[0]: tensor([ 0.0483, -0.1005], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1230,  0.0386], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1178, -0.0307], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1201, 0.0468], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0686], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1027, -0.1247], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:12 step:6 train loss:0.912531, train acc:74.957, train f1:73.130, train precision:76.883, train recall:72.714, train kappa:73.953
self.nodes_fc[0]: tensor([ 0.0482, -0.1006], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1230,  0.0385], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1176, -0.0310], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1203, 0.0466], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0688], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1026, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:12 step:7 train loss:0.907544, train acc:75.339, train f1:73.408, train precision:75.791, train recall:73.151, train kappa:74.347
self.nodes_fc[0]: tensor([ 0.0482, -0.1006], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1229,  0.0386], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1175, -0.0312], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1206, 0.0464], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0690], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1026, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:12 step:8 train loss:0.898386, train acc:75.610, train f1:73.594, train precision:77.741, train recall:73.249, train kappa:74.614
self.nodes_fc[0]: tensor([ 0.0482, -0.1006], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1228,  0.0387], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1173, -0.0314], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1207, 0.0463], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0693], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1026, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:12 step:9 train loss:0.897094, train acc:75.568, train f1:73.774, train precision:78.797, train recall:73.359, train kappa:74.557
self.nodes_fc[0]: tensor([ 0.0483, -0.1006], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1228,  0.0387], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1173, -0.0315], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1208, 0.0462], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0349, -0.0697], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1026, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:12 step:10 train loss:0.889026, train acc:75.708, train f1:73.706, train precision:77.948, train recall:73.207, train kappa:74.717
self.nodes_fc[0]: tensor([ 0.0484, -0.1006], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1228,  0.0388], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1172, -0.0317], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1209, 0.0463], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0350, -0.0699], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1026, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:12 step:11 train loss:0.904997, train acc:75.466, train f1:73.699, train precision:76.297, train recall:73.510, train kappa:74.495
self.nodes_fc[0]: tensor([ 0.0484, -0.1006], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1228,  0.0389], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1171, -0.0317], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1210, 0.0464], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0352, -0.0702], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1026, -0.1245], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0484, -0.1006], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1228,  0.0389], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1171, -0.0317], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1210, 0.0464], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0352, -0.0702], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1026, -0.1245], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:12        valid loss:0.825630, valid acc:77.519, valid f1:53.452, valid precision:51.653, valid recall:68.979, valid kappa:74.791
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0484, -0.1006], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1228,  0.0389], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1171, -0.0317], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1210, 0.0464], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0352, -0.0702], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1026, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:13 step:0 train loss:0.892099, train acc:75.623, train f1:73.961, train precision:78.168, train recall:73.363, train kappa:74.650
self.nodes_fc[0]: tensor([ 0.0485, -0.1006], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1229,  0.0390], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1170, -0.0318], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1210, 0.0466], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0353, -0.0703], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1026, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:13 step:1 train loss:0.882688, train acc:75.494, train f1:73.831, train precision:76.974, train recall:73.213, train kappa:74.487
self.nodes_fc[0]: tensor([ 0.0485, -0.1006], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1229,  0.0391], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1170, -0.0318], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1209, 0.0470], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0354, -0.0703], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1026, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:13 step:2 train loss:0.877921, train acc:75.861, train f1:73.923, train precision:77.079, train recall:73.436, train kappa:74.885
self.nodes_fc[0]: tensor([ 0.0485, -0.1006], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1230,  0.0391], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1170, -0.0319], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1209, 0.0473], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0355, -0.0703], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1025, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:13 step:3 train loss:0.878438, train acc:75.961, train f1:74.178, train precision:77.508, train recall:73.812, train kappa:74.988
self.nodes_fc[0]: tensor([ 0.0485, -0.1007], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1230,  0.0392], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1170, -0.0319], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1208, 0.0475], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0356, -0.0704], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1025, -0.1247], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:13 step:4 train loss:0.878884, train acc:76.089, train f1:74.239, train precision:78.731, train recall:73.758, train kappa:75.114
self.nodes_fc[0]: tensor([ 0.0486, -0.1008], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1231,  0.0393], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1171, -0.0319], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1207, 0.0477], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0356, -0.0704], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1025, -0.1248], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:13 step:5 train loss:0.869303, train acc:75.885, train f1:74.264, train precision:77.626, train recall:73.889, train kappa:74.893
self.nodes_fc[0]: tensor([ 0.0487, -0.1009], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1231,  0.0394], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1171, -0.0319], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1206, 0.0479], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0357, -0.0705], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1025, -0.1249], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:13 step:6 train loss:0.873917, train acc:76.147, train f1:74.261, train precision:76.851, train recall:73.838, train kappa:75.193
self.nodes_fc[0]: tensor([ 0.0489, -0.1009], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1231,  0.0395], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1172, -0.0320], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1205, 0.0482], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0357, -0.0707], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1024, -0.1249], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:13 step:7 train loss:0.863088, train acc:76.559, train f1:74.563, train precision:77.823, train recall:74.310, train kappa:75.600
self.nodes_fc[0]: tensor([ 0.0489, -0.1009], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1232,  0.0395], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1172, -0.0321], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1205, 0.0485], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0357, -0.0708], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1023, -0.1249], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:13 step:8 train loss:0.867273, train acc:76.303, train f1:74.478, train precision:77.838, train recall:74.176, train kappa:75.338
self.nodes_fc[0]: tensor([ 0.0490, -0.1008], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1233,  0.0396], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1173, -0.0322], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1204, 0.0488], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0357, -0.0709], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1023, -0.1250], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:13 step:9 train loss:0.869720, train acc:76.309, train f1:74.356, train precision:77.961, train recall:73.937, train kappa:75.335
self.nodes_fc[0]: tensor([ 0.0490, -0.1007], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1233,  0.0396], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1173, -0.0323], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1203, 0.0490], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0358, -0.0710], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1022, -0.1250], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:13 step:10 train loss:0.866380, train acc:76.074, train f1:74.464, train precision:78.032, train recall:73.793, train kappa:75.090
self.nodes_fc[0]: tensor([ 0.0491, -0.1005], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1234,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1172, -0.0324], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1202, 0.0492], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0358, -0.0710], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1022, -0.1250], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:13 step:11 train loss:0.866353, train acc:76.334, train f1:74.098, train precision:76.881, train recall:73.975, train kappa:75.352
self.nodes_fc[0]: tensor([ 0.0491, -0.1005], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1234,  0.0397], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1172, -0.0325], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1202, 0.0493], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0358, -0.0711], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1022, -0.1250], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0491, -0.1005], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1234,  0.0397], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1172, -0.0325], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1202, 0.0493], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0358, -0.0711], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1022, -0.1250], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:13        valid loss:0.804380, valid acc:78.183, valid f1:53.894, valid precision:51.544, valid recall:69.390, valid kappa:75.499
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0491, -0.1005], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1234,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1172, -0.0325], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1202, 0.0493], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0358, -0.0711], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1022, -0.1250], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:14 step:0 train loss:0.842832, train acc:76.938, train f1:75.078, train precision:78.203, train recall:74.627, train kappa:76.009
self.nodes_fc[0]: tensor([ 0.0491, -0.1004], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1234,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1173, -0.0325], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1203, 0.0494], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0358, -0.0712], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1021, -0.1250], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:14 step:1 train loss:0.857968, train acc:76.605, train f1:74.935, train precision:77.899, train recall:74.427, train kappa:75.654
self.nodes_fc[0]: tensor([ 0.0491, -0.1004], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1234,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1174, -0.0325], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1205, 0.0496], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0359, -0.0713], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1020, -0.1250], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:14 step:2 train loss:0.854719, train acc:76.343, train f1:74.619, train precision:78.448, train recall:74.092, train kappa:75.388
self.nodes_fc[0]: tensor([ 0.0490, -0.1004], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1235,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1174, -0.0325], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1206, 0.0498], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0359, -0.0712], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1019, -0.1251], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:14 step:3 train loss:0.845133, train acc:76.788, train f1:75.210, train precision:78.177, train recall:75.020, train kappa:75.856
self.nodes_fc[0]: tensor([ 0.0489, -0.1003], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1235,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1174, -0.0325], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1206, 0.0500], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0359, -0.0712], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1019, -0.1251], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:14 step:4 train loss:0.842158, train acc:76.828, train f1:75.002, train precision:79.081, train recall:74.995, train kappa:75.906
self.nodes_fc[0]: tensor([ 0.0488, -0.1003], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1235,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1174, -0.0325], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1208, 0.0502], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0358, -0.0711], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1018, -0.1251], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:14 step:5 train loss:0.840238, train acc:76.813, train f1:74.834, train precision:77.493, train recall:74.626, train kappa:75.884
self.nodes_fc[0]: tensor([ 0.0487, -0.1001], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1236,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1174, -0.0326], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1208, 0.0503], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0358, -0.0710], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1018, -0.1251], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:14 step:6 train loss:0.842850, train acc:76.874, train f1:75.038, train precision:78.184, train recall:74.769, train kappa:75.933
self.nodes_fc[0]: tensor([ 0.0487, -0.0999], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1236,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1173, -0.0326], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1208, 0.0503], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0358, -0.0708], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1017, -0.1252], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:14 step:7 train loss:0.839948, train acc:76.947, train f1:75.412, train precision:78.347, train recall:74.999, train kappa:75.993
self.nodes_fc[0]: tensor([ 0.0487, -0.0996], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1236,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1174, -0.0326], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1208, 0.0501], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0358, -0.0707], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1016, -0.1252], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:14 step:8 train loss:0.842557, train acc:76.685, train f1:75.104, train precision:78.955, train recall:74.439, train kappa:75.748
self.nodes_fc[0]: tensor([ 0.0486, -0.0994], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1237,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1174, -0.0325], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1209, 0.0500], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0358, -0.0705], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1015, -0.1252], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:14 step:9 train loss:0.832750, train acc:77.063, train f1:75.409, train precision:78.969, train recall:74.897, train kappa:76.118
self.nodes_fc[0]: tensor([ 0.0486, -0.0992], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1238,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1175, -0.0324], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1208, 0.0498], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0358, -0.0703], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1014, -0.1253], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:14 step:10 train loss:0.813874, train acc:77.469, train f1:75.661, train precision:79.161, train recall:75.220, train kappa:76.546
self.nodes_fc[0]: tensor([ 0.0485, -0.0991], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1239,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1175, -0.0323], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1206, 0.0496], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0358, -0.0703], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1014, -0.1253], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:14 step:11 train loss:0.839340, train acc:76.740, train f1:75.014, train precision:78.657, train recall:74.610, train kappa:75.807
self.nodes_fc[0]: tensor([ 0.0484, -0.0991], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1240,  0.0399], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1175, -0.0323], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1204, 0.0495], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0358, -0.0703], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1013, -0.1254], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0484, -0.0991], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1240,  0.0399], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1175, -0.0323], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1204, 0.0495], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0358, -0.0703], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1013, -0.1254], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:14        valid loss:0.790865, valid acc:78.467, valid f1:54.360, valid precision:52.216, valid recall:69.797, valid kappa:75.810
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0484, -0.0991], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1240,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1175, -0.0323], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1204, 0.0495], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0358, -0.0703], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1013, -0.1254], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:15 step:0 train loss:0.830363, train acc:77.014, train f1:75.369, train precision:78.868, train recall:74.965, train kappa:76.082
self.nodes_fc[0]: tensor([ 0.0482, -0.0991], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1242,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1174, -0.0323], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1203, 0.0494], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0357, -0.0703], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1012, -0.1254], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:15 step:1 train loss:0.815522, train acc:77.518, train f1:75.905, train precision:78.829, train recall:75.994, train kappa:76.626
self.nodes_fc[0]: tensor([ 0.0481, -0.0992], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1243,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1173, -0.0323], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1201, 0.0494], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0357, -0.0702], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1012, -0.1255], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:15 step:2 train loss:0.821072, train acc:77.475, train f1:75.638, train precision:77.723, train recall:75.829, train kappa:76.577
self.nodes_fc[0]: tensor([ 0.0480, -0.0992], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1243,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1171, -0.0322], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1199, 0.0493], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0356, -0.0702], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1011, -0.1255], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:15 step:3 train loss:0.808575, train acc:77.637, train f1:75.965, train precision:78.608, train recall:75.858, train kappa:76.736
self.nodes_fc[0]: tensor([ 0.0479, -0.0992], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1243,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1171, -0.0323], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1198, 0.0495], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0357, -0.0701], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1010, -0.1255], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:15 step:4 train loss:0.807393, train acc:77.792, train f1:75.704, train precision:78.806, train recall:75.406, train kappa:76.898
self.nodes_fc[0]: tensor([ 0.0479, -0.0992], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1243,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1171, -0.0322], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1197, 0.0496], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0356, -0.0701], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1010, -0.1255], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:15 step:5 train loss:0.817493, train acc:77.176, train f1:75.521, train precision:80.055, train recall:74.920, train kappa:76.258
self.nodes_fc[0]: tensor([ 0.0479, -0.0992], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1243,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1171, -0.0321], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1195, 0.0497], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0356, -0.0701], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1010, -0.1255], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:15 step:6 train loss:0.817389, train acc:77.405, train f1:75.897, train precision:78.491, train recall:75.131, train kappa:76.469
self.nodes_fc[0]: tensor([ 0.0480, -0.0993], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1243,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1172, -0.0318], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1194, 0.0498], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0356, -0.0702], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1009, -0.1256], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:15 step:7 train loss:0.815883, train acc:77.512, train f1:75.852, train precision:78.517, train recall:75.180, train kappa:76.607
self.nodes_fc[0]: tensor([ 0.0481, -0.0994], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1243,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1174, -0.0316], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1195, 0.0500], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0357, -0.0701], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1009, -0.1256], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:15 step:8 train loss:0.814757, train acc:77.386, train f1:75.842, train precision:78.242, train recall:75.751, train kappa:76.464
self.nodes_fc[0]: tensor([ 0.0480, -0.0994], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1243,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1175, -0.0313], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1196, 0.0501], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0357, -0.0700], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1009, -0.1257], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:15 step:9 train loss:0.814582, train acc:77.374, train f1:75.619, train precision:78.238, train recall:75.316, train kappa:76.479
self.nodes_fc[0]: tensor([ 0.0480, -0.0994], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1243,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1176, -0.0311], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1197, 0.0502], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0356, -0.0699], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1008, -0.1257], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:15 step:10 train loss:0.810875, train acc:77.280, train f1:75.775, train precision:77.988, train recall:75.761, train kappa:76.347
self.nodes_fc[0]: tensor([ 0.0479, -0.0994], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1243,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1177, -0.0309], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1196, 0.0503], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0356, -0.0697], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1008, -0.1258], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:15 step:11 train loss:0.787200, train acc:78.757, train f1:76.746, train precision:79.475, train recall:76.455, train kappa:77.927
self.nodes_fc[0]: tensor([ 0.0479, -0.0993], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1243,  0.0397], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1177, -0.0306], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1195, 0.0506], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0356, -0.0696], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1008, -0.1258], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0479, -0.0993], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1243,  0.0397], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1177, -0.0306], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1195, 0.0506], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0356, -0.0696], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1008, -0.1258], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:15        valid loss:0.775919, valid acc:78.983, valid f1:54.274, valid precision:51.894, valid recall:69.165, valid kappa:76.379
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0479, -0.0993], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1243,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1177, -0.0306], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1195, 0.0506], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0356, -0.0696], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1008, -0.1258], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:16 step:0 train loss:0.782163, train acc:78.250, train f1:76.536, train precision:79.688, train recall:76.385, train kappa:77.368
self.nodes_fc[0]: tensor([ 0.0478, -0.0992], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1244,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1178, -0.0304], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1194, 0.0509], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0356, -0.0694], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1008, -0.1259], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:16 step:1 train loss:0.781206, train acc:78.275, train f1:76.560, train precision:81.061, train recall:76.069, train kappa:77.416
self.nodes_fc[0]: tensor([ 0.0477, -0.0993], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1244,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1178, -0.0301], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1193, 0.0511], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0355, -0.0692], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1008, -0.1260], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:16 step:2 train loss:0.796152, train acc:77.808, train f1:76.136, train precision:79.429, train recall:75.841, train kappa:76.904
self.nodes_fc[0]: tensor([ 0.0476, -0.0994], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1245,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1179, -0.0300], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1191, 0.0513], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0355, -0.0690], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1008, -0.1260], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:16 step:3 train loss:0.792729, train acc:77.875, train f1:76.298, train precision:81.232, train recall:75.796, train kappa:76.994
self.nodes_fc[0]: tensor([ 0.0475, -0.0995], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1246,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1179, -0.0298], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1191, 0.0515], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0354, -0.0688], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1008, -0.1261], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:16 step:4 train loss:0.792473, train acc:77.927, train f1:76.384, train precision:79.570, train recall:76.021, train kappa:77.045
self.nodes_fc[0]: tensor([ 0.0474, -0.0995], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1247,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1179, -0.0296], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1192, 0.0517], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0353, -0.0687], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1007, -0.1261], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:16 step:5 train loss:0.788530, train acc:77.780, train f1:76.065, train precision:78.427, train recall:75.929, train kappa:76.885
self.nodes_fc[0]: tensor([ 0.0474, -0.0996], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1248,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1179, -0.0295], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1193, 0.0519], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0353, -0.0686], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1007, -0.1262], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:16 step:6 train loss:0.793688, train acc:78.156, train f1:76.543, train precision:79.057, train recall:76.551, train kappa:77.297
self.nodes_fc[0]: tensor([ 0.0474, -0.0995], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1249,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1180, -0.0295], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1195, 0.0521], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0352, -0.0686], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1007, -0.1262], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:16 step:7 train loss:0.783553, train acc:78.094, train f1:76.533, train precision:79.044, train recall:76.472, train kappa:77.219
self.nodes_fc[0]: tensor([ 0.0473, -0.0994], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1249,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1180, -0.0296], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1197, 0.0522], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0352, -0.0686], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1007, -0.1263], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:16 step:8 train loss:0.784759, train acc:77.942, train f1:76.396, train precision:79.348, train recall:75.885, train kappa:77.044
self.nodes_fc[0]: tensor([ 0.0473, -0.0992], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1249,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1180, -0.0297], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1199, 0.0523], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0352, -0.0686], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1007, -0.1263], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:16 step:9 train loss:0.782930, train acc:78.329, train f1:76.702, train precision:79.604, train recall:76.205, train kappa:77.445
self.nodes_fc[0]: tensor([ 0.0473, -0.0991], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1249,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1180, -0.0299], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1201, 0.0523], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0352, -0.0686], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1007, -0.1263], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:16 step:10 train loss:0.773093, train acc:78.424, train f1:76.562, train precision:79.766, train recall:75.997, train kappa:77.538
self.nodes_fc[0]: tensor([ 0.0474, -0.0990], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1249,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1182, -0.0300], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1202, 0.0522], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0352, -0.0687], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1006, -0.1261], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:16 step:11 train loss:0.788146, train acc:77.975, train f1:75.708, train precision:77.866, train recall:75.834, train kappa:77.077
self.nodes_fc[0]: tensor([ 0.0474, -0.0990], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1248,  0.0401], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1182, -0.0298], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1202, 0.0521], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0352, -0.0688], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1005, -0.1259], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0474, -0.0990], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1248,  0.0401], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1182, -0.0298], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1202, 0.0521], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0352, -0.0688], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1005, -0.1259], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:16        valid loss:0.759336, valid acc:79.391, valid f1:55.137, valid precision:52.721, valid recall:69.500, valid kappa:76.836
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0474, -0.0990], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1248,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1182, -0.0298], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1202, 0.0521], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0352, -0.0688], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1005, -0.1259], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:17 step:0 train loss:0.765634, train acc:78.644, train f1:77.337, train precision:81.029, train recall:76.921, train kappa:77.793
self.nodes_fc[0]: tensor([ 0.0474, -0.0989], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1248,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1183, -0.0297], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1202, 0.0521], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0351, -0.0689], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1004, -0.1258], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:17 step:1 train loss:0.760130, train acc:78.809, train f1:77.132, train precision:80.157, train recall:76.935, train kappa:77.947
self.nodes_fc[0]: tensor([ 0.0474, -0.0988], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1247,  0.0402], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1183, -0.0296], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1201, 0.0521], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0350, -0.0689], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1002, -0.1257], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:17 step:2 train loss:0.776451, train acc:78.217, train f1:76.467, train precision:79.963, train recall:76.225, train kappa:77.335
self.nodes_fc[0]: tensor([ 0.0474, -0.0988], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1246,  0.0402], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1183, -0.0296], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1201, 0.0521], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0349, -0.0690], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1001, -0.1256], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:17 step:3 train loss:0.755152, train acc:78.888, train f1:77.428, train precision:79.997, train recall:77.459, train kappa:78.031
self.nodes_fc[0]: tensor([ 0.0474, -0.0987], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1244,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1182, -0.0295], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1201, 0.0521], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0689], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1000, -0.1255], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:17 step:4 train loss:0.780955, train acc:78.226, train f1:76.887, train precision:79.677, train recall:76.570, train kappa:77.347
self.nodes_fc[0]: tensor([ 0.0475, -0.0985], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1243,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1181, -0.0295], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1202, 0.0520], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0690], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0999, -0.1254], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:17 step:5 train loss:0.758759, train acc:78.665, train f1:76.870, train precision:79.684, train recall:76.724, train kappa:77.800
self.nodes_fc[0]: tensor([ 0.0476, -0.0985], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1242,  0.0404], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1180, -0.0295], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1201, 0.0520], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0689], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0998, -0.1253], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:17 step:6 train loss:0.768219, train acc:78.326, train f1:77.032, train precision:80.250, train recall:76.605, train kappa:77.460
self.nodes_fc[0]: tensor([ 0.0476, -0.0986], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1242,  0.0404], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1179, -0.0295], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1201, 0.0519], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0689], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0997, -0.1252], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:17 step:7 train loss:0.749781, train acc:78.897, train f1:77.226, train precision:79.952, train recall:77.081, train kappa:78.047
self.nodes_fc[0]: tensor([ 0.0476, -0.0986], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1242,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1180, -0.0295], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1200, 0.0519], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0346, -0.0690], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0996, -0.1251], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:17 step:8 train loss:0.772490, train acc:78.320, train f1:76.846, train precision:79.261, train recall:76.635, train kappa:77.484
self.nodes_fc[0]: tensor([ 0.0475, -0.0987], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1243,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1181, -0.0296], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1200, 0.0518], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0346, -0.0690], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0995, -0.1250], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:17 step:9 train loss:0.763179, train acc:78.766, train f1:77.166, train precision:79.509, train recall:77.159, train kappa:77.941
self.nodes_fc[0]: tensor([ 0.0475, -0.0990], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1245,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1182, -0.0296], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1199, 0.0519], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0690], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0994, -0.1248], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:17 step:10 train loss:0.747134, train acc:79.037, train f1:77.272, train precision:79.502, train recall:77.206, train kappa:78.198
self.nodes_fc[0]: tensor([ 0.0475, -0.0992], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1246,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1183, -0.0298], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1197, 0.0519], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0691], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0993, -0.1247], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:17 step:11 train loss:0.736917, train acc:79.297, train f1:77.446, train precision:79.396, train recall:77.696, train kappa:78.491
self.nodes_fc[0]: tensor([ 0.0475, -0.0994], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1246,  0.0403], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1184, -0.0298], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1194, 0.0519], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0693], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0992, -0.1245], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0475, -0.0994], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1246,  0.0403], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1184, -0.0298], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1194, 0.0519], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0693], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0992, -0.1245], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:17        valid loss:0.753923, valid acc:79.706, valid f1:55.423, valid precision:52.677, valid recall:69.511, valid kappa:77.172
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0475, -0.0994], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1246,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1184, -0.0298], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1194, 0.0519], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0693], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0992, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:18 step:0 train loss:0.749322, train acc:78.925, train f1:77.484, train precision:80.496, train recall:77.019, train kappa:78.091
self.nodes_fc[0]: tensor([ 0.0475, -0.0995], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1245,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1184, -0.0298], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1191, 0.0520], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0695], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0991, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:18 step:1 train loss:0.748389, train acc:78.955, train f1:77.289, train precision:79.972, train recall:76.927, train kappa:78.110
self.nodes_fc[0]: tensor([ 0.0475, -0.0995], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1244,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1184, -0.0298], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1188, 0.0520], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0698], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0990, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:18 step:2 train loss:0.760525, train acc:78.610, train f1:77.066, train precision:80.463, train recall:76.866, train kappa:77.755
self.nodes_fc[0]: tensor([ 0.0475, -0.0994], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1244,  0.0404], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1183, -0.0299], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1184, 0.0521], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0699], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0989, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:18 step:3 train loss:0.740501, train acc:79.321, train f1:77.807, train precision:80.486, train recall:77.567, train kappa:78.497
self.nodes_fc[0]: tensor([ 0.0474, -0.0994], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1244,  0.0404], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1184, -0.0299], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1180, 0.0520], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0699], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0988, -0.1240], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:18 step:4 train loss:0.742820, train acc:79.218, train f1:77.634, train precision:79.812, train recall:77.541, train kappa:78.369
self.nodes_fc[0]: tensor([ 0.0473, -0.0994], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1246,  0.0404], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1184, -0.0299], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1175, 0.0519], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0698], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0987, -0.1239], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:18 step:5 train loss:0.739858, train acc:79.080, train f1:77.476, train precision:80.150, train recall:77.561, train kappa:78.250
self.nodes_fc[0]: tensor([ 0.0472, -0.0995], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1248,  0.0404], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1186, -0.0298], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1171, 0.0518], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0346, -0.0697], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0987, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:18 step:6 train loss:0.740304, train acc:79.251, train f1:77.760, train precision:80.438, train recall:77.591, train kappa:78.423
self.nodes_fc[0]: tensor([ 0.0471, -0.0996], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1250,  0.0405], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1187, -0.0298], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1167, 0.0516], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0346, -0.0695], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0986, -0.1237], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:18 step:7 train loss:0.753850, train acc:78.729, train f1:77.310, train precision:79.793, train recall:77.008, train kappa:77.894
self.nodes_fc[0]: tensor([ 0.0470, -0.0996], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1252,  0.0405], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1189, -0.0299], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0516], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0693], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0986, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:18 step:8 train loss:0.745345, train acc:78.928, train f1:77.435, train precision:80.106, train recall:77.162, train kappa:78.076
self.nodes_fc[0]: tensor([ 0.0469, -0.0996], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1254,  0.0406], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1190, -0.0299], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0516], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0689], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0985, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:18 step:9 train loss:0.743577, train acc:79.147, train f1:77.554, train precision:80.068, train recall:77.508, train kappa:78.317
self.nodes_fc[0]: tensor([ 0.0467, -0.0995], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1255,  0.0406], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1191, -0.0300], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0517], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0686], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0984, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:18 step:10 train loss:0.733694, train acc:79.309, train f1:77.720, train precision:80.446, train recall:77.476, train kappa:78.480
self.nodes_fc[0]: tensor([ 0.0465, -0.0995], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1256,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1192, -0.0301], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1159, 0.0518], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0349, -0.0684], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0984, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:18 step:11 train loss:0.754803, train acc:78.564, train f1:77.096, train precision:80.369, train recall:76.665, train kappa:77.683
self.nodes_fc[0]: tensor([ 0.0465, -0.0995], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1256,  0.0407], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1194, -0.0301], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1158, 0.0520], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0350, -0.0683], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0984, -0.1233], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0465, -0.0995], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1256,  0.0407], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1194, -0.0301], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1158, 0.0520], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0350, -0.0683], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0984, -0.1233], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:18        valid loss:0.745997, valid acc:79.948, valid f1:55.478, valid precision:52.671, valid recall:69.692, valid kappa:77.452
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0465, -0.0995], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1256,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1194, -0.0301], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1158, 0.0520], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0350, -0.0683], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0984, -0.1233], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:19 step:0 train loss:0.731452, train acc:79.507, train f1:77.779, train precision:80.105, train recall:77.814, train kappa:78.692
self.nodes_fc[0]: tensor([ 0.0464, -0.0996], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1255,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1195, -0.0301], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1156, 0.0521], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0350, -0.0681], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0983, -0.1232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:19 step:1 train loss:0.729807, train acc:79.294, train f1:77.764, train precision:80.030, train recall:77.708, train kappa:78.474
self.nodes_fc[0]: tensor([ 0.0464, -0.0997], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1256,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1197, -0.0301], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1156, 0.0520], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0351, -0.0680], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0983, -0.1232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:19 step:2 train loss:0.726561, train acc:79.410, train f1:77.699, train precision:80.182, train recall:77.642, train kappa:78.576
self.nodes_fc[0]: tensor([ 0.0464, -0.0998], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1257,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1199, -0.0300], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1156, 0.0520], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0351, -0.0678], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0983, -0.1231], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:19 step:3 train loss:0.715924, train acc:79.752, train f1:78.426, train precision:80.793, train recall:78.306, train kappa:78.951
self.nodes_fc[0]: tensor([ 0.0462, -0.0998], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1257,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1202, -0.0299], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1158, 0.0519], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0352, -0.0676], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0983, -0.1231], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:19 step:4 train loss:0.723501, train acc:79.587, train f1:77.972, train precision:79.930, train recall:78.067, train kappa:78.780
self.nodes_fc[0]: tensor([ 0.0461, -0.0998], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1258,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1203, -0.0298], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1159, 0.0519], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0351, -0.0675], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0983, -0.1231], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:19 step:5 train loss:0.733698, train acc:79.492, train f1:78.039, train precision:80.290, train recall:77.759, train kappa:78.685
self.nodes_fc[0]: tensor([ 0.0459, -0.0998], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1259,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1205, -0.0297], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0519], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0351, -0.0675], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0983, -0.1230], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:19 step:6 train loss:0.711107, train acc:79.880, train f1:78.492, train precision:81.641, train recall:78.076, train kappa:79.065
self.nodes_fc[0]: tensor([ 0.0458, -0.0996], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1259,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1207, -0.0297], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0519], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0350, -0.0674], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0982, -0.1229], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:19 step:7 train loss:0.722137, train acc:79.855, train f1:78.486, train precision:80.824, train recall:78.415, train kappa:79.057
self.nodes_fc[0]: tensor([ 0.0456, -0.0996], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1260,  0.0408], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1210, -0.0297], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0519], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0350, -0.0672], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0982, -0.1229], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:19 step:8 train loss:0.717485, train acc:79.797, train f1:78.264, train precision:80.855, train recall:78.152, train kappa:78.999
self.nodes_fc[0]: tensor([ 0.0454, -0.0995], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1260,  0.0409], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1213, -0.0296], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0518], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0350, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0982, -0.1228], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:19 step:9 train loss:0.727193, train acc:79.498, train f1:77.981, train precision:80.419, train recall:77.964, train kappa:78.685
self.nodes_fc[0]: tensor([ 0.0453, -0.0995], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1261,  0.0409], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1215, -0.0294], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0517], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0350, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0982, -0.1228], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:19 step:10 train loss:0.721224, train acc:79.715, train f1:78.300, train precision:80.664, train recall:78.318, train kappa:78.907
self.nodes_fc[0]: tensor([ 0.0452, -0.0994], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1262,  0.0410], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1218, -0.0289], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0517], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0351, -0.0668], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0982, -0.1227], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:19 step:11 train loss:0.720719, train acc:79.510, train f1:78.120, train precision:80.409, train recall:78.103, train kappa:78.711
self.nodes_fc[0]: tensor([ 0.0450, -0.0994], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1264,  0.0410], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1220, -0.0286], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0519], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0351, -0.0666], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0982, -0.1227], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0450, -0.0994], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1264,  0.0410], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1220, -0.0286], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0519], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0351, -0.0666], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0982, -0.1227], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:19        valid loss:0.731135, valid acc:80.297, valid f1:56.138, valid precision:53.632, valid recall:70.094, valid kappa:77.831
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0450, -0.0994], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1264,  0.0410], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1220, -0.0286], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0519], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0351, -0.0666], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0982, -0.1227], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:20 step:0 train loss:0.705051, train acc:79.782, train f1:78.150, train precision:80.496, train recall:78.013, train kappa:78.964
self.nodes_fc[0]: tensor([ 0.0449, -0.0992], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1264,  0.0410], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1222, -0.0283], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0521], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0350, -0.0664], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0981, -0.1227], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:20 step:1 train loss:0.698938, train acc:80.359, train f1:78.933, train precision:81.913, train recall:78.656, train kappa:79.587
self.nodes_fc[0]: tensor([ 0.0447, -0.0991], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1265,  0.0411], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1224, -0.0281], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0523], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0349, -0.0663], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0981, -0.1226], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:20 step:2 train loss:0.715384, train acc:79.590, train f1:78.047, train precision:80.909, train recall:77.789, train kappa:78.785
self.nodes_fc[0]: tensor([ 0.0447, -0.0991], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1265,  0.0411], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1225, -0.0281], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0525], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0349, -0.0662], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0980, -0.1226], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:20 step:3 train loss:0.711792, train acc:79.599, train f1:78.168, train precision:80.878, train recall:77.970, train kappa:78.762
self.nodes_fc[0]: tensor([ 0.0446, -0.0989], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1264,  0.0411], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1224, -0.0282], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0528], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0662], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0980, -0.1225], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:20 step:4 train loss:0.690818, train acc:80.219, train f1:78.593, train precision:81.320, train recall:78.477, train kappa:79.437
self.nodes_fc[0]: tensor([ 0.0446, -0.0988], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1264,  0.0411], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1224, -0.0283], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0531], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0662], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0980, -0.1225], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:20 step:5 train loss:0.707586, train acc:79.846, train f1:78.390, train precision:80.924, train recall:78.247, train kappa:79.029
self.nodes_fc[0]: tensor([ 0.0446, -0.0986], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1264,  0.0412], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1224, -0.0285], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0534], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0661], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0980, -0.1225], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:20 step:6 train loss:0.704149, train acc:79.889, train f1:78.487, train precision:81.620, train recall:78.094, train kappa:79.091
self.nodes_fc[0]: tensor([ 0.0445, -0.0985], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1265,  0.0412], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1224, -0.0285], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1167, 0.0536], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0660], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0980, -0.1224], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:20 step:7 train loss:0.717260, train acc:79.669, train f1:78.185, train precision:80.420, train recall:78.461, train kappa:78.862
self.nodes_fc[0]: tensor([ 0.0445, -0.0984], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1267,  0.0412], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1225, -0.0285], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0540], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0660], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0980, -0.1224], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:20 step:8 train loss:0.705708, train acc:79.935, train f1:78.508, train precision:80.636, train recall:78.757, train kappa:79.137
self.nodes_fc[0]: tensor([ 0.0445, -0.0983], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1269,  0.0412], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1226, -0.0284], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0544], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0659], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0980, -0.1223], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:20 step:9 train loss:0.705935, train acc:80.142, train f1:78.555, train precision:80.704, train recall:78.773, train kappa:79.380
self.nodes_fc[0]: tensor([ 0.0445, -0.0983], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0411], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1227, -0.0283], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0547], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0659], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0980, -0.1223], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:20 step:10 train loss:0.705879, train acc:79.898, train f1:78.311, train precision:80.479, train recall:78.531, train kappa:79.111
self.nodes_fc[0]: tensor([ 0.0445, -0.0984], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0411], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1227, -0.0283], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0549], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0659], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0980, -0.1223], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:20 step:11 train loss:0.689007, train acc:80.427, train f1:79.059, train precision:82.621, train recall:78.598, train kappa:79.652
self.nodes_fc[0]: tensor([ 0.0444, -0.0987], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0411], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1226, -0.0282], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0551], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0660], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0981, -0.1222], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0444, -0.0987], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0411], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1226, -0.0282], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0551], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0660], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0981, -0.1222], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:20        valid loss:0.727172, valid acc:80.416, valid f1:56.677, valid precision:54.036, valid recall:69.700, valid kappa:77.970
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0444, -0.0987], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0411], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1226, -0.0282], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0551], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0660], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0981, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:21 step:0 train loss:0.696781, train acc:80.286, train f1:78.962, train precision:82.815, train recall:78.589, train kappa:79.504
self.nodes_fc[0]: tensor([ 0.0444, -0.0988], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0411], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1224, -0.0281], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0552], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0660], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0981, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:21 step:1 train loss:0.692489, train acc:80.319, train f1:79.261, train precision:82.375, train recall:78.841, train kappa:79.542
self.nodes_fc[0]: tensor([ 0.0443, -0.0988], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0412], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1222, -0.0281], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0552], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0660], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0981, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:21 step:2 train loss:0.688777, train acc:80.475, train f1:79.084, train precision:81.595, train recall:78.887, train kappa:79.684
self.nodes_fc[0]: tensor([ 0.0442, -0.0987], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0413], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1220, -0.0281], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1156, 0.0553], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0660], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0982, -0.1221], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:21 step:3 train loss:0.694110, train acc:80.161, train f1:78.669, train precision:80.569, train recall:78.667, train kappa:79.371
self.nodes_fc[0]: tensor([ 0.0441, -0.0985], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0412], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1219, -0.0280], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1153, 0.0555], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0349, -0.0661], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0982, -0.1221], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:21 step:4 train loss:0.696812, train acc:79.938, train f1:78.636, train precision:81.184, train recall:78.718, train kappa:79.138
self.nodes_fc[0]: tensor([ 0.0439, -0.0983], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0411], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1218, -0.0279], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1151, 0.0555], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0349, -0.0663], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0982, -0.1221], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:21 step:5 train loss:0.684718, train acc:80.643, train f1:78.967, train precision:80.924, train recall:79.182, train kappa:79.871
self.nodes_fc[0]: tensor([ 0.0437, -0.0981], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0410], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1217, -0.0279], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1151, 0.0556], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0349, -0.0665], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0982, -0.1221], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:21 step:6 train loss:0.699292, train acc:80.130, train f1:78.647, train precision:81.376, train recall:78.845, train kappa:79.349
self.nodes_fc[0]: tensor([ 0.0435, -0.0980], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0410], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1217, -0.0278], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1151, 0.0557], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0349, -0.0667], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0982, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:21 step:7 train loss:0.683888, train acc:80.508, train f1:79.031, train precision:81.243, train recall:78.953, train kappa:79.748
self.nodes_fc[0]: tensor([ 0.0434, -0.0980], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0409], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1216, -0.0278], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1151, 0.0558], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0349, -0.0669], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0982, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:21 step:8 train loss:0.681068, train acc:80.573, train f1:79.184, train precision:81.822, train recall:79.014, train kappa:79.796
self.nodes_fc[0]: tensor([ 0.0432, -0.0979], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1269,  0.0409], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1215, -0.0278], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1152, 0.0558], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0982, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:21 step:9 train loss:0.673950, train acc:80.801, train f1:79.328, train precision:81.781, train recall:79.038, train kappa:80.041
self.nodes_fc[0]: tensor([ 0.0431, -0.0979], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1269,  0.0409], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1215, -0.0278], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1154, 0.0558], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0671], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0981, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:21 step:10 train loss:0.691151, train acc:80.408, train f1:79.122, train precision:81.783, train recall:78.921, train kappa:79.631
self.nodes_fc[0]: tensor([ 0.0431, -0.0980], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1269,  0.0410], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1214, -0.0278], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1156, 0.0558], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0672], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0981, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:21 step:11 train loss:0.668358, train acc:80.919, train f1:79.555, train precision:82.167, train recall:79.355, train kappa:80.175
self.nodes_fc[0]: tensor([ 0.0431, -0.0982], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0410], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1216, -0.0278], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0558], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0674], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0981, -0.1222], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0431, -0.0982], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0410], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1216, -0.0278], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0558], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0674], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0981, -0.1222], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:21        valid loss:0.726062, valid acc:80.371, valid f1:56.690, valid precision:53.674, valid recall:70.102, valid kappa:77.920
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0431, -0.0982], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0410], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1216, -0.0278], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0558], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0674], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0981, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:22 step:0 train loss:0.684755, train acc:80.310, train f1:79.154, train precision:81.554, train recall:79.214, train kappa:79.527
self.nodes_fc[0]: tensor([ 0.0431, -0.0984], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0410], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1217, -0.0277], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0557], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0347, -0.0675], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0981, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:22 step:1 train loss:0.674746, train acc:80.820, train f1:79.217, train precision:81.793, train recall:79.059, train kappa:80.053
self.nodes_fc[0]: tensor([ 0.0431, -0.0984], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0411], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1217, -0.0277], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1168, 0.0556], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0676], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0981, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:22 step:2 train loss:0.682771, train acc:80.533, train f1:79.253, train precision:82.136, train recall:78.925, train kappa:79.779
self.nodes_fc[0]: tensor([ 0.0432, -0.0984], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0411], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1219, -0.0276], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1170, 0.0555], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0677], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0981, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:22 step:3 train loss:0.677349, train acc:80.444, train f1:79.103, train precision:81.998, train recall:78.928, train kappa:79.677
self.nodes_fc[0]: tensor([ 0.0433, -0.0985], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1269,  0.0411], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1219, -0.0276], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1173, 0.0556], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0348, -0.0679], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0980, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:22 step:4 train loss:0.673702, train acc:80.807, train f1:79.334, train precision:82.147, train recall:79.276, train kappa:80.042
self.nodes_fc[0]: tensor([ 0.0435, -0.0986], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1268,  0.0411], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1219, -0.0276], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1177, 0.0556], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0349, -0.0680], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0980, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:22 step:5 train loss:0.661429, train acc:80.710, train f1:79.405, train precision:81.867, train recall:79.175, train kappa:79.957
self.nodes_fc[0]: tensor([ 0.0437, -0.0986], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1267,  0.0412], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1218, -0.0276], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1181, 0.0556], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0350, -0.0680], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0980, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:22 step:6 train loss:0.670819, train acc:80.807, train f1:79.386, train precision:82.329, train recall:79.316, train kappa:80.042
self.nodes_fc[0]: tensor([ 0.0438, -0.0987], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1265,  0.0412], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1216, -0.0276], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1183, 0.0557], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0350, -0.0681], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0979, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:22 step:7 train loss:0.670145, train acc:81.052, train f1:79.436, train precision:81.553, train recall:79.533, train kappa:80.297
self.nodes_fc[0]: tensor([ 0.0440, -0.0987], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1264,  0.0412], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1215, -0.0277], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1185, 0.0559], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0351, -0.0682], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0979, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:22 step:8 train loss:0.669984, train acc:80.762, train f1:79.466, train precision:81.286, train recall:79.771, train kappa:79.996
self.nodes_fc[0]: tensor([ 0.0441, -0.0986], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1264,  0.0413], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1215, -0.0277], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1186, 0.0560], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0351, -0.0683], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0978, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:22 step:9 train loss:0.665070, train acc:81.052, train f1:79.389, train precision:81.118, train recall:79.697, train kappa:80.305
self.nodes_fc[0]: tensor([ 0.0443, -0.0985], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1265,  0.0413], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1216, -0.0275], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1187, 0.0561], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0351, -0.0683], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0977, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:22 step:10 train loss:0.673082, train acc:80.838, train f1:79.548, train precision:82.091, train recall:79.466, train kappa:80.087
self.nodes_fc[0]: tensor([ 0.0444, -0.0984], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1267,  0.0413], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1220, -0.0272], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1188, 0.0561], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0350, -0.0684], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0977, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:22 step:11 train loss:0.652395, train acc:80.957, train f1:79.321, train precision:82.509, train recall:79.310, train kappa:80.200
self.nodes_fc[0]: tensor([ 0.0445, -0.0984], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1268,  0.0414], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1222, -0.0269], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1188, 0.0562], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0350, -0.0685], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0976, -0.1222], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0445, -0.0984], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1268,  0.0414], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1222, -0.0269], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1188, 0.0562], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0350, -0.0685], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0976, -0.1222], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:22        valid loss:0.710761, valid acc:80.945, valid f1:57.143, valid precision:54.036, valid recall:70.016, valid kappa:78.560
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0445, -0.0984], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1268,  0.0414], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1222, -0.0269], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1188, 0.0562], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0350, -0.0685], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0976, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:23 step:0 train loss:0.660146, train acc:81.055, train f1:79.741, train precision:81.824, train recall:79.603, train kappa:80.304
self.nodes_fc[0]: tensor([ 0.0446, -0.0985], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1269,  0.0415], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1224, -0.0267], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1189, 0.0562], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0349, -0.0685], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0976, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:23 step:1 train loss:0.654149, train acc:81.348, train f1:80.140, train precision:82.885, train recall:79.756, train kappa:80.601
self.nodes_fc[0]: tensor([ 0.0447, -0.0986], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0416], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1225, -0.0265], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1189, 0.0563], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0349, -0.0686], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0975, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:23 step:2 train loss:0.672650, train acc:80.527, train f1:79.454, train precision:81.975, train recall:79.052, train kappa:79.750
self.nodes_fc[0]: tensor([ 0.0447, -0.0987], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0416], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1226, -0.0264], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1188, 0.0565], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0349, -0.0687], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0975, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:23 step:3 train loss:0.646420, train acc:81.305, train f1:80.063, train precision:82.333, train recall:79.615, train kappa:80.569
self.nodes_fc[0]: tensor([ 0.0447, -0.0988], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0416], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1226, -0.0263], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1186, 0.0566], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0349, -0.0688], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0975, -0.1222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:23 step:4 train loss:0.679308, train acc:80.359, train f1:79.394, train precision:81.335, train recall:79.452, train kappa:79.574
self.nodes_fc[0]: tensor([ 0.0447, -0.0987], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0416], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1225, -0.0263], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1185, 0.0566], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0349, -0.0688], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0975, -0.1223], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:23 step:5 train loss:0.663740, train acc:80.838, train f1:79.635, train precision:81.428, train recall:79.738, train kappa:80.075
self.nodes_fc[0]: tensor([ 0.0446, -0.0986], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0416], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1224, -0.0264], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1185, 0.0566], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0350, -0.0687], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0974, -0.1223], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:23 step:6 train loss:0.659931, train acc:81.042, train f1:79.570, train precision:81.208, train recall:79.803, train kappa:80.300
self.nodes_fc[0]: tensor([ 0.0446, -0.0985], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0416], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1223, -0.0264], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1185, 0.0566], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0351, -0.0687], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0974, -0.1223], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:23 step:7 train loss:0.660301, train acc:81.171, train f1:79.488, train precision:81.485, train recall:79.695, train kappa:80.427
self.nodes_fc[0]: tensor([ 0.0447, -0.0984], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0416], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1223, -0.0265], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1185, 0.0565], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0353, -0.0687], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0974, -0.1224], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:23 step:8 train loss:0.645613, train acc:81.558, train f1:79.949, train precision:82.211, train recall:79.987, train kappa:80.839
self.nodes_fc[0]: tensor([ 0.0447, -0.0984], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0416], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1221, -0.0264], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1185, 0.0564], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0355, -0.0686], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0973, -0.1224], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:23 step:9 train loss:0.652440, train acc:81.393, train f1:79.808, train precision:82.653, train recall:79.693, train kappa:80.660
self.nodes_fc[0]: tensor([ 0.0448, -0.0985], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0415], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1220, -0.0265], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1183, 0.0562], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0356, -0.0685], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0973, -0.1224], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:23 step:10 train loss:0.659763, train acc:80.978, train f1:79.496, train precision:82.154, train recall:79.408, train kappa:80.235
self.nodes_fc[0]: tensor([ 0.0450, -0.0986], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0415], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1219, -0.0265], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1183, 0.0561], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0358, -0.0684], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0974, -0.1225], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:23 step:11 train loss:0.660383, train acc:80.900, train f1:79.511, train precision:81.653, train recall:79.285, train kappa:80.141
self.nodes_fc[0]: tensor([ 0.0451, -0.0987], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0415], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1219, -0.0266], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1183, 0.0558], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0361, -0.0681], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0974, -0.1225], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0451, -0.0987], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0415], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1219, -0.0266], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1183, 0.0558], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0361, -0.0681], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0974, -0.1225], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:23        valid loss:0.707770, valid acc:81.052, valid f1:56.948, valid precision:53.909, valid recall:70.283, valid kappa:78.685
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0451, -0.0987], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0415], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1219, -0.0266], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1183, 0.0558], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0361, -0.0681], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0974, -0.1225], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:24 step:0 train loss:0.632823, train acc:81.595, train f1:80.392, train precision:83.141, train recall:80.349, train kappa:80.858
self.nodes_fc[0]: tensor([ 0.0453, -0.0988], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0414], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1219, -0.0267], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1183, 0.0555], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0362, -0.0679], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0974, -0.1225], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:24 step:1 train loss:0.663877, train acc:80.838, train f1:79.630, train precision:81.556, train recall:79.615, train kappa:80.086
self.nodes_fc[0]: tensor([ 0.0454, -0.0988], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0413], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1219, -0.0269], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1183, 0.0554], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0364, -0.0677], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0974, -0.1225], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:24 step:2 train loss:0.639781, train acc:81.702, train f1:80.326, train precision:82.877, train recall:80.459, train kappa:80.966
self.nodes_fc[0]: tensor([ 0.0456, -0.0989], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1273,  0.0412], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1219, -0.0270], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1184, 0.0553], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0365, -0.0676], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0974, -0.1225], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:24 step:3 train loss:0.656745, train acc:81.305, train f1:79.861, train precision:82.059, train recall:79.791, train kappa:80.568
self.nodes_fc[0]: tensor([ 0.0458, -0.0991], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1273,  0.0411], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1219, -0.0270], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1186, 0.0554], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0365, -0.0675], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0974, -0.1226], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:24 step:4 train loss:0.649599, train acc:81.372, train f1:79.955, train precision:81.500, train recall:79.807, train kappa:80.640
self.nodes_fc[0]: tensor([ 0.0459, -0.0992], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0409], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1217, -0.0272], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1187, 0.0554], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0367, -0.0675], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0974, -0.1227], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:24 step:5 train loss:0.662150, train acc:81.015, train f1:79.781, train precision:81.824, train recall:79.526, train kappa:80.264
self.nodes_fc[0]: tensor([ 0.0460, -0.0993], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1215, -0.0274], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1189, 0.0554], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0368, -0.0674], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0973, -0.1227], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:24 step:6 train loss:0.638391, train acc:81.769, train f1:80.201, train precision:82.297, train recall:80.169, train kappa:81.049
self.nodes_fc[0]: tensor([ 0.0460, -0.0993], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0405], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1212, -0.0275], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1190, 0.0555], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0369, -0.0674], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0973, -0.1228], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:24 step:7 train loss:0.632535, train acc:81.915, train f1:80.383, train precision:82.937, train recall:80.233, train kappa:81.215
self.nodes_fc[0]: tensor([ 0.0459, -0.0992], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1269,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1211, -0.0276], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1191, 0.0556], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0369, -0.0672], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0972, -0.1229], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:24 step:8 train loss:0.636962, train acc:81.674, train f1:80.118, train precision:82.538, train recall:80.190, train kappa:80.952
self.nodes_fc[0]: tensor([ 0.0459, -0.0993], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0402], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1211, -0.0276], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1190, 0.0558], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0369, -0.0671], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0972, -0.1230], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:24 step:9 train loss:0.636270, train acc:81.766, train f1:80.472, train precision:82.747, train recall:80.557, train kappa:81.052
self.nodes_fc[0]: tensor([ 0.0460, -0.0993], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1212, -0.0276], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1188, 0.0558], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0369, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0972, -0.1231], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:24 step:10 train loss:0.636717, train acc:81.592, train f1:80.046, train precision:82.523, train recall:80.195, train kappa:80.865
self.nodes_fc[0]: tensor([ 0.0461, -0.0994], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1214, -0.0276], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1187, 0.0559], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0369, -0.0669], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0971, -0.1232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:24 step:11 train loss:0.636437, train acc:81.565, train f1:79.993, train precision:81.494, train recall:80.158, train kappa:80.848
self.nodes_fc[0]: tensor([ 0.0462, -0.0995], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1273,  0.0400], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1217, -0.0276], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1187, 0.0558], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0370, -0.0666], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0971, -0.1233], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0462, -0.0995], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1273,  0.0400], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1217, -0.0276], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1187, 0.0558], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0370, -0.0666], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0971, -0.1233], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:24        valid loss:0.716649, valid acc:80.729, valid f1:56.824, valid precision:54.047, valid recall:70.263, valid kappa:78.329
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0462, -0.0995], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1273,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1217, -0.0276], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1187, 0.0558], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0370, -0.0666], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0971, -0.1233], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:25 step:0 train loss:0.637698, train acc:81.589, train f1:80.295, train precision:82.958, train recall:80.515, train kappa:80.862
self.nodes_fc[0]: tensor([ 0.0463, -0.0996], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1274,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1218, -0.0277], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1187, 0.0558], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0371, -0.0664], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0971, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:25 step:1 train loss:0.631839, train acc:81.732, train f1:80.457, train precision:82.834, train recall:80.450, train kappa:81.027
self.nodes_fc[0]: tensor([ 0.0465, -0.0995], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1220, -0.0277], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1184, 0.0558], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0371, -0.0663], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0971, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:25 step:2 train loss:0.638344, train acc:81.479, train f1:80.209, train precision:82.439, train recall:80.296, train kappa:80.757
self.nodes_fc[0]: tensor([ 0.0468, -0.0994], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1221, -0.0277], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1182, 0.0559], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0372, -0.0662], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0971, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:25 step:3 train loss:0.631596, train acc:81.918, train f1:80.428, train precision:83.059, train recall:80.274, train kappa:81.195
self.nodes_fc[0]: tensor([ 0.0469, -0.0992], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1274,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1221, -0.0278], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1180, 0.0560], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0372, -0.0662], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0971, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:25 step:4 train loss:0.638168, train acc:81.528, train f1:80.376, train precision:82.720, train recall:80.347, train kappa:80.800
self.nodes_fc[0]: tensor([ 0.0471, -0.0990], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1273,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1221, -0.0279], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1178, 0.0561], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0372, -0.0662], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0971, -0.1237], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:25 step:5 train loss:0.620681, train acc:82.159, train f1:80.706, train precision:83.217, train recall:80.696, train kappa:81.462
self.nodes_fc[0]: tensor([ 0.0472, -0.0987], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1220, -0.0279], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1178, 0.0562], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0371, -0.0662], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0970, -0.1237], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:25 step:6 train loss:0.628208, train acc:81.796, train f1:80.493, train precision:82.521, train recall:80.339, train kappa:81.085
self.nodes_fc[0]: tensor([ 0.0472, -0.0984], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0396], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1222, -0.0279], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1177, 0.0564], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0371, -0.0661], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0970, -0.1237], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:25 step:7 train loss:0.627681, train acc:81.766, train f1:80.467, train precision:82.477, train recall:80.542, train kappa:81.047
self.nodes_fc[0]: tensor([ 0.0472, -0.0983], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1274,  0.0395], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1224, -0.0276], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1176, 0.0566], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0371, -0.0661], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0970, -0.1237], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:25 step:8 train loss:0.634480, train acc:81.857, train f1:80.625, train precision:82.376, train recall:80.786, train kappa:81.140
self.nodes_fc[0]: tensor([ 0.0472, -0.0981], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0394], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1227, -0.0272], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1176, 0.0568], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0371, -0.0660], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0970, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:25 step:9 train loss:0.624250, train acc:81.860, train f1:80.823, train precision:82.952, train recall:80.592, train kappa:81.143
self.nodes_fc[0]: tensor([ 0.0472, -0.0981], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1276,  0.0394], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1229, -0.0270], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1177, 0.0569], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0371, -0.0659], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0970, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:25 step:10 train loss:0.633870, train acc:81.958, train f1:80.514, train precision:82.822, train recall:80.430, train kappa:81.231
self.nodes_fc[0]: tensor([ 0.0472, -0.0980], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1276,  0.0395], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1231, -0.0269], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1178, 0.0569], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0372, -0.0657], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0970, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:25 step:11 train loss:0.627894, train acc:81.768, train f1:80.668, train precision:83.151, train recall:80.403, train kappa:81.072
self.nodes_fc[0]: tensor([ 0.0471, -0.0980], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0395], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1232, -0.0268], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1179, 0.0569], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0372, -0.0657], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0970, -0.1236], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0471, -0.0980], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0395], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1232, -0.0268], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1179, 0.0569], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0372, -0.0657], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0970, -0.1236], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:25        valid loss:0.707567, valid acc:80.839, valid f1:57.265, valid precision:54.243, valid recall:70.223, valid kappa:78.448
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0471, -0.0980], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0395], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1232, -0.0268], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1179, 0.0569], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0372, -0.0657], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0970, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:26 step:0 train loss:0.620749, train acc:81.946, train f1:80.841, train precision:82.693, train recall:80.803, train kappa:81.233
self.nodes_fc[0]: tensor([ 0.0470, -0.0979], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1274,  0.0396], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1232, -0.0267], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1181, 0.0569], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0372, -0.0657], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0971, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:26 step:1 train loss:0.615616, train acc:82.257, train f1:80.821, train precision:82.346, train recall:81.308, train kappa:81.561
self.nodes_fc[0]: tensor([ 0.0471, -0.0978], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1273,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1232, -0.0266], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1183, 0.0569], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0373, -0.0657], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0970, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:26 step:2 train loss:0.625032, train acc:81.952, train f1:80.683, train precision:82.717, train recall:81.040, train kappa:81.250
self.nodes_fc[0]: tensor([ 0.0471, -0.0977], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1232, -0.0266], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1185, 0.0569], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0373, -0.0658], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0970, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:26 step:3 train loss:0.622640, train acc:81.958, train f1:80.597, train precision:82.310, train recall:80.812, train kappa:81.249
self.nodes_fc[0]: tensor([ 0.0472, -0.0976], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1233, -0.0264], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1186, 0.0570], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0373, -0.0659], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0970, -0.1237], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:26 step:4 train loss:0.615591, train acc:82.120, train f1:80.919, train precision:83.312, train recall:80.673, train kappa:81.427
self.nodes_fc[0]: tensor([ 0.0472, -0.0976], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1234, -0.0262], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1187, 0.0572], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0373, -0.0660], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0970, -0.1237], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:26 step:5 train loss:0.619210, train acc:82.053, train f1:80.351, train precision:83.464, train recall:80.099, train kappa:81.330
self.nodes_fc[0]: tensor([ 0.0472, -0.0977], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1237, -0.0260], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1187, 0.0573], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0372, -0.0661], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0970, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:26 step:6 train loss:0.616571, train acc:82.150, train f1:80.830, train precision:82.845, train recall:80.631, train kappa:81.440
self.nodes_fc[0]: tensor([ 0.0472, -0.0977], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1239, -0.0256], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1188, 0.0575], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0372, -0.0662], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0969, -0.1239], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:26 step:7 train loss:0.624825, train acc:81.912, train f1:80.733, train precision:83.258, train recall:80.615, train kappa:81.194
self.nodes_fc[0]: tensor([ 0.0473, -0.0978], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1242, -0.0253], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1188, 0.0576], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0371, -0.0663], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0969, -0.1239], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:26 step:8 train loss:0.615579, train acc:82.001, train f1:80.766, train precision:83.500, train recall:80.505, train kappa:81.304
self.nodes_fc[0]: tensor([ 0.0472, -0.0976], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1244, -0.0251], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1188, 0.0578], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0371, -0.0664], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0968, -0.1239], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:26 step:9 train loss:0.617222, train acc:82.104, train f1:80.683, train precision:83.584, train recall:80.648, train kappa:81.405
self.nodes_fc[0]: tensor([ 0.0471, -0.0974], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0405], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1245, -0.0250], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1189, 0.0580], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0370, -0.0665], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0967, -0.1240], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:26 step:10 train loss:0.623949, train acc:82.141, train f1:81.000, train precision:82.769, train recall:81.472, train kappa:81.453
self.nodes_fc[0]: tensor([ 0.0471, -0.0973], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1269,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1246, -0.0249], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1189, 0.0582], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0370, -0.0665], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0967, -0.1240], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:26 step:11 train loss:0.585789, train acc:82.897, train f1:81.336, train precision:82.635, train recall:81.949, train kappa:82.221
self.nodes_fc[0]: tensor([ 0.0470, -0.0974], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1268,  0.0409], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1245, -0.0249], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1190, 0.0583], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0371, -0.0665], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0967, -0.1241], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0470, -0.0974], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1268,  0.0409], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1245, -0.0249], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1190, 0.0583], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0371, -0.0665], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0967, -0.1241], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:26        valid loss:0.702515, valid acc:81.240, valid f1:57.218, valid precision:54.298, valid recall:70.115, valid kappa:78.892
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0470, -0.0974], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1268,  0.0409], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1245, -0.0249], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1190, 0.0583], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0371, -0.0665], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0967, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:27 step:0 train loss:0.614466, train acc:82.077, train f1:80.646, train precision:82.794, train recall:80.683, train kappa:81.377
self.nodes_fc[0]: tensor([ 0.0471, -0.0974], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1267,  0.0410], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1246, -0.0249], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1191, 0.0582], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0371, -0.0665], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0966, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:27 step:1 train loss:0.616810, train acc:82.104, train f1:80.824, train precision:83.013, train recall:80.943, train kappa:81.404
self.nodes_fc[0]: tensor([ 0.0471, -0.0976], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1268,  0.0412], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1247, -0.0251], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1192, 0.0582], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0370, -0.0664], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0966, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:27 step:2 train loss:0.597680, train acc:82.425, train f1:81.153, train precision:83.430, train recall:81.009, train kappa:81.737
self.nodes_fc[0]: tensor([ 0.0471, -0.0978], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1269,  0.0413], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1248, -0.0251], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1193, 0.0583], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0370, -0.0663], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0965, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:27 step:3 train loss:0.607064, train acc:82.358, train f1:80.951, train precision:83.536, train recall:80.758, train kappa:81.655
self.nodes_fc[0]: tensor([ 0.0472, -0.0980], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0413], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1249, -0.0253], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1193, 0.0583], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0371, -0.0662], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0964, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:27 step:4 train loss:0.612094, train acc:82.092, train f1:80.856, train precision:83.614, train recall:80.538, train kappa:81.377
self.nodes_fc[0]: tensor([ 0.0473, -0.0981], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0413], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1250, -0.0254], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1195, 0.0585], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0371, -0.0662], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0963, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:27 step:5 train loss:0.616929, train acc:82.370, train f1:81.002, train precision:83.617, train recall:80.687, train kappa:81.675
self.nodes_fc[0]: tensor([ 0.0473, -0.0980], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0414], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1250, -0.0256], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1196, 0.0585], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0372, -0.0663], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0963, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:27 step:6 train loss:0.605198, train acc:82.236, train f1:80.856, train precision:82.740, train recall:80.880, train kappa:81.530
self.nodes_fc[0]: tensor([ 0.0474, -0.0979], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0414], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1250, -0.0257], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1197, 0.0587], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0373, -0.0664], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0962, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:27 step:7 train loss:0.604100, train acc:82.513, train f1:81.279, train precision:83.208, train recall:81.598, train kappa:81.830
self.nodes_fc[0]: tensor([ 0.0474, -0.0979], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0414], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1250, -0.0258], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1198, 0.0588], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0373, -0.0665], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0962, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:27 step:8 train loss:0.607841, train acc:82.291, train f1:80.774, train precision:82.111, train recall:81.230, train kappa:81.609
self.nodes_fc[0]: tensor([ 0.0474, -0.0979], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0414], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1250, -0.0260], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1199, 0.0590], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0374, -0.0666], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0961, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:27 step:9 train loss:0.598095, train acc:82.660, train f1:81.469, train precision:83.704, train recall:81.875, train kappa:81.987
self.nodes_fc[0]: tensor([ 0.0473, -0.0979], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0414], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1249, -0.0261], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1199, 0.0591], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0374, -0.0667], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0960, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:27 step:10 train loss:0.612966, train acc:82.202, train f1:81.009, train precision:83.323, train recall:80.943, train kappa:81.500
self.nodes_fc[0]: tensor([ 0.0472, -0.0980], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0415], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1249, -0.0263], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1201, 0.0592], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0375, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0960, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:27 step:11 train loss:0.596008, train acc:82.434, train f1:81.097, train precision:82.898, train recall:81.023, train kappa:81.739
self.nodes_fc[0]: tensor([ 0.0472, -0.0980], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0416], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1249, -0.0264], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1200, 0.0592], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0375, -0.0673], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0959, -0.1244], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0472, -0.0980], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0416], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1249, -0.0264], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1200, 0.0592], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0375, -0.0673], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0959, -0.1244], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:27        valid loss:0.695683, valid acc:81.559, valid f1:57.696, valid precision:54.948, valid recall:70.019, valid kappa:79.232
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0472, -0.0980], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0416], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1249, -0.0264], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1200, 0.0592], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0375, -0.0673], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0959, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:28 step:0 train loss:0.599637, train acc:82.651, train f1:81.153, train precision:83.253, train recall:81.318, train kappa:81.967
self.nodes_fc[0]: tensor([ 0.0472, -0.0980], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0417], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1248, -0.0266], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1199, 0.0593], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0375, -0.0676], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0958, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:28 step:1 train loss:0.596278, train acc:82.571, train f1:81.330, train precision:83.489, train recall:81.173, train kappa:81.878
self.nodes_fc[0]: tensor([ 0.0471, -0.0980], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0418], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1248, -0.0267], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1198, 0.0592], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0376, -0.0679], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0958, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:28 step:2 train loss:0.597146, train acc:82.419, train f1:81.166, train precision:83.152, train recall:81.338, train kappa:81.722
self.nodes_fc[0]: tensor([ 0.0470, -0.0979], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0418], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1249, -0.0267], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1197, 0.0590], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0682], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0957, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:28 step:3 train loss:0.592960, train acc:82.687, train f1:81.590, train precision:83.832, train recall:81.465, train kappa:82.010
self.nodes_fc[0]: tensor([ 0.0469, -0.0979], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0418], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1250, -0.0268], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1195, 0.0589], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0685], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0956, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:28 step:4 train loss:0.603131, train acc:82.550, train f1:81.358, train precision:83.603, train recall:81.299, train kappa:81.880
self.nodes_fc[0]: tensor([ 0.0469, -0.0979], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0418], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1250, -0.0268], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1193, 0.0588], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0688], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0956, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:28 step:5 train loss:0.594712, train acc:82.968, train f1:81.440, train precision:83.300, train recall:81.694, train kappa:82.296
self.nodes_fc[0]: tensor([ 0.0468, -0.0978], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0417], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1251, -0.0268], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1191, 0.0587], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0689], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0955, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:28 step:6 train loss:0.592265, train acc:82.773, train f1:81.547, train precision:83.609, train recall:81.587, train kappa:82.094
self.nodes_fc[0]: tensor([ 0.0467, -0.0977], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0417], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1252, -0.0267], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1189, 0.0586], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0690], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0955, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:28 step:7 train loss:0.593241, train acc:82.715, train f1:81.841, train precision:84.144, train recall:81.917, train kappa:82.052
self.nodes_fc[0]: tensor([ 0.0467, -0.0976], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1273,  0.0416], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1254, -0.0266], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1187, 0.0586], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0690], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0954, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:28 step:8 train loss:0.591604, train acc:82.672, train f1:81.534, train precision:83.347, train recall:81.664, train kappa:82.016
self.nodes_fc[0]: tensor([ 0.0466, -0.0975], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1273,  0.0415], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1254, -0.0265], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1186, 0.0586], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0690], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0953, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:28 step:9 train loss:0.597309, train acc:82.617, train f1:81.287, train precision:83.056, train recall:81.500, train kappa:81.929
self.nodes_fc[0]: tensor([ 0.0465, -0.0974], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1273,  0.0414], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1254, -0.0264], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1185, 0.0587], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0689], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:28 step:10 train loss:0.608328, train acc:82.349, train f1:81.074, train precision:83.388, train recall:81.062, train kappa:81.650
self.nodes_fc[0]: tensor([ 0.0464, -0.0973], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1273,  0.0413], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1254, -0.0264], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1183, 0.0587], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0688], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:28 step:11 train loss:0.610485, train acc:82.299, train f1:81.305, train precision:83.746, train recall:81.251, train kappa:81.594
self.nodes_fc[0]: tensor([ 0.0464, -0.0972], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1274,  0.0413], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1254, -0.0264], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1180, 0.0589], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0687], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1245], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0464, -0.0972], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1274,  0.0413], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1254, -0.0264], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1180, 0.0589], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0687], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1245], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:28        valid loss:0.690052, valid acc:81.722, valid f1:58.038, valid precision:55.344, valid recall:70.066, valid kappa:79.418
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0464, -0.0972], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1274,  0.0413], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1254, -0.0264], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1180, 0.0589], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0687], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:29 step:0 train loss:0.593333, train acc:82.611, train f1:81.674, train precision:83.803, train recall:81.621, train kappa:81.936
self.nodes_fc[0]: tensor([ 0.0463, -0.0971], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1274,  0.0412], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1254, -0.0263], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1178, 0.0589], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0686], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:29 step:1 train loss:0.575530, train acc:83.163, train f1:81.879, train precision:83.846, train recall:81.768, train kappa:82.499
self.nodes_fc[0]: tensor([ 0.0463, -0.0969], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1274,  0.0412], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1254, -0.0262], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1175, 0.0591], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0686], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:29 step:2 train loss:0.589927, train acc:82.782, train f1:81.457, train precision:83.104, train recall:81.550, train kappa:82.115
self.nodes_fc[0]: tensor([ 0.0464, -0.0969], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1276,  0.0412], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1255, -0.0260], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1174, 0.0592], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0686], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0949, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:29 step:3 train loss:0.585762, train acc:83.081, train f1:82.001, train precision:84.033, train recall:82.248, train kappa:82.422
self.nodes_fc[0]: tensor([ 0.0464, -0.0969], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1277,  0.0411], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1257, -0.0259], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1173, 0.0592], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0688], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0949, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:29 step:4 train loss:0.586003, train acc:82.849, train f1:81.709, train precision:83.691, train recall:81.740, train kappa:82.180
self.nodes_fc[0]: tensor([ 0.0464, -0.0970], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1278,  0.0411], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1259, -0.0257], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1172, 0.0593], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0378, -0.0689], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:29 step:5 train loss:0.590047, train acc:82.938, train f1:81.936, train precision:83.959, train recall:82.066, train kappa:82.274
self.nodes_fc[0]: tensor([ 0.0465, -0.0971], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1280,  0.0410], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1261, -0.0254], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1173, 0.0595], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0380, -0.0691], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:29 step:6 train loss:0.589045, train acc:82.996, train f1:81.700, train precision:83.923, train recall:81.612, train kappa:82.333
self.nodes_fc[0]: tensor([ 0.0465, -0.0973], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1281,  0.0410], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1262, -0.0252], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1175, 0.0596], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0381, -0.0692], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:29 step:7 train loss:0.582748, train acc:82.767, train f1:81.635, train precision:83.425, train recall:81.792, train kappa:82.087
self.nodes_fc[0]: tensor([ 0.0466, -0.0974], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1281,  0.0410], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1263, -0.0251], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1175, 0.0598], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0382, -0.0693], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:29 step:8 train loss:0.590619, train acc:82.623, train f1:81.666, train precision:83.773, train recall:81.692, train kappa:81.949
self.nodes_fc[0]: tensor([ 0.0465, -0.0975], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1281,  0.0410], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1263, -0.0250], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1177, 0.0599], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0383, -0.0692], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:29 step:9 train loss:0.585805, train acc:82.971, train f1:81.742, train precision:83.998, train recall:81.888, train kappa:82.286
self.nodes_fc[0]: tensor([ 0.0465, -0.0975], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1281,  0.0410], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1262, -0.0250], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1178, 0.0599], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0384, -0.0691], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:29 step:10 train loss:0.578818, train acc:83.121, train f1:81.958, train precision:83.766, train recall:82.063, train kappa:82.457
self.nodes_fc[0]: tensor([ 0.0465, -0.0975], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1281,  0.0411], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1262, -0.0249], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1179, 0.0598], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0691], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:29 step:11 train loss:0.581594, train acc:82.830, train f1:81.936, train precision:84.754, train recall:81.924, train kappa:82.175
self.nodes_fc[0]: tensor([ 0.0466, -0.0974], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1281,  0.0411], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1262, -0.0250], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1180, 0.0597], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0388, -0.0691], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1244], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0466, -0.0974], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1281,  0.0411], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1262, -0.0250], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1180, 0.0597], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0388, -0.0691], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1244], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:29        valid loss:0.692681, valid acc:81.643, valid f1:57.895, valid precision:55.072, valid recall:70.265, valid kappa:79.331
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0466, -0.0974], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1281,  0.0411], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1262, -0.0250], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1180, 0.0597], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0388, -0.0691], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:30 step:0 train loss:0.565879, train acc:83.356, train f1:82.153, train precision:83.814, train recall:82.309, train kappa:82.698
self.nodes_fc[0]: tensor([ 0.0466, -0.0973], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1281,  0.0411], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1262, -0.0250], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1181, 0.0595], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0691], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:30 step:1 train loss:0.589917, train acc:82.700, train f1:81.714, train precision:83.536, train recall:81.810, train kappa:82.032
self.nodes_fc[0]: tensor([ 0.0466, -0.0973], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1281,  0.0411], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1263, -0.0251], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1182, 0.0593], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0392, -0.0691], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:30 step:2 train loss:0.577880, train acc:83.002, train f1:81.594, train precision:83.270, train recall:81.629, train kappa:82.351
self.nodes_fc[0]: tensor([ 0.0466, -0.0973], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1280,  0.0411], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1263, -0.0252], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1183, 0.0590], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0394, -0.0690], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:30 step:3 train loss:0.576245, train acc:83.087, train f1:82.109, train precision:83.977, train recall:82.191, train kappa:82.409
self.nodes_fc[0]: tensor([ 0.0466, -0.0974], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1280,  0.0410], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1264, -0.0252], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1184, 0.0588], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0394, -0.0690], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0949, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:30 step:4 train loss:0.577153, train acc:82.938, train f1:81.951, train precision:83.979, train recall:81.866, train kappa:82.277
self.nodes_fc[0]: tensor([ 0.0466, -0.0974], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1279,  0.0409], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1265, -0.0253], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1184, 0.0586], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0395, -0.0689], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0949, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:30 step:5 train loss:0.571551, train acc:83.121, train f1:82.014, train precision:84.122, train recall:81.891, train kappa:82.446
self.nodes_fc[0]: tensor([ 0.0467, -0.0975], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1279,  0.0409], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1265, -0.0254], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1184, 0.0582], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0396, -0.0688], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0949, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:30 step:6 train loss:0.585015, train acc:82.703, train f1:81.569, train precision:83.410, train recall:81.653, train kappa:82.017
self.nodes_fc[0]: tensor([ 0.0467, -0.0978], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1279,  0.0408], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1266, -0.0253], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1184, 0.0581], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0397, -0.0687], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0949, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:30 step:7 train loss:0.576635, train acc:83.173, train f1:81.961, train precision:83.648, train recall:82.165, train kappa:82.520
self.nodes_fc[0]: tensor([ 0.0468, -0.0980], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1279,  0.0408], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1267, -0.0254], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1183, 0.0579], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0399, -0.0686], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:30 step:8 train loss:0.571502, train acc:83.411, train f1:82.150, train precision:84.108, train recall:82.243, train kappa:82.777
self.nodes_fc[0]: tensor([ 0.0470, -0.0981], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1278,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1267, -0.0256], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1182, 0.0577], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0400, -0.0685], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:30 step:9 train loss:0.577233, train acc:83.127, train f1:82.132, train precision:84.046, train recall:82.173, train kappa:82.477
self.nodes_fc[0]: tensor([ 0.0473, -0.0983], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1278,  0.0406], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1267, -0.0258], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1181, 0.0576], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0401, -0.0685], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:30 step:10 train loss:0.573140, train acc:83.362, train f1:81.852, train precision:83.883, train recall:82.034, train kappa:82.710
self.nodes_fc[0]: tensor([ 0.0475, -0.0983], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1277,  0.0405], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1267, -0.0259], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1179, 0.0575], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0402, -0.0685], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1240], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:30 step:11 train loss:0.561516, train acc:83.486, train f1:81.904, train precision:83.763, train recall:82.015, train kappa:82.808
self.nodes_fc[0]: tensor([ 0.0477, -0.0983], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1277,  0.0404], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1267, -0.0261], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1176, 0.0575], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0402, -0.0687], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1240], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0477, -0.0983], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1277,  0.0404], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1267, -0.0261], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1176, 0.0575], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0402, -0.0687], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1240], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:30        valid loss:0.692281, valid acc:81.747, valid f1:58.117, valid precision:55.425, valid recall:70.258, valid kappa:79.455
None
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0477, -0.0983], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1277,  0.0404], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1267, -0.0261], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1176, 0.0575], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0402, -0.0687], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1240], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:31 step:0 train loss:0.568723, train acc:83.255, train f1:81.894, train precision:83.757, train recall:82.069, train kappa:82.606
self.nodes_fc[0]: tensor([ 0.0480, -0.0984], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1277,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1267, -0.0263], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1175, 0.0576], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0403, -0.0688], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1240], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:31 step:1 train loss:0.576056, train acc:83.154, train f1:82.166, train precision:84.370, train recall:82.177, train kappa:82.489
self.nodes_fc[0]: tensor([ 0.0482, -0.0986], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1277,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1267, -0.0261], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1173, 0.0576], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0402, -0.0688], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1240], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:31 step:2 train loss:0.571221, train acc:83.249, train f1:82.215, train precision:84.154, train recall:82.297, train kappa:82.595
self.nodes_fc[0]: tensor([ 0.0483, -0.0987], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1277,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1268, -0.0257], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1173, 0.0576], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0403, -0.0689], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1240], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:31 step:3 train loss:0.569627, train acc:83.466, train f1:82.289, train precision:84.229, train recall:82.366, train kappa:82.819
self.nodes_fc[0]: tensor([ 0.0484, -0.0989], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1277,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1269, -0.0253], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1172, 0.0575], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0403, -0.0690], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1240], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:31 step:4 train loss:0.573035, train acc:83.316, train f1:81.964, train precision:83.796, train recall:82.102, train kappa:82.674
self.nodes_fc[0]: tensor([ 0.0485, -0.0989], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1277,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1270, -0.0249], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1171, 0.0575], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0403, -0.0691], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1240], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:31 step:5 train loss:0.560576, train acc:83.493, train f1:82.411, train precision:83.908, train recall:82.556, train kappa:82.849
self.nodes_fc[0]: tensor([ 0.0486, -0.0989], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1276,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1270, -0.0246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1169, 0.0575], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0402, -0.0692], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1239], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:31 step:6 train loss:0.572475, train acc:83.041, train f1:81.850, train precision:83.469, train recall:82.033, train kappa:82.380
self.nodes_fc[0]: tensor([ 0.0487, -0.0989], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1271, -0.0245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1168, 0.0576], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0401, -0.0694], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1239], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:31 step:7 train loss:0.568144, train acc:83.273, train f1:82.230, train precision:84.327, train recall:82.182, train kappa:82.616
self.nodes_fc[0]: tensor([ 0.0489, -0.0987], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0402], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1271, -0.0243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1167, 0.0576], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0400, -0.0696], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0949, -0.1239], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:31 step:8 train loss:0.557733, train acc:83.615, train f1:82.292, train precision:84.042, train recall:82.507, train kappa:82.962
self.nodes_fc[0]: tensor([ 0.0490, -0.0986], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1274,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1270, -0.0240], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0577], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0399, -0.0696], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0949, -0.1239], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:31 step:9 train loss:0.574944, train acc:83.112, train f1:81.938, train precision:83.869, train recall:81.905, train kappa:82.455
self.nodes_fc[0]: tensor([ 0.0491, -0.0985], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1270, -0.0239], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0578], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0397, -0.0696], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0949, -0.1239], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:31 step:10 train loss:0.565482, train acc:83.383, train f1:82.312, train precision:84.488, train recall:82.386, train kappa:82.748
self.nodes_fc[0]: tensor([ 0.0491, -0.0984], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1270, -0.0236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0580], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0395, -0.0696], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1240], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:31 step:11 train loss:0.566890, train acc:83.457, train f1:81.926, train precision:84.186, train recall:82.033, train kappa:82.795
self.nodes_fc[0]: tensor([ 0.0491, -0.0982], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0400], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1271, -0.0232], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0582], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0393, -0.0696], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1241], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0491, -0.0982], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0400], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1271, -0.0232], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0582], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0393, -0.0696], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1241], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:31        valid loss:0.691151, valid acc:81.898, valid f1:58.132, valid precision:55.578, valid recall:70.225, valid kappa:79.620
[81.89829676733392, 58.131775509369966, 55.5781411919288, 70.22499911798555, 79.6197455299895]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0491, -0.0982], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1271, -0.0232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0582], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0393, -0.0696], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:32 step:0 train loss:0.559599, train acc:83.276, train f1:82.147, train precision:84.240, train recall:82.324, train kappa:82.614
self.nodes_fc[0]: tensor([ 0.0492, -0.0980], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1272, -0.0226], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1158, 0.0583], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0696], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0947, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:32 step:1 train loss:0.544708, train acc:84.024, train f1:82.803, train precision:84.993, train recall:82.793, train kappa:83.399
self.nodes_fc[0]: tensor([ 0.0493, -0.0979], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1273, -0.0222], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1156, 0.0584], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0695], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0946, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:32 step:2 train loss:0.563534, train acc:83.292, train f1:82.245, train precision:84.192, train recall:82.261, train kappa:82.648
self.nodes_fc[0]: tensor([ 0.0495, -0.0977], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1275, -0.0218], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1154, 0.0585], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0387, -0.0695], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0946, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:32 step:3 train loss:0.571750, train acc:83.081, train f1:82.052, train precision:83.751, train recall:82.172, train kappa:82.434
self.nodes_fc[0]: tensor([ 0.0497, -0.0976], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1269,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1277, -0.0216], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1151, 0.0586], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0385, -0.0695], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0945, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:32 step:4 train loss:0.552902, train acc:83.774, train f1:82.757, train precision:84.815, train recall:82.815, train kappa:83.144
self.nodes_fc[0]: tensor([ 0.0499, -0.0975], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1269,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1277, -0.0213], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1150, 0.0586], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0384, -0.0695], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0945, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:32 step:5 train loss:0.564140, train acc:83.240, train f1:82.165, train precision:84.799, train recall:82.062, train kappa:82.568
self.nodes_fc[0]: tensor([ 0.0501, -0.0974], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1269,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1278, -0.0211], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1148, 0.0587], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0382, -0.0696], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0945, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:32 step:6 train loss:0.561740, train acc:83.469, train f1:82.401, train precision:84.344, train recall:82.369, train kappa:82.817
self.nodes_fc[0]: tensor([ 0.0504, -0.0972], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1278, -0.0208], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1147, 0.0587], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0381, -0.0696], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0945, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:32 step:7 train loss:0.555268, train acc:83.679, train f1:82.414, train precision:84.297, train recall:82.521, train kappa:83.044
self.nodes_fc[0]: tensor([ 0.0506, -0.0971], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1278, -0.0206], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1147, 0.0587], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0381, -0.0697], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0945, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:32 step:8 train loss:0.561726, train acc:83.557, train f1:82.298, train precision:83.937, train recall:82.630, train kappa:82.927
self.nodes_fc[0]: tensor([ 0.0508, -0.0970], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1279, -0.0203], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1147, 0.0586], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0381, -0.0698], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0945, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:32 step:9 train loss:0.565871, train acc:83.179, train f1:82.009, train precision:83.568, train recall:82.208, train kappa:82.530
self.nodes_fc[0]: tensor([ 0.0509, -0.0968], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0402], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1279, -0.0201], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1148, 0.0584], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0380, -0.0698], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0945, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:32 step:10 train loss:0.551765, train acc:83.789, train f1:82.614, train precision:84.489, train recall:82.891, train kappa:83.157
self.nodes_fc[0]: tensor([ 0.0511, -0.0966], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0402], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1279, -0.0201], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1149, 0.0583], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0380, -0.0699], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0945, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:32 step:11 train loss:0.546794, train acc:83.911, train f1:82.587, train precision:84.510, train recall:82.714, train kappa:83.271
self.nodes_fc[0]: tensor([ 0.0514, -0.0964], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1269,  0.0402], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1279, -0.0201], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1151, 0.0582], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0380, -0.0700], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0945, -0.1246], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0514, -0.0964], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1269,  0.0402], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1279, -0.0201], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1151, 0.0582], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0380, -0.0700], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0945, -0.1246], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:32        valid loss:0.683117, valid acc:82.031, valid f1:58.333, valid precision:55.274, valid recall:70.582, valid kappa:79.765
[1;31mTest score increased (81.898297 --> 82.031202).[0m
[82.03120207741223, 58.33348583466207, 55.27351715963912, 70.58244349009492, 79.76534411560539]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0514, -0.0964], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1269,  0.0402], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1279, -0.0201], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1151, 0.0582], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0380, -0.0700], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0945, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:33 step:0 train loss:0.555150, train acc:83.511, train f1:82.436, train precision:84.100, train recall:82.670, train kappa:82.872
self.nodes_fc[0]: tensor([ 0.0516, -0.0962], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1268,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1280, -0.0201], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1153, 0.0580], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0381, -0.0700], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0945, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:33 step:1 train loss:0.558390, train acc:83.548, train f1:82.449, train precision:84.196, train recall:82.561, train kappa:82.905
self.nodes_fc[0]: tensor([ 0.0518, -0.0961], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1267,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1279, -0.0201], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1155, 0.0579], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0382, -0.0701], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0945, -0.1247], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:33 step:2 train loss:0.541454, train acc:84.164, train f1:82.896, train precision:84.911, train recall:82.928, train kappa:83.536
self.nodes_fc[0]: tensor([ 0.0520, -0.0959], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1267,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1278, -0.0200], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1157, 0.0578], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0383, -0.0701], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0946, -0.1247], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:33 step:3 train loss:0.551254, train acc:83.691, train f1:82.576, train precision:84.517, train recall:82.572, train kappa:83.052
self.nodes_fc[0]: tensor([ 0.0522, -0.0958], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1267,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1278, -0.0200], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1159, 0.0578], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0384, -0.0702], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0946, -0.1247], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:33 step:4 train loss:0.552952, train acc:83.780, train f1:82.666, train precision:84.713, train recall:82.596, train kappa:83.155
self.nodes_fc[0]: tensor([ 0.0523, -0.0957], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1266,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1277, -0.0199], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0580], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0704], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0946, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:33 step:5 train loss:0.543103, train acc:83.884, train f1:82.655, train precision:84.233, train recall:82.793, train kappa:83.260
self.nodes_fc[0]: tensor([ 0.0523, -0.0957], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1266,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1276, -0.0200], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0581], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0387, -0.0705], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0947, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:33 step:6 train loss:0.563001, train acc:83.453, train f1:82.726, train precision:84.570, train recall:82.747, train kappa:82.807
self.nodes_fc[0]: tensor([ 0.0523, -0.0956], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1266,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1275, -0.0201], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0583], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0388, -0.0705], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:33 step:7 train loss:0.548150, train acc:83.777, train f1:82.752, train precision:84.410, train recall:82.943, train kappa:83.148
self.nodes_fc[0]: tensor([ 0.0522, -0.0956], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1267,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1274, -0.0201], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0584], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0706], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:33 step:8 train loss:0.558126, train acc:83.627, train f1:82.452, train precision:84.063, train recall:82.586, train kappa:82.989
self.nodes_fc[0]: tensor([ 0.0521, -0.0956], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1268,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1275, -0.0200], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0586], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0706], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:33 step:9 train loss:0.548010, train acc:83.762, train f1:82.769, train precision:84.533, train recall:82.940, train kappa:83.128
self.nodes_fc[0]: tensor([ 0.0520, -0.0956], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1269,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1276, -0.0199], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1169, 0.0587], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0707], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0949, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:33 step:10 train loss:0.546464, train acc:84.109, train f1:83.130, train precision:84.939, train recall:83.379, train kappa:83.494
self.nodes_fc[0]: tensor([ 0.0520, -0.0955], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0396], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1277, -0.0197], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1170, 0.0589], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0708], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0949, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:33 step:11 train loss:0.560928, train acc:83.650, train f1:82.268, train precision:84.206, train recall:82.671, train kappa:83.036
self.nodes_fc[0]: tensor([ 0.0519, -0.0954], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0395], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1278, -0.0196], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1171, 0.0589], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0709], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0949, -0.1243], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0519, -0.0954], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0395], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1278, -0.0196], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1171, 0.0589], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0709], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0949, -0.1243], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:33        valid loss:0.674962, valid acc:82.467, valid f1:58.624, valid precision:56.304, valid recall:70.262, valid kappa:80.246
[1;31mTest score increased (82.031202 --> 82.466723).[0m
[82.46672255505348, 58.62417298164936, 56.30352051555536, 70.26184277760876, 80.24556623500703]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0519, -0.0954], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0395], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1278, -0.0196], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1171, 0.0589], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0709], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0949, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:34 step:0 train loss:0.543699, train acc:84.085, train f1:82.881, train precision:85.076, train recall:82.923, train kappa:83.454
self.nodes_fc[0]: tensor([ 0.0519, -0.0954], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0396], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1277, -0.0197], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1172, 0.0590], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0711], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0949, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:34 step:1 train loss:0.546544, train acc:83.844, train f1:82.750, train precision:85.241, train recall:82.679, train kappa:83.203
self.nodes_fc[0]: tensor([ 0.0518, -0.0953], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1269,  0.0396], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1277, -0.0197], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1175, 0.0588], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0712], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:34 step:2 train loss:0.547707, train acc:83.716, train f1:82.676, train precision:84.869, train recall:82.611, train kappa:83.079
self.nodes_fc[0]: tensor([ 0.0519, -0.0953], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1268,  0.0396], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1277, -0.0198], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1177, 0.0587], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0712], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:34 step:3 train loss:0.539432, train acc:84.299, train f1:83.229, train precision:85.023, train recall:83.202, train kappa:83.686
self.nodes_fc[0]: tensor([ 0.0519, -0.0953], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1267,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1278, -0.0198], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1178, 0.0588], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0711], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1240], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:34 step:4 train loss:0.548825, train acc:83.612, train f1:82.429, train precision:83.673, train recall:82.784, train kappa:82.976
self.nodes_fc[0]: tensor([ 0.0519, -0.0954], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1268,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1280, -0.0197], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1180, 0.0588], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0711], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1240], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:34 step:5 train loss:0.542939, train acc:84.033, train f1:83.030, train precision:84.344, train recall:83.239, train kappa:83.412
self.nodes_fc[0]: tensor([ 0.0520, -0.0954], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1269,  0.0396], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1281, -0.0195], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1181, 0.0590], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0710], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1240], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:34 step:6 train loss:0.541425, train acc:83.759, train f1:82.866, train precision:84.567, train recall:82.987, train kappa:83.142
self.nodes_fc[0]: tensor([ 0.0520, -0.0955], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0396], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1283, -0.0191], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1181, 0.0591], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0710], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1239], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:34 step:7 train loss:0.540477, train acc:83.939, train f1:82.897, train precision:84.500, train recall:83.063, train kappa:83.316
self.nodes_fc[0]: tensor([ 0.0521, -0.0956], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0396], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1285, -0.0188], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1181, 0.0593], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0709], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1239], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:34 step:8 train loss:0.545603, train acc:83.646, train f1:82.514, train precision:84.399, train recall:82.549, train kappa:83.002
self.nodes_fc[0]: tensor([ 0.0521, -0.0956], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0396], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1286, -0.0187], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1179, 0.0594], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0708], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:34 step:9 train loss:0.543209, train acc:83.850, train f1:82.787, train precision:84.790, train recall:82.807, train kappa:83.227
self.nodes_fc[0]: tensor([ 0.0520, -0.0955], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1288, -0.0187], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1178, 0.0596], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0707], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:34 step:10 train loss:0.550426, train acc:83.710, train f1:82.457, train precision:84.437, train recall:82.505, train kappa:83.084
self.nodes_fc[0]: tensor([ 0.0519, -0.0955], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1290, -0.0188], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1176, 0.0597], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0706], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1237], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:34 step:11 train loss:0.515709, train acc:84.393, train f1:83.143, train precision:84.914, train recall:83.137, train kappa:83.783
self.nodes_fc[0]: tensor([ 0.0519, -0.0956], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0398], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1291, -0.0187], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1173, 0.0599], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0705], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1237], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0519, -0.0956], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0398], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1291, -0.0187], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1173, 0.0599], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0705], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1237], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:34        valid loss:0.686143, valid acc:81.831, valid f1:58.314, valid precision:55.300, valid recall:70.432, valid kappa:79.564
[1;31mEarlyStopping counter: 1 out of 50[0m
[82.46672255505348, 58.62417298164936, 56.30352051555536, 70.26184277760876, 80.24556623500703]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0519, -0.0956], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1291, -0.0187], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1173, 0.0599], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0705], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1237], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:35 step:0 train loss:0.535277, train acc:84.320, train f1:83.186, train precision:84.929, train recall:83.335, train kappa:83.702
self.nodes_fc[0]: tensor([ 0.0519, -0.0956], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1293, -0.0186], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1172, 0.0601], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0704], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:35 step:1 train loss:0.553656, train acc:83.676, train f1:82.794, train precision:84.093, train recall:83.233, train kappa:83.061
self.nodes_fc[0]: tensor([ 0.0520, -0.0956], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1295, -0.0184], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1171, 0.0602], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0703], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:35 step:2 train loss:0.533601, train acc:84.064, train f1:83.003, train precision:84.082, train recall:83.455, train kappa:83.442
self.nodes_fc[0]: tensor([ 0.0521, -0.0957], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1297, -0.0182], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1170, 0.0603], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0703], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:35 step:3 train loss:0.542540, train acc:83.978, train f1:83.007, train precision:84.708, train recall:83.000, train kappa:83.347
self.nodes_fc[0]: tensor([ 0.0522, -0.0957], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1299, -0.0179], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1167, 0.0604], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0702], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:35 step:4 train loss:0.540838, train acc:84.006, train f1:82.953, train precision:85.050, train recall:82.761, train kappa:83.376
self.nodes_fc[0]: tensor([ 0.0523, -0.0957], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1273,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1300, -0.0175], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0605], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0703], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:35 step:5 train loss:0.533339, train acc:84.290, train f1:83.341, train precision:85.674, train recall:83.339, train kappa:83.674
self.nodes_fc[0]: tensor([ 0.0524, -0.0957], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1273,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1302, -0.0173], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0604], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0392, -0.0703], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:35 step:6 train loss:0.539156, train acc:84.262, train f1:83.041, train precision:85.573, train recall:83.121, train kappa:83.646
self.nodes_fc[0]: tensor([ 0.0526, -0.0956], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1273,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1303, -0.0173], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0603], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0393, -0.0703], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:35 step:7 train loss:0.534499, train acc:84.219, train f1:82.959, train precision:85.366, train recall:82.888, train kappa:83.603
self.nodes_fc[0]: tensor([ 0.0527, -0.0955], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1304, -0.0175], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0602], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0394, -0.0704], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:35 step:8 train loss:0.529161, train acc:84.372, train f1:83.441, train precision:85.321, train recall:83.650, train kappa:83.774
self.nodes_fc[0]: tensor([ 0.0528, -0.0955], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1305, -0.0178], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0602], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0394, -0.0704], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:35 step:9 train loss:0.523692, train acc:84.375, train f1:83.329, train precision:84.766, train recall:83.750, train kappa:83.786
self.nodes_fc[0]: tensor([ 0.0529, -0.0955], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1267,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1306, -0.0181], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1159, 0.0601], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0395, -0.0705], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:35 step:10 train loss:0.526613, train acc:84.360, train f1:82.946, train precision:84.252, train recall:83.301, train kappa:83.760
self.nodes_fc[0]: tensor([ 0.0530, -0.0954], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1265,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1307, -0.0185], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1158, 0.0600], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0395, -0.0705], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:35 step:11 train loss:0.556336, train acc:83.573, train f1:82.542, train precision:84.028, train recall:82.936, train kappa:82.917
self.nodes_fc[0]: tensor([ 0.0531, -0.0954], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1264,  0.0397], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1309, -0.0186], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1157, 0.0599], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0396, -0.0704], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1234], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0531, -0.0954], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1264,  0.0397], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1309, -0.0186], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1157, 0.0599], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0396, -0.0704], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1234], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:35        valid loss:0.677458, valid acc:82.391, valid f1:58.540, valid precision:55.729, valid recall:70.353, valid kappa:80.171
[1;31mEarlyStopping counter: 2 out of 50[0m
[82.46672255505348, 58.62417298164936, 56.30352051555536, 70.26184277760876, 80.24556623500703]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0531, -0.0954], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1264,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1309, -0.0186], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1157, 0.0599], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0396, -0.0704], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:36 step:0 train loss:0.528168, train acc:84.253, train f1:83.055, train precision:84.838, train recall:83.294, train kappa:83.641
self.nodes_fc[0]: tensor([ 0.0532, -0.0955], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1265,  0.0396], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1311, -0.0185], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1156, 0.0597], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0397, -0.0703], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:36 step:1 train loss:0.526884, train acc:84.448, train f1:83.453, train precision:85.588, train recall:83.292, train kappa:83.840
self.nodes_fc[0]: tensor([ 0.0532, -0.0956], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1265,  0.0395], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1312, -0.0185], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1154, 0.0595], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0398, -0.0702], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1233], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:36 step:2 train loss:0.529772, train acc:84.021, train f1:82.789, train precision:84.779, train recall:82.551, train kappa:83.377
self.nodes_fc[0]: tensor([ 0.0532, -0.0957], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1265,  0.0394], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1313, -0.0186], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1154, 0.0593], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0398, -0.0701], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1233], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:36 step:3 train loss:0.528804, train acc:84.192, train f1:82.862, train precision:84.903, train recall:82.704, train kappa:83.568
self.nodes_fc[0]: tensor([ 0.0531, -0.0958], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1265,  0.0393], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1314, -0.0187], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1153, 0.0592], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0399, -0.0699], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1233], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:36 step:4 train loss:0.531273, train acc:84.103, train f1:83.284, train precision:84.703, train recall:83.469, train kappa:83.491
self.nodes_fc[0]: tensor([ 0.0531, -0.0958], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1264,  0.0393], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1314, -0.0188], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1154, 0.0591], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0399, -0.0699], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:36 step:5 train loss:0.529398, train acc:84.180, train f1:83.122, train precision:84.157, train recall:83.565, train kappa:83.566
self.nodes_fc[0]: tensor([ 0.0532, -0.0959], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1261,  0.0393], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1314, -0.0190], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1154, 0.0591], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0399, -0.0698], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1233], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:36 step:6 train loss:0.531060, train acc:84.454, train f1:83.593, train precision:85.053, train recall:83.885, train kappa:83.858
self.nodes_fc[0]: tensor([ 0.0532, -0.0959], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1260,  0.0393], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1314, -0.0191], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1154, 0.0591], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0399, -0.0698], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1233], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:36 step:7 train loss:0.528426, train acc:84.482, train f1:83.350, train precision:85.240, train recall:83.617, train kappa:83.877
self.nodes_fc[0]: tensor([ 0.0531, -0.0960], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1259,  0.0393], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1314, -0.0191], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1153, 0.0589], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0399, -0.0698], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:36 step:8 train loss:0.525090, train acc:84.503, train f1:83.259, train precision:84.819, train recall:83.687, train kappa:83.913
self.nodes_fc[0]: tensor([ 0.0530, -0.0961], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1256,  0.0393], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1314, -0.0191], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1152, 0.0588], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0399, -0.0698], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:36 step:9 train loss:0.527063, train acc:84.421, train f1:83.522, train precision:85.537, train recall:83.585, train kappa:83.829
self.nodes_fc[0]: tensor([ 0.0529, -0.0962], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1255,  0.0393], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1313, -0.0192], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1152, 0.0587], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0399, -0.0697], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0953, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:36 step:10 train loss:0.525399, train acc:84.515, train f1:83.481, train precision:86.021, train recall:83.415, train kappa:83.917
self.nodes_fc[0]: tensor([ 0.0527, -0.0962], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1254,  0.0393], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1312, -0.0193], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1151, 0.0587], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0400, -0.0697], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0953, -0.1237], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:36 step:11 train loss:0.530064, train acc:84.239, train f1:83.385, train precision:85.421, train recall:83.361, train kappa:83.615
self.nodes_fc[0]: tensor([ 0.0527, -0.0962], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1255,  0.0393], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1310, -0.0193], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1151, 0.0587], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0399, -0.0697], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0953, -0.1238], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0527, -0.0962], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1255,  0.0393], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1310, -0.0193], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1151, 0.0587], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0399, -0.0697], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0953, -0.1238], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:36        valid loss:0.671783, valid acc:82.473, valid f1:58.869, valid precision:55.836, valid recall:70.437, valid kappa:80.263
[1;31mTest score increased (82.466723 --> 82.472857).[0m
[82.47285664628785, 58.86946163340402, 55.83614789938153, 70.43715204106763, 80.26272722424028]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0527, -0.0962], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1255,  0.0393], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1310, -0.0193], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1151, 0.0587], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0399, -0.0697], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0953, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:37 step:0 train loss:0.528062, train acc:84.174, train f1:83.380, train precision:84.910, train recall:83.425, train kappa:83.566
self.nodes_fc[0]: tensor([ 0.0525, -0.0961], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1258,  0.0392], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1308, -0.0193], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1151, 0.0587], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0399, -0.0697], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0953, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:37 step:1 train loss:0.519410, train acc:84.665, train f1:83.422, train precision:84.433, train recall:83.609, train kappa:84.076
self.nodes_fc[0]: tensor([ 0.0525, -0.0959], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1260,  0.0390], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1307, -0.0194], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1151, 0.0587], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0398, -0.0697], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0953, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:37 step:2 train loss:0.522386, train acc:84.430, train f1:83.419, train precision:84.355, train recall:83.742, train kappa:83.842
self.nodes_fc[0]: tensor([ 0.0524, -0.0958], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1262,  0.0389], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1306, -0.0194], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1152, 0.0588], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0396, -0.0697], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1239], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:37 step:3 train loss:0.522853, train acc:84.418, train f1:83.421, train precision:84.494, train recall:83.708, train kappa:83.817
self.nodes_fc[0]: tensor([ 0.0523, -0.0956], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1263,  0.0389], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1305, -0.0196], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1153, 0.0587], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0395, -0.0695], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:37 step:4 train loss:0.539194, train acc:84.116, train f1:83.156, train precision:84.744, train recall:83.454, train kappa:83.520
self.nodes_fc[0]: tensor([ 0.0523, -0.0954], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1263,  0.0389], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1304, -0.0198], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1153, 0.0587], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0395, -0.0694], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:37 step:5 train loss:0.530666, train acc:84.338, train f1:83.213, train precision:84.926, train recall:83.170, train kappa:83.724
self.nodes_fc[0]: tensor([ 0.0523, -0.0952], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1262,  0.0389], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1305, -0.0199], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1154, 0.0586], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0394, -0.0694], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:37 step:6 train loss:0.522086, train acc:84.290, train f1:83.211, train precision:86.130, train recall:83.102, train kappa:83.671
self.nodes_fc[0]: tensor([ 0.0523, -0.0951], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1262,  0.0389], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1305, -0.0196], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1154, 0.0586], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0393, -0.0694], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:37 step:7 train loss:0.520566, train acc:84.592, train f1:83.407, train precision:85.722, train recall:83.430, train kappa:83.973
self.nodes_fc[0]: tensor([ 0.0524, -0.0949], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1262,  0.0389], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1306, -0.0193], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1154, 0.0587], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0392, -0.0694], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:37 step:8 train loss:0.515286, train acc:84.570, train f1:83.312, train precision:85.149, train recall:83.541, train kappa:83.969
self.nodes_fc[0]: tensor([ 0.0524, -0.0948], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1263,  0.0389], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1306, -0.0191], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1154, 0.0587], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0392, -0.0693], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:37 step:9 train loss:0.526228, train acc:84.460, train f1:83.245, train precision:84.806, train recall:83.554, train kappa:83.855
self.nodes_fc[0]: tensor([ 0.0525, -0.0948], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1263,  0.0388], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1307, -0.0188], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1156, 0.0588], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0392, -0.0691], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:37 step:10 train loss:0.517551, train acc:84.702, train f1:83.683, train precision:84.857, train recall:84.053, train kappa:84.108
self.nodes_fc[0]: tensor([ 0.0526, -0.0948], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1264,  0.0388], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1307, -0.0186], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1159, 0.0588], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0689], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:37 step:11 train loss:0.544184, train acc:83.544, train f1:82.671, train precision:83.611, train recall:83.198, train kappa:82.916
self.nodes_fc[0]: tensor([ 0.0529, -0.0949], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1266,  0.0387], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1307, -0.0183], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0587], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0687], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1238], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0529, -0.0949], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1266,  0.0387], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1307, -0.0183], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0587], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0687], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1238], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:37        valid loss:0.675415, valid acc:82.352, valid f1:59.056, valid precision:56.153, valid recall:70.398, valid kappa:80.132
[1;31mEarlyStopping counter: 1 out of 50[0m
[82.47285664628785, 58.86946163340402, 55.83614789938153, 70.43715204106763, 80.26272722424028]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0529, -0.0949], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1266,  0.0387], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1307, -0.0183], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0587], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0687], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:38 step:0 train loss:0.511328, train acc:84.689, train f1:83.724, train precision:85.516, train recall:83.660, train kappa:84.115
self.nodes_fc[0]: tensor([ 0.0532, -0.0949], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1268,  0.0386], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1307, -0.0180], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0586], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0684], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:38 step:1 train loss:0.517863, train acc:84.561, train f1:83.645, train precision:85.202, train recall:83.742, train kappa:83.967
self.nodes_fc[0]: tensor([ 0.0536, -0.0948], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1269,  0.0384], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1306, -0.0179], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0587], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0388, -0.0681], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:38 step:2 train loss:0.514206, train acc:84.515, train f1:83.329, train precision:85.022, train recall:83.492, train kappa:83.918
self.nodes_fc[0]: tensor([ 0.0539, -0.0947], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1269,  0.0384], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1305, -0.0179], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0587], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0387, -0.0678], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:38 step:3 train loss:0.513924, train acc:84.601, train f1:83.294, train precision:85.019, train recall:83.290, train kappa:84.009
self.nodes_fc[0]: tensor([ 0.0542, -0.0946], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1269,  0.0383], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1304, -0.0180], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1168, 0.0586], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0387, -0.0676], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:38 step:4 train loss:0.516693, train acc:84.372, train f1:83.498, train precision:85.578, train recall:83.638, train kappa:83.757
self.nodes_fc[0]: tensor([ 0.0545, -0.0946], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1269,  0.0383], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1303, -0.0181], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1168, 0.0586], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0387, -0.0674], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1239], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:38 step:5 train loss:0.510491, train acc:84.976, train f1:83.708, train precision:85.585, train recall:83.883, train kappa:84.391
self.nodes_fc[0]: tensor([ 0.0547, -0.0947], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1268,  0.0383], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1303, -0.0182], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1168, 0.0586], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0387, -0.0672], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0952, -0.1239], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:38 step:6 train loss:0.515152, train acc:84.583, train f1:83.330, train precision:84.829, train recall:83.620, train kappa:83.984
self.nodes_fc[0]: tensor([ 0.0549, -0.0948], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1268,  0.0383], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1303, -0.0182], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1168, 0.0587], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1240], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:38 step:7 train loss:0.519284, train acc:84.357, train f1:83.467, train precision:85.109, train recall:83.706, train kappa:83.752
self.nodes_fc[0]: tensor([ 0.0550, -0.0950], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1268,  0.0384], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1304, -0.0182], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1167, 0.0588], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0667], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:38 step:8 train loss:0.521129, train acc:84.659, train f1:83.544, train precision:84.943, train recall:84.040, train kappa:84.066
self.nodes_fc[0]: tensor([ 0.0552, -0.0952], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1268,  0.0384], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1306, -0.0181], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0589], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0666], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:38 step:9 train loss:0.527044, train acc:84.180, train f1:83.295, train precision:84.835, train recall:83.604, train kappa:83.570
self.nodes_fc[0]: tensor([ 0.0553, -0.0953], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1268,  0.0384], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1307, -0.0181], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0590], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0665], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:38 step:10 train loss:0.514235, train acc:84.717, train f1:83.671, train precision:85.077, train recall:83.971, train kappa:84.112
self.nodes_fc[0]: tensor([ 0.0555, -0.0954], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1269,  0.0384], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1308, -0.0181], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0591], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0665], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0951, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:38 step:11 train loss:0.526527, train acc:84.364, train f1:83.648, train precision:85.851, train recall:83.657, train kappa:83.745
self.nodes_fc[0]: tensor([ 0.0556, -0.0957], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0385], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1309, -0.0181], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0591], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0666], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1243], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0556, -0.0957], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0385], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1309, -0.0181], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0591], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0666], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1243], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:38        valid loss:0.670672, valid acc:82.581, valid f1:59.171, valid precision:56.563, valid recall:70.332, valid kappa:80.377
[1;31mTest score increased (82.472857 --> 82.581226).[0m
[82.58122559142863, 59.17131026446872, 56.56279394487561, 70.33196969873089, 80.37718850853938]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0556, -0.0957], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0385], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1309, -0.0181], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0591], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0666], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:39 step:0 train loss:0.517250, train acc:84.521, train f1:83.686, train precision:85.650, train recall:83.624, train kappa:83.922
self.nodes_fc[0]: tensor([ 0.0556, -0.0959], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0386], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1309, -0.0181], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0591], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0387, -0.0667], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:39 step:1 train loss:0.510569, train acc:84.763, train f1:83.994, train precision:85.955, train recall:83.941, train kappa:84.154
self.nodes_fc[0]: tensor([ 0.0556, -0.0961], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0386], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1310, -0.0182], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0592], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0387, -0.0668], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:39 step:2 train loss:0.518950, train acc:84.464, train f1:83.620, train precision:85.364, train recall:83.456, train kappa:83.868
self.nodes_fc[0]: tensor([ 0.0555, -0.0963], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0387], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1311, -0.0183], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0592], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0668], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0950, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:39 step:3 train loss:0.506980, train acc:84.836, train f1:83.952, train precision:85.499, train recall:84.007, train kappa:84.227
self.nodes_fc[0]: tensor([ 0.0556, -0.0963], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1273,  0.0387], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1313, -0.0183], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0593], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0669], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0949, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:39 step:4 train loss:0.506101, train acc:84.912, train f1:83.861, train precision:84.865, train recall:84.283, train kappa:84.336
self.nodes_fc[0]: tensor([ 0.0555, -0.0964], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0386], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1315, -0.0182], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0592], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0392, -0.0671], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0949, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:39 step:5 train loss:0.509357, train acc:84.796, train f1:83.968, train precision:85.199, train recall:84.181, train kappa:84.217
self.nodes_fc[0]: tensor([ 0.0556, -0.0965], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1277,  0.0385], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1317, -0.0183], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0592], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0393, -0.0672], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0949, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:39 step:6 train loss:0.519733, train acc:84.442, train f1:83.471, train precision:84.994, train recall:83.734, train kappa:83.839
self.nodes_fc[0]: tensor([ 0.0555, -0.0965], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1278,  0.0385], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1318, -0.0183], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0592], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0393, -0.0672], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0949, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:39 step:7 train loss:0.509212, train acc:84.583, train f1:83.595, train precision:85.589, train recall:83.720, train kappa:83.980
self.nodes_fc[0]: tensor([ 0.0555, -0.0964], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1278,  0.0385], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1319, -0.0184], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0593], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0394, -0.0673], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0949, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:39 step:8 train loss:0.505849, train acc:84.927, train f1:83.888, train precision:86.220, train recall:83.651, train kappa:84.345
self.nodes_fc[0]: tensor([ 0.0553, -0.0963], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1278,  0.0385], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1320, -0.0184], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0594], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0395, -0.0673], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0949, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:39 step:9 train loss:0.515613, train acc:84.625, train f1:83.584, train precision:85.724, train recall:83.451, train kappa:84.038
self.nodes_fc[0]: tensor([ 0.0552, -0.0961], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1278,  0.0384], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1321, -0.0184], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0594], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0396, -0.0675], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:39 step:10 train loss:0.503581, train acc:84.882, train f1:83.698, train precision:85.701, train recall:83.864, train kappa:84.304
self.nodes_fc[0]: tensor([ 0.0550, -0.0960], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1278,  0.0384], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1322, -0.0184], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0595], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0397, -0.0676], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:39 step:11 train loss:0.509588, train acc:84.693, train f1:83.639, train precision:85.029, train recall:83.921, train kappa:84.094
self.nodes_fc[0]: tensor([ 0.0549, -0.0959], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1277,  0.0384], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1324, -0.0183], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0594], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0397, -0.0678], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1245], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0549, -0.0959], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1277,  0.0384], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1324, -0.0183], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0594], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0397, -0.0678], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1245], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:39        valid loss:0.674000, valid acc:82.456, valid f1:58.900, valid precision:55.837, valid recall:70.486, valid kappa:80.254
[1;31mEarlyStopping counter: 1 out of 50[0m
[82.58122559142863, 59.17131026446872, 56.56279394487561, 70.33196969873089, 80.37718850853938]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0549, -0.0959], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1277,  0.0384], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1324, -0.0183], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0594], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0397, -0.0678], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:40 step:0 train loss:0.496412, train acc:85.245, train f1:84.264, train precision:85.586, train recall:84.482, train kappa:84.680
self.nodes_fc[0]: tensor([ 0.0548, -0.0959], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1277,  0.0383], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1325, -0.0181], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0593], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0398, -0.0681], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0948, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:40 step:1 train loss:0.506880, train acc:84.769, train f1:83.825, train precision:85.075, train recall:84.233, train kappa:84.197
self.nodes_fc[0]: tensor([ 0.0546, -0.0959], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1276,  0.0383], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1327, -0.0180], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0593], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0399, -0.0683], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0947, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:40 step:2 train loss:0.504282, train acc:84.729, train f1:83.756, train precision:85.030, train recall:84.051, train kappa:84.138
self.nodes_fc[0]: tensor([ 0.0544, -0.0959], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0382], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1329, -0.0179], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0592], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0400, -0.0685], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0947, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:40 step:3 train loss:0.505428, train acc:84.973, train f1:83.865, train precision:85.252, train recall:84.137, train kappa:84.379
self.nodes_fc[0]: tensor([ 0.0542, -0.0959], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1274,  0.0381], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1330, -0.0180], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0592], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0400, -0.0686], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0947, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:40 step:4 train loss:0.502156, train acc:85.025, train f1:84.060, train precision:85.888, train recall:84.065, train kappa:84.445
self.nodes_fc[0]: tensor([ 0.0541, -0.0958], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1271,  0.0381], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1330, -0.0182], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0592], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0401, -0.0687], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0947, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:40 step:5 train loss:0.509501, train acc:84.689, train f1:83.731, train precision:85.422, train recall:83.995, train kappa:84.101
self.nodes_fc[0]: tensor([ 0.0540, -0.0957], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1268,  0.0380], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1330, -0.0183], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0591], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0401, -0.0688], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0946, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:40 step:6 train loss:0.500091, train acc:84.882, train f1:83.763, train precision:85.517, train recall:83.813, train kappa:84.306
self.nodes_fc[0]: tensor([ 0.0538, -0.0957], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1266,  0.0380], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1331, -0.0185], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0590], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0402, -0.0687], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0946, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:40 step:7 train loss:0.500503, train acc:84.769, train f1:83.919, train precision:85.602, train recall:83.895, train kappa:84.175
self.nodes_fc[0]: tensor([ 0.0537, -0.0955], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1264,  0.0379], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1330, -0.0188], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0590], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0402, -0.0687], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0946, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:40 step:8 train loss:0.498342, train acc:85.074, train f1:84.244, train precision:85.623, train recall:84.335, train kappa:84.506
self.nodes_fc[0]: tensor([ 0.0536, -0.0954], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1263,  0.0379], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1330, -0.0191], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0590], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0402, -0.0687], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0945, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:40 step:9 train loss:0.512748, train acc:84.708, train f1:83.571, train precision:84.863, train recall:84.102, train kappa:84.110
self.nodes_fc[0]: tensor([ 0.0535, -0.0953], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1263,  0.0378], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1330, -0.0192], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0592], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0402, -0.0686], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0944, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:40 step:10 train loss:0.504782, train acc:84.824, train f1:83.783, train precision:85.059, train recall:84.038, train kappa:84.237
self.nodes_fc[0]: tensor([ 0.0534, -0.0952], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1262,  0.0378], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1331, -0.0194], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0594], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0403, -0.0686], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0944, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:40 step:11 train loss:0.494910, train acc:85.021, train f1:83.990, train precision:85.862, train recall:84.173, train kappa:84.435
self.nodes_fc[0]: tensor([ 0.0532, -0.0952], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1261,  0.0379], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1331, -0.0196], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0597], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0403, -0.0688], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0944, -0.1241], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0532, -0.0952], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1261,  0.0379], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1331, -0.0196], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0597], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0403, -0.0688], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0944, -0.1241], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:40        valid loss:0.669370, valid acc:82.782, valid f1:59.644, valid precision:56.582, valid recall:70.290, valid kappa:80.604
[1;31mTest score increased (82.581226 --> 82.781606).[0m
[82.78160590508517, 59.64427022200428, 56.582164738307384, 70.2898774609892, 80.6037287608647]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0532, -0.0952], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1261,  0.0379], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1331, -0.0196], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0597], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0403, -0.0688], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0944, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:41 step:0 train loss:0.492310, train acc:85.168, train f1:84.248, train precision:86.212, train recall:84.389, train kappa:84.598
self.nodes_fc[0]: tensor([ 0.0530, -0.0952], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1260,  0.0379], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1330, -0.0198], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1159, 0.0601], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0404, -0.0691], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0943, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:41 step:1 train loss:0.489716, train acc:84.998, train f1:83.906, train precision:85.832, train recall:83.899, train kappa:84.414
self.nodes_fc[0]: tensor([ 0.0529, -0.0952], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1260,  0.0380], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1330, -0.0199], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1159, 0.0604], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0404, -0.0693], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0943, -0.1240], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:41 step:2 train loss:0.495988, train acc:85.172, train f1:84.192, train precision:85.878, train recall:84.161, train kappa:84.584
self.nodes_fc[0]: tensor([ 0.0527, -0.0953], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1260,  0.0381], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1329, -0.0199], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1159, 0.0607], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0405, -0.0695], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0943, -0.1239], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:41 step:3 train loss:0.502457, train acc:84.909, train f1:84.104, train precision:85.346, train recall:84.338, train kappa:84.329
self.nodes_fc[0]: tensor([ 0.0526, -0.0953], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1261,  0.0382], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1328, -0.0199], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1159, 0.0610], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0405, -0.0696], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0943, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:41 step:4 train loss:0.494634, train acc:85.114, train f1:83.844, train precision:85.645, train recall:83.876, train kappa:84.536
self.nodes_fc[0]: tensor([ 0.0525, -0.0953], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1261,  0.0382], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1327, -0.0198], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1159, 0.0612], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0405, -0.0697], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0942, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:41 step:5 train loss:0.503622, train acc:84.793, train f1:83.849, train precision:85.657, train recall:83.886, train kappa:84.218
self.nodes_fc[0]: tensor([ 0.0523, -0.0954], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1263,  0.0382], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1325, -0.0197], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0613], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0404, -0.0697], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0943, -0.1237], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:41 step:6 train loss:0.512770, train acc:84.888, train f1:84.033, train precision:85.652, train recall:84.136, train kappa:84.305
self.nodes_fc[0]: tensor([ 0.0522, -0.0955], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1265,  0.0382], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1325, -0.0195], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0615], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0403, -0.0697], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0943, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:41 step:7 train loss:0.506079, train acc:84.756, train f1:83.886, train precision:85.374, train recall:84.082, train kappa:84.182
self.nodes_fc[0]: tensor([ 0.0521, -0.0957], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1266,  0.0382], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1325, -0.0194], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0617], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0403, -0.0698], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0943, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:41 step:8 train loss:0.501259, train acc:84.729, train f1:83.821, train precision:84.846, train recall:84.293, train kappa:84.128
self.nodes_fc[0]: tensor([ 0.0521, -0.0958], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1268,  0.0383], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1324, -0.0194], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0618], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0402, -0.0698], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0943, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:41 step:9 train loss:0.494611, train acc:85.107, train f1:83.998, train precision:85.102, train recall:84.262, train kappa:84.536
self.nodes_fc[0]: tensor([ 0.0520, -0.0958], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1269,  0.0384], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1323, -0.0194], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0620], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0402, -0.0697], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0943, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:41 step:10 train loss:0.504360, train acc:84.851, train f1:83.899, train precision:85.306, train recall:84.083, train kappa:84.260
self.nodes_fc[0]: tensor([ 0.0520, -0.0959], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1270,  0.0384], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1322, -0.0194], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0622], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0402, -0.0697], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0943, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:41 step:11 train loss:0.487207, train acc:85.474, train f1:83.999, train precision:85.805, train recall:84.118, train kappa:84.907
self.nodes_fc[0]: tensor([ 0.0521, -0.0959], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0385], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1322, -0.0193], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0622], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0402, -0.0696], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0943, -0.1235], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0521, -0.0959], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0385], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1322, -0.0193], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0622], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0402, -0.0696], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0943, -0.1235], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:41        valid loss:0.671515, valid acc:82.655, valid f1:59.503, valid precision:56.527, valid recall:70.665, valid kappa:80.470
[1;31mEarlyStopping counter: 1 out of 50[0m
[82.78160590508517, 59.64427022200428, 56.582164738307384, 70.2898774609892, 80.6037287608647]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0521, -0.0959], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1272,  0.0385], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1322, -0.0193], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0622], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0402, -0.0696], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0943, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:42 step:0 train loss:0.487227, train acc:85.297, train f1:84.279, train precision:85.819, train recall:84.656, train kappa:84.736
self.nodes_fc[0]: tensor([ 0.0523, -0.0958], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1273,  0.0385], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1320, -0.0192], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0623], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0402, -0.0695], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0942, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:42 step:1 train loss:0.493103, train acc:85.016, train f1:84.011, train precision:85.734, train recall:84.136, train kappa:84.436
self.nodes_fc[0]: tensor([ 0.0524, -0.0956], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0385], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1319, -0.0191], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0623], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0402, -0.0695], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0942, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:42 step:2 train loss:0.492293, train acc:85.150, train f1:84.074, train precision:85.772, train recall:84.250, train kappa:84.572
self.nodes_fc[0]: tensor([ 0.0525, -0.0955], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0384], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1316, -0.0192], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1168, 0.0623], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0402, -0.0695], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0941, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:42 step:3 train loss:0.499114, train acc:84.998, train f1:84.139, train precision:86.187, train recall:84.202, train kappa:84.423
self.nodes_fc[0]: tensor([ 0.0526, -0.0953], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1276,  0.0384], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1315, -0.0192], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1169, 0.0623], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0402, -0.0694], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0941, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:42 step:4 train loss:0.487634, train acc:85.364, train f1:84.381, train precision:86.115, train recall:84.476, train kappa:84.796
self.nodes_fc[0]: tensor([ 0.0528, -0.0953], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1276,  0.0383], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1314, -0.0191], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1170, 0.0623], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0402, -0.0694], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0941, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:42 step:5 train loss:0.499951, train acc:85.333, train f1:84.204, train precision:85.436, train recall:84.515, train kappa:84.772
self.nodes_fc[0]: tensor([ 0.0529, -0.0953], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1277,  0.0383], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1314, -0.0189], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1170, 0.0623], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0402, -0.0693], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0940, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:42 step:6 train loss:0.501947, train acc:84.863, train f1:83.951, train precision:85.407, train recall:84.352, train kappa:84.283
self.nodes_fc[0]: tensor([ 0.0530, -0.0952], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1277,  0.0383], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1313, -0.0188], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1170, 0.0623], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0402, -0.0693], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0940, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:42 step:7 train loss:0.486776, train acc:85.565, train f1:84.441, train precision:86.101, train recall:84.538, train kappa:85.007
self.nodes_fc[0]: tensor([ 0.0530, -0.0952], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1277,  0.0384], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1314, -0.0185], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1170, 0.0622], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0402, -0.0692], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0941, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:42 step:8 train loss:0.493200, train acc:85.168, train f1:84.179, train precision:85.650, train recall:84.259, train kappa:84.597
self.nodes_fc[0]: tensor([ 0.0531, -0.0952], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1278,  0.0384], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1315, -0.0181], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1170, 0.0622], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0402, -0.0691], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0941, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:42 step:9 train loss:0.496614, train acc:85.220, train f1:84.171, train precision:85.479, train recall:84.515, train kappa:84.647
self.nodes_fc[0]: tensor([ 0.0532, -0.0951], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1279,  0.0385], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1317, -0.0176], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1169, 0.0623], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0401, -0.0692], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0941, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:42 step:10 train loss:0.494530, train acc:85.025, train f1:84.054, train precision:85.708, train recall:84.092, train kappa:84.442
self.nodes_fc[0]: tensor([ 0.0533, -0.0951], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1279,  0.0386], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1319, -0.0172], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1169, 0.0623], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0401, -0.0692], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0941, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:42 step:11 train loss:0.490934, train acc:85.108, train f1:83.693, train precision:85.253, train recall:84.016, train kappa:84.520
self.nodes_fc[0]: tensor([ 0.0534, -0.0950], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1280,  0.0385], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1320, -0.0168], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1169, 0.0623], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0401, -0.0692], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0942, -0.1235], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0534, -0.0950], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1280,  0.0385], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1320, -0.0168], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1169, 0.0623], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0401, -0.0692], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0942, -0.1235], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:42        valid loss:0.671882, valid acc:82.735, valid f1:59.424, valid precision:56.373, valid recall:70.504, valid kappa:80.562
[1;31mEarlyStopping counter: 2 out of 50[0m
[82.78160590508517, 59.64427022200428, 56.582164738307384, 70.2898774609892, 80.6037287608647]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0534, -0.0950], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1280,  0.0385], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1320, -0.0168], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1169, 0.0623], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0401, -0.0692], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0942, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:43 step:0 train loss:0.475057, train acc:85.886, train f1:84.593, train precision:86.604, train recall:84.655, train kappa:85.347
self.nodes_fc[0]: tensor([ 0.0535, -0.0949], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1280,  0.0385], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1322, -0.0165], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1169, 0.0624], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0400, -0.0693], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0942, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:43 step:1 train loss:0.481596, train acc:85.321, train f1:84.493, train precision:86.320, train recall:84.579, train kappa:84.756
self.nodes_fc[0]: tensor([ 0.0536, -0.0948], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1280,  0.0384], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1323, -0.0163], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1169, 0.0625], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0399, -0.0693], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0942, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:43 step:2 train loss:0.493577, train acc:85.013, train f1:84.023, train precision:85.521, train recall:84.152, train kappa:84.427
self.nodes_fc[0]: tensor([ 0.0536, -0.0948], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1280,  0.0383], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1324, -0.0161], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1168, 0.0626], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0398, -0.0692], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0943, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:43 step:3 train loss:0.478302, train acc:85.629, train f1:84.784, train precision:86.535, train recall:84.839, train kappa:85.067
self.nodes_fc[0]: tensor([ 0.0536, -0.0948], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1281,  0.0383], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1325, -0.0159], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1167, 0.0627], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0397, -0.0692], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0943, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:43 step:4 train loss:0.494016, train acc:85.217, train f1:84.159, train precision:85.414, train recall:84.331, train kappa:84.649
self.nodes_fc[0]: tensor([ 0.0535, -0.0947], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1281,  0.0382], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1326, -0.0158], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0629], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0396, -0.0691], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0943, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:43 step:5 train loss:0.490409, train acc:85.446, train f1:84.459, train precision:85.854, train recall:84.615, train kappa:84.885
self.nodes_fc[0]: tensor([ 0.0533, -0.0946], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1281,  0.0381], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1327, -0.0158], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0630], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0395, -0.0691], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0943, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:43 step:6 train loss:0.489712, train acc:84.970, train f1:83.965, train precision:85.172, train recall:84.251, train kappa:84.390
self.nodes_fc[0]: tensor([ 0.0533, -0.0946], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1281,  0.0381], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1328, -0.0157], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1167, 0.0630], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0394, -0.0690], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0942, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:43 step:7 train loss:0.486007, train acc:85.196, train f1:84.324, train precision:85.785, train recall:84.558, train kappa:84.628
self.nodes_fc[0]: tensor([ 0.0533, -0.0946], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1282,  0.0382], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1328, -0.0155], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0630], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0393, -0.0689], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0943, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:43 step:8 train loss:0.495890, train acc:85.126, train f1:84.097, train precision:85.456, train recall:84.387, train kappa:84.559
self.nodes_fc[0]: tensor([ 0.0533, -0.0947], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1283,  0.0382], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1329, -0.0153], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0631], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0392, -0.0689], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0943, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:43 step:9 train loss:0.487535, train acc:85.388, train f1:84.510, train precision:85.832, train recall:84.803, train kappa:84.831
self.nodes_fc[0]: tensor([ 0.0533, -0.0948], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1284,  0.0383], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1330, -0.0151], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0631], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0688], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0942, -0.1237], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:43 step:10 train loss:0.486852, train acc:85.220, train f1:84.111, train precision:85.758, train recall:84.257, train kappa:84.645
self.nodes_fc[0]: tensor([ 0.0533, -0.0949], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1286,  0.0384], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1332, -0.0150], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0631], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0688], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0942, -0.1237], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:43 step:11 train loss:0.491498, train acc:84.963, train f1:84.632, train precision:85.997, train recall:84.705, train kappa:84.389
self.nodes_fc[0]: tensor([ 0.0534, -0.0950], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1287,  0.0385], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1333, -0.0150], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0633], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0687], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0942, -0.1238], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0534, -0.0950], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1287,  0.0385], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1333, -0.0150], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0633], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0687], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0942, -0.1238], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:43        valid loss:0.666739, valid acc:82.825, valid f1:59.561, valid precision:56.456, valid recall:70.379, valid kappa:80.660
[1;31mTest score increased (82.781606 --> 82.824545).[0m
[82.82454454372584, 59.56078518857354, 56.45576905355006, 70.37893960827745, 80.66002305777143]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0534, -0.0950], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1287,  0.0385], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1333, -0.0150], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0633], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0687], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0942, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:44 step:0 train loss:0.476520, train acc:85.751, train f1:84.692, train precision:86.397, train recall:84.883, train kappa:85.199
self.nodes_fc[0]: tensor([ 0.0536, -0.0951], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1288,  0.0385], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1335, -0.0150], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0636], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0388, -0.0686], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0942, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:44 step:1 train loss:0.488919, train acc:85.229, train f1:84.055, train precision:85.620, train recall:84.179, train kappa:84.647
self.nodes_fc[0]: tensor([ 0.0537, -0.0952], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1289,  0.0386], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1336, -0.0149], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0639], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0387, -0.0684], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0942, -0.1239], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:44 step:2 train loss:0.490080, train acc:85.184, train f1:84.261, train precision:85.523, train recall:84.510, train kappa:84.618
self.nodes_fc[0]: tensor([ 0.0538, -0.0953], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1289,  0.0386], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1337, -0.0148], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0641], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0387, -0.0683], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0941, -0.1240], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:44 step:3 train loss:0.487170, train acc:85.229, train f1:84.234, train precision:85.309, train recall:84.647, train kappa:84.668
self.nodes_fc[0]: tensor([ 0.0539, -0.0953], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1290,  0.0386], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1338, -0.0148], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0643], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0681], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0941, -0.1240], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:44 step:4 train loss:0.475946, train acc:85.776, train f1:84.674, train precision:86.005, train recall:84.960, train kappa:85.217
self.nodes_fc[0]: tensor([ 0.0540, -0.0952], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1290,  0.0386], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1337, -0.0148], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0644], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0679], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0941, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:44 step:5 train loss:0.486318, train acc:85.168, train f1:84.343, train precision:85.462, train recall:84.608, train kappa:84.601
self.nodes_fc[0]: tensor([ 0.0542, -0.0951], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1291,  0.0386], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1337, -0.0147], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1159, 0.0646], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0677], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0940, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:44 step:6 train loss:0.477252, train acc:85.699, train f1:84.779, train precision:86.251, train recall:84.932, train kappa:85.157
self.nodes_fc[0]: tensor([ 0.0544, -0.0951], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1292,  0.0385], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1337, -0.0147], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1158, 0.0648], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0675], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0940, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:44 step:7 train loss:0.483998, train acc:85.455, train f1:84.629, train precision:86.187, train recall:84.641, train kappa:84.889
self.nodes_fc[0]: tensor([ 0.0545, -0.0951], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1292,  0.0384], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1338, -0.0147], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1158, 0.0650], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0672], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0940, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:44 step:8 train loss:0.472596, train acc:85.382, train f1:84.539, train precision:86.207, train recall:84.586, train kappa:84.821
self.nodes_fc[0]: tensor([ 0.0547, -0.0953], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1293,  0.0384], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1340, -0.0146], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1158, 0.0652], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0385, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0940, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:44 step:9 train loss:0.487117, train acc:85.406, train f1:84.369, train precision:86.031, train recall:84.362, train kappa:84.839
self.nodes_fc[0]: tensor([ 0.0548, -0.0954], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1294,  0.0384], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0147], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1158, 0.0654], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0384, -0.0669], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0939, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:44 step:10 train loss:0.483993, train acc:85.455, train f1:84.616, train precision:86.261, train recall:84.655, train kappa:84.896
self.nodes_fc[0]: tensor([ 0.0549, -0.0955], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1294,  0.0385], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0149], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1158, 0.0656], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0383, -0.0669], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0939, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:44 step:11 train loss:0.488488, train acc:85.040, train f1:84.201, train precision:85.497, train recall:84.386, train kappa:84.472
self.nodes_fc[0]: tensor([ 0.0550, -0.0956], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1295,  0.0386], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0151], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0659], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0383, -0.0670], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0939, -0.1242], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0550, -0.0956], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1295,  0.0386], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0151], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0659], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0383, -0.0670], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0939, -0.1242], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:44        valid loss:0.669486, valid acc:82.843, valid f1:59.603, valid precision:56.450, valid recall:70.498, valid kappa:80.675
[1;31mTest score increased (82.824545 --> 82.842947).[0m
[82.842946817429, 59.60251831453186, 56.44995419883693, 70.49828402145647, 80.67485946817602]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0550, -0.0956], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1295,  0.0386], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0151], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0659], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0383, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0939, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:45 step:0 train loss:0.484658, train acc:85.580, train f1:84.579, train precision:86.110, train recall:84.820, train kappa:85.023
self.nodes_fc[0]: tensor([ 0.0551, -0.0956], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1295,  0.0387], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0152], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0662], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0382, -0.0671], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0939, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:45 step:1 train loss:0.475483, train acc:85.571, train f1:84.712, train precision:85.807, train recall:85.182, train kappa:85.019
self.nodes_fc[0]: tensor([ 0.0553, -0.0957], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1296,  0.0388], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0153], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0663], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0382, -0.0672], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0939, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:45 step:2 train loss:0.474915, train acc:85.666, train f1:84.721, train precision:86.377, train recall:84.995, train kappa:85.118
self.nodes_fc[0]: tensor([ 0.0555, -0.0957], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1295,  0.0389], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0153], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0665], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0382, -0.0671], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0939, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:45 step:3 train loss:0.471272, train acc:85.907, train f1:84.788, train precision:86.230, train recall:85.095, train kappa:85.363
self.nodes_fc[0]: tensor([ 0.0557, -0.0958], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1295,  0.0390], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1343, -0.0152], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0668], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0382, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0939, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:45 step:4 train loss:0.471163, train acc:85.678, train f1:84.791, train precision:86.420, train recall:84.885, train kappa:85.132
self.nodes_fc[0]: tensor([ 0.0558, -0.0958], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1296,  0.0392], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0151], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1167, 0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0381, -0.0669], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0939, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:45 step:5 train loss:0.477984, train acc:85.599, train f1:84.714, train precision:86.499, train recall:84.811, train kappa:85.033
self.nodes_fc[0]: tensor([ 0.0560, -0.0958], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1295,  0.0393], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0151], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1168, 0.0671], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0381, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0939, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:45 step:6 train loss:0.485030, train acc:85.275, train f1:84.474, train precision:85.994, train recall:84.452, train kappa:84.703
self.nodes_fc[0]: tensor([ 0.0561, -0.0956], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1293,  0.0394], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0151], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1168, 0.0672], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0381, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0939, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:45 step:7 train loss:0.478680, train acc:85.571, train f1:84.451, train precision:85.829, train recall:84.623, train kappa:85.016
self.nodes_fc[0]: tensor([ 0.0561, -0.0954], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1292,  0.0395], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0150], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1168, 0.0672], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0382, -0.0671], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0940, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:45 step:8 train loss:0.480700, train acc:85.529, train f1:84.580, train precision:86.050, train recall:84.783, train kappa:84.967
self.nodes_fc[0]: tensor([ 0.0562, -0.0953], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1292,  0.0396], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0149], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1168, 0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0382, -0.0671], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0940, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:45 step:9 train loss:0.476510, train acc:85.587, train f1:84.473, train precision:85.738, train recall:84.711, train kappa:85.040
self.nodes_fc[0]: tensor([ 0.0561, -0.0952], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1292,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0147], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1168, 0.0669], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0381, -0.0673], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0940, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:45 step:10 train loss:0.484415, train acc:85.455, train f1:84.500, train precision:86.079, train recall:84.602, train kappa:84.901
self.nodes_fc[0]: tensor([ 0.0561, -0.0951], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1292,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0146], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1169, 0.0667], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0382, -0.0674], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0940, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:45 step:11 train loss:0.475577, train acc:85.667, train f1:84.412, train precision:86.067, train recall:84.707, train kappa:85.111
self.nodes_fc[0]: tensor([ 0.0561, -0.0950], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1293,  0.0400], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0144], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1168, 0.0664], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0382, -0.0674], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0940, -0.1243], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0561, -0.0950], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1293,  0.0400], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0144], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1168, 0.0664], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0382, -0.0674], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0940, -0.1243], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:45        valid loss:0.666455, valid acc:83.004, valid f1:59.872, valid precision:56.934, valid recall:70.279, valid kappa:80.856
[1;31mTest score increased (82.842947 --> 83.004478).[0m
[83.00447788660111, 59.87178510500585, 56.93416683691138, 70.27889712213278, 80.85592117153564]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0561, -0.0950], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1293,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0144], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1168, 0.0664], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0382, -0.0674], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0940, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:46 step:0 train loss:0.477373, train acc:85.458, train f1:84.460, train precision:85.957, train recall:84.693, train kappa:84.900
self.nodes_fc[0]: tensor([ 0.0560, -0.0950], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1294,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0143], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1169, 0.0661], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0382, -0.0675], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0940, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:46 step:1 train loss:0.471821, train acc:85.889, train f1:84.787, train precision:86.374, train recall:85.039, train kappa:85.348
self.nodes_fc[0]: tensor([ 0.0559, -0.0951], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1294,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0142], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1170, 0.0658], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0383, -0.0677], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0941, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:46 step:2 train loss:0.461468, train acc:85.947, train f1:84.718, train precision:86.622, train recall:84.983, train kappa:85.401
self.nodes_fc[0]: tensor([ 0.0558, -0.0951], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1294,  0.0402], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0142], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1170, 0.0654], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0384, -0.0679], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0941, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:46 step:3 train loss:0.471503, train acc:85.574, train f1:84.805, train precision:86.357, train recall:85.154, train kappa:85.016
self.nodes_fc[0]: tensor([ 0.0557, -0.0951], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1293,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0143], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1170, 0.0653], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0385, -0.0680], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0941, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:46 step:4 train loss:0.481185, train acc:85.388, train f1:84.440, train precision:85.992, train recall:84.584, train kappa:84.826
self.nodes_fc[0]: tensor([ 0.0556, -0.0950], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1293,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0145], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1169, 0.0650], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0385, -0.0681], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0941, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:46 step:5 train loss:0.466253, train acc:85.999, train f1:84.990, train precision:86.684, train recall:84.991, train kappa:85.464
self.nodes_fc[0]: tensor([ 0.0555, -0.0949], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1293,  0.0404], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0147], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1170, 0.0648], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0384, -0.0682], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0941, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:46 step:6 train loss:0.480045, train acc:85.724, train f1:84.765, train precision:86.291, train recall:84.780, train kappa:85.176
self.nodes_fc[0]: tensor([ 0.0554, -0.0949], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1292,  0.0404], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1343, -0.0149], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1169, 0.0645], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0384, -0.0684], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0941, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:46 step:7 train loss:0.475299, train acc:85.553, train f1:84.697, train precision:86.234, train recall:84.551, train kappa:84.996
self.nodes_fc[0]: tensor([ 0.0552, -0.0948], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1293,  0.0404], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0150], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1168, 0.0642], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0383, -0.0685], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0941, -0.1240], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:46 step:8 train loss:0.475209, train acc:85.764, train f1:84.886, train precision:86.122, train recall:85.002, train kappa:85.220
self.nodes_fc[0]: tensor([ 0.0551, -0.0946], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1294,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1340, -0.0149], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1167, 0.0639], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0382, -0.0685], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0941, -0.1239], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:46 step:9 train loss:0.471953, train acc:85.391, train f1:84.758, train precision:85.809, train recall:85.036, train kappa:84.829
self.nodes_fc[0]: tensor([ 0.0550, -0.0945], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1296,  0.0402], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1337, -0.0149], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0636], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0381, -0.0685], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0941, -0.1239], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:46 step:10 train loss:0.472366, train acc:85.852, train f1:84.836, train precision:86.313, train recall:85.182, train kappa:85.300
self.nodes_fc[0]: tensor([ 0.0550, -0.0943], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1297,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1335, -0.0148], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0634], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0380, -0.0684], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0941, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:46 step:11 train loss:0.459108, train acc:85.899, train f1:84.709, train precision:85.926, train recall:84.876, train kappa:85.348
self.nodes_fc[0]: tensor([ 0.0551, -0.0942], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1297,  0.0400], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1333, -0.0148], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0634], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0378, -0.0684], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0941, -0.1238], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0551, -0.0942], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1297,  0.0400], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1333, -0.0148], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0634], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0378, -0.0684], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0941, -0.1238], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:46        valid loss:0.670690, valid acc:82.990, valid f1:59.637, valid precision:56.352, valid recall:70.454, valid kappa:80.849
[1;31mEarlyStopping counter: 1 out of 50[0m
[83.00447788660111, 59.87178510500585, 56.93416683691138, 70.27889712213278, 80.85592117153564]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0551, -0.0942], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1297,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1333, -0.0148], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0634], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0378, -0.0684], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0941, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:47 step:0 train loss:0.467927, train acc:85.825, train f1:84.630, train precision:85.812, train recall:84.948, train kappa:85.282
self.nodes_fc[0]: tensor([ 0.0552, -0.0941], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1296,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1331, -0.0149], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1159, 0.0635], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0683], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0941, -0.1237], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:47 step:1 train loss:0.463856, train acc:86.026, train f1:85.433, train precision:87.056, train recall:85.536, train kappa:85.488
self.nodes_fc[0]: tensor([ 0.0552, -0.0941], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1295,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1330, -0.0149], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1157, 0.0636], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0376, -0.0683], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0940, -0.1237], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:47 step:2 train loss:0.468959, train acc:85.699, train f1:84.769, train precision:86.263, train recall:84.926, train kappa:85.157
self.nodes_fc[0]: tensor([ 0.0552, -0.0941], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1294,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1330, -0.0149], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1155, 0.0636], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0375, -0.0683], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0940, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:47 step:3 train loss:0.463560, train acc:85.916, train f1:84.908, train precision:86.431, train recall:85.041, train kappa:85.370
self.nodes_fc[0]: tensor([ 0.0553, -0.0941], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1292,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1330, -0.0148], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1152, 0.0636], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0374, -0.0683], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0939, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:47 step:4 train loss:0.464104, train acc:85.852, train f1:84.783, train precision:86.053, train recall:85.045, train kappa:85.314
self.nodes_fc[0]: tensor([ 0.0553, -0.0942], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1290,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1329, -0.0148], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1151, 0.0636], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0374, -0.0683], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0939, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:47 step:5 train loss:0.463051, train acc:85.764, train f1:84.982, train precision:86.295, train recall:85.173, train kappa:85.222
self.nodes_fc[0]: tensor([ 0.0554, -0.0942], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1288,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1328, -0.0148], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1150, 0.0637], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0373, -0.0683], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0939, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:47 step:6 train loss:0.472106, train acc:85.919, train f1:85.102, train precision:86.421, train recall:85.225, train kappa:85.374
self.nodes_fc[0]: tensor([ 0.0553, -0.0943], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1285,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1326, -0.0148], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1148, 0.0638], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0373, -0.0682], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0938, -0.1237], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:47 step:7 train loss:0.464224, train acc:85.971, train f1:84.906, train precision:86.378, train recall:85.156, train kappa:85.424
self.nodes_fc[0]: tensor([ 0.0553, -0.0943], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1284,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1326, -0.0149], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1147, 0.0639], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0373, -0.0681], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0938, -0.1237], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:47 step:8 train loss:0.466211, train acc:85.623, train f1:84.549, train precision:85.844, train recall:84.823, train kappa:85.054
self.nodes_fc[0]: tensor([ 0.0551, -0.0944], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1283,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1326, -0.0149], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1146, 0.0640], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0373, -0.0681], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0938, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:47 step:9 train loss:0.471792, train acc:85.498, train f1:84.434, train precision:85.699, train recall:84.741, train kappa:84.944
self.nodes_fc[0]: tensor([ 0.0550, -0.0944], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1283,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1325, -0.0148], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1145, 0.0639], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0373, -0.0679], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0938, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:47 step:10 train loss:0.465068, train acc:85.815, train f1:85.036, train precision:86.304, train recall:85.166, train kappa:85.280
self.nodes_fc[0]: tensor([ 0.0549, -0.0944], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1282,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1324, -0.0149], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1144, 0.0640], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0373, -0.0677], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0938, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:47 step:11 train loss:0.497722, train acc:85.011, train f1:84.047, train precision:85.404, train recall:84.499, train kappa:84.435
self.nodes_fc[0]: tensor([ 0.0546, -0.0944], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1280,  0.0399], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1324, -0.0148], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1143, 0.0640], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0372, -0.0677], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0938, -0.1238], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0546, -0.0944], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1280,  0.0399], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1324, -0.0148], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1143, 0.0640], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0372, -0.0677], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0938, -0.1238], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:47        valid loss:0.662698, valid acc:83.062, valid f1:59.910, valid precision:56.759, valid recall:70.455, valid kappa:80.925
[1;31mTest score increased (83.004478 --> 83.061729).[0m
[83.06172940478868, 59.910359055498105, 56.75883838488184, 70.4554910256711, 80.92459994043514]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0546, -0.0944], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1280,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1324, -0.0148], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1143, 0.0640], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0372, -0.0677], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0938, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:48 step:0 train loss:0.459996, train acc:85.965, train f1:85.043, train precision:86.462, train recall:85.218, train kappa:85.428
self.nodes_fc[0]: tensor([ 0.0544, -0.0944], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1279,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1325, -0.0146], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1143, 0.0641], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0371, -0.0677], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0938, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:48 step:1 train loss:0.452382, train acc:86.285, train f1:85.293, train precision:86.926, train recall:85.253, train kappa:85.757
self.nodes_fc[0]: tensor([ 0.0541, -0.0944], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1278,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1326, -0.0145], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1143, 0.0641], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0371, -0.0677], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0938, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:48 step:2 train loss:0.462071, train acc:85.904, train f1:85.064, train precision:86.574, train recall:85.153, train kappa:85.360
self.nodes_fc[0]: tensor([ 0.0539, -0.0943], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1276,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1328, -0.0143], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1144, 0.0642], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0372, -0.0677], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0938, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:48 step:3 train loss:0.460822, train acc:86.041, train f1:85.251, train precision:86.579, train recall:85.485, train kappa:85.496
self.nodes_fc[0]: tensor([ 0.0537, -0.0942], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1330, -0.0144], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1144, 0.0641], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0372, -0.0677], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0939, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:48 step:4 train loss:0.464345, train acc:85.941, train f1:85.045, train precision:86.606, train recall:84.956, train kappa:85.402
self.nodes_fc[0]: tensor([ 0.0535, -0.0942], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1274,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1332, -0.0141], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1145, 0.0639], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0373, -0.0676], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0939, -0.1237], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:48 step:5 train loss:0.459738, train acc:85.892, train f1:84.870, train precision:86.163, train recall:85.039, train kappa:85.356
self.nodes_fc[0]: tensor([ 0.0533, -0.0942], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1334, -0.0138], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1145, 0.0638], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0373, -0.0676], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0938, -0.1237], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:48 step:6 train loss:0.466747, train acc:85.754, train f1:84.776, train precision:86.161, train recall:85.063, train kappa:85.202
self.nodes_fc[0]: tensor([ 0.0533, -0.0942], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1336, -0.0135], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1146, 0.0636], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0373, -0.0674], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0938, -0.1237], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:48 step:7 train loss:0.467595, train acc:85.886, train f1:84.991, train precision:86.307, train recall:85.288, train kappa:85.350
self.nodes_fc[0]: tensor([ 0.0532, -0.0942], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1277,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1338, -0.0131], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1147, 0.0634], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0374, -0.0673], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0938, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:48 step:8 train loss:0.455195, train acc:86.234, train f1:85.187, train precision:86.394, train recall:85.325, train kappa:85.704
self.nodes_fc[0]: tensor([ 0.0531, -0.0942], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1277,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1340, -0.0130], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1148, 0.0632], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0375, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0938, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:48 step:9 train loss:0.466648, train acc:85.855, train f1:85.147, train precision:87.023, train recall:85.216, train kappa:85.313
self.nodes_fc[0]: tensor([ 0.0529, -0.0942], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1278,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0130], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1150, 0.0631], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0668], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0937, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:48 step:10 train loss:0.473865, train acc:85.715, train f1:85.030, train precision:86.360, train recall:85.221, train kappa:85.163
self.nodes_fc[0]: tensor([ 0.0529, -0.0942], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1278,  0.0402], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0132], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1152, 0.0631], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0378, -0.0666], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0937, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:48 step:11 train loss:0.453495, train acc:86.015, train f1:85.153, train precision:86.709, train recall:85.105, train kappa:85.476
self.nodes_fc[0]: tensor([ 0.0528, -0.0941], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1278,  0.0402], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0134], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1153, 0.0633], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0664], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0937, -0.1235], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0528, -0.0941], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1278,  0.0402], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0134], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1153, 0.0633], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0664], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0937, -0.1235], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:48        valid loss:0.666071, valid acc:83.150, valid f1:60.049, valid precision:56.932, valid recall:70.308, valid kappa:81.018
[1;31mTest score increased (83.061729 --> 83.149651).[0m
[83.14965137914818, 60.04944868903355, 56.931616715132925, 70.30828155857978, 81.01845807891702]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0528, -0.0941], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1278,  0.0402], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0134], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1153, 0.0633], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0664], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0937, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:49 step:0 train loss:0.455209, train acc:86.145, train f1:85.357, train precision:86.662, train recall:85.575, train kappa:85.613
self.nodes_fc[0]: tensor([ 0.0528, -0.0940], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1277,  0.0402], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0136], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1155, 0.0634], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0662], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0936, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:49 step:1 train loss:0.448376, train acc:86.432, train f1:85.387, train precision:86.705, train recall:85.689, train kappa:85.922
self.nodes_fc[0]: tensor([ 0.0527, -0.0939], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1276,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0137], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1155, 0.0635], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0660], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0935, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:49 step:2 train loss:0.459172, train acc:85.803, train f1:84.872, train precision:86.067, train recall:85.225, train kappa:85.256
self.nodes_fc[0]: tensor([ 0.0526, -0.0939], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1276,  0.0404], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1348, -0.0137], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1157, 0.0636], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0659], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0935, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:49 step:3 train loss:0.458206, train acc:86.261, train f1:85.421, train precision:86.572, train recall:85.679, train kappa:85.739
self.nodes_fc[0]: tensor([ 0.0526, -0.0939], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1276,  0.0405], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1351, -0.0136], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1158, 0.0637], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0658], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0934, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:49 step:4 train loss:0.453280, train acc:86.133, train f1:85.096, train precision:86.470, train recall:85.283, train kappa:85.593
self.nodes_fc[0]: tensor([ 0.0525, -0.0939], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0406], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1353, -0.0137], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1159, 0.0638], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0658], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0933, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:49 step:5 train loss:0.460158, train acc:86.002, train f1:85.261, train precision:86.351, train recall:85.573, train kappa:85.470
self.nodes_fc[0]: tensor([ 0.0525, -0.0938], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0406], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1355, -0.0138], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0639], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0378, -0.0658], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0933, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:49 step:6 train loss:0.452274, train acc:86.188, train f1:85.143, train precision:86.608, train recall:85.218, train kappa:85.645
self.nodes_fc[0]: tensor([ 0.0524, -0.0936], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0405], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1356, -0.0139], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0639], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0378, -0.0658], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0933, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:49 step:7 train loss:0.468413, train acc:85.651, train f1:85.103, train precision:86.844, train recall:85.030, train kappa:85.102
self.nodes_fc[0]: tensor([ 0.0524, -0.0935], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0404], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1356, -0.0139], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0640], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0379, -0.0658], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0933, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:49 step:8 train loss:0.454377, train acc:86.252, train f1:85.277, train precision:86.839, train recall:85.509, train kappa:85.718
self.nodes_fc[0]: tensor([ 0.0524, -0.0935], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1276,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1356, -0.0139], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0640], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0379, -0.0657], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0933, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:49 step:9 train loss:0.457980, train acc:86.145, train f1:85.315, train precision:86.642, train recall:85.490, train kappa:85.620
self.nodes_fc[0]: tensor([ 0.0525, -0.0934], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1276,  0.0402], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1356, -0.0139], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0641], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0379, -0.0655], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0933, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:49 step:10 train loss:0.466283, train acc:85.754, train f1:84.867, train precision:86.245, train recall:84.933, train kappa:85.205
self.nodes_fc[0]: tensor([ 0.0525, -0.0935], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1276,  0.0402], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1355, -0.0140], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0642], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0379, -0.0655], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0933, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:49 step:11 train loss:0.475997, train acc:85.552, train f1:84.469, train precision:85.313, train recall:85.006, train kappa:84.987
self.nodes_fc[0]: tensor([ 0.0525, -0.0936], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1276,  0.0401], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1354, -0.0141], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0642], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0379, -0.0656], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0932, -0.1234], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0525, -0.0936], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1276,  0.0401], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1354, -0.0141], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0642], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0379, -0.0656], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0932, -0.1234], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:49        valid loss:0.673070, valid acc:82.957, valid f1:59.842, valid precision:56.714, valid recall:70.555, valid kappa:80.804
[1;31mEarlyStopping counter: 1 out of 50[0m
[83.14965137914818, 60.04944868903355, 56.931616715132925, 70.30828155857978, 81.01845807891702]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0525, -0.0936], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1276,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1354, -0.0141], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0642], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0379, -0.0656], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0932, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:50 step:0 train loss:0.444678, train acc:86.392, train f1:85.382, train precision:86.582, train recall:85.750, train kappa:85.867
self.nodes_fc[0]: tensor([ 0.0526, -0.0937], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1353, -0.0142], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0641], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0379, -0.0657], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0932, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:50 step:1 train loss:0.455517, train acc:86.133, train f1:85.162, train precision:86.426, train recall:85.395, train kappa:85.603
self.nodes_fc[0]: tensor([ 0.0528, -0.0938], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1352, -0.0143], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0640], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0380, -0.0657], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0932, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:50 step:2 train loss:0.457786, train acc:86.124, train f1:85.389, train precision:86.923, train recall:85.518, train kappa:85.590
self.nodes_fc[0]: tensor([ 0.0530, -0.0938], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1352, -0.0143], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0637], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0380, -0.0657], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0932, -0.1233], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:50 step:3 train loss:0.444928, train acc:86.554, train f1:85.461, train precision:86.995, train recall:85.574, train kappa:86.032
self.nodes_fc[0]: tensor([ 0.0532, -0.0939], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1352, -0.0143], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0635], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0380, -0.0657], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0932, -0.1233], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:50 step:4 train loss:0.457691, train acc:86.026, train f1:85.178, train precision:86.633, train recall:85.301, train kappa:85.500
self.nodes_fc[0]: tensor([ 0.0534, -0.0938], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1276,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1351, -0.0143], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0633], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0381, -0.0658], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0933, -0.1233], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:50 step:5 train loss:0.459991, train acc:85.876, train f1:84.974, train precision:86.198, train recall:85.257, train kappa:85.347
self.nodes_fc[0]: tensor([ 0.0536, -0.0936], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1350, -0.0144], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0633], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0382, -0.0660], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0933, -0.1233], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:50 step:6 train loss:0.445495, train acc:86.459, train f1:85.646, train precision:86.865, train recall:85.900, train kappa:85.932
self.nodes_fc[0]: tensor([ 0.0539, -0.0935], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1348, -0.0146], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1159, 0.0632], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0383, -0.0662], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0933, -0.1232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:50 step:7 train loss:0.457855, train acc:86.023, train f1:85.152, train precision:86.267, train recall:85.462, train kappa:85.483
self.nodes_fc[0]: tensor([ 0.0541, -0.0933], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1276,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1347, -0.0148], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1159, 0.0631], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0384, -0.0664], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0933, -0.1232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:50 step:8 train loss:0.446868, train acc:86.243, train f1:85.307, train precision:86.703, train recall:85.565, train kappa:85.727
self.nodes_fc[0]: tensor([ 0.0544, -0.0932], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1276,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1346, -0.0149], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1159, 0.0632], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0385, -0.0666], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0933, -0.1232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:50 step:9 train loss:0.463439, train acc:85.999, train f1:85.102, train precision:86.200, train recall:85.448, train kappa:85.461
self.nodes_fc[0]: tensor([ 0.0546, -0.0931], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1276,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1346, -0.0150], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1158, 0.0631], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0667], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0933, -0.1232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:50 step:10 train loss:0.463136, train acc:85.822, train f1:84.902, train precision:86.302, train recall:85.190, train kappa:85.269
self.nodes_fc[0]: tensor([ 0.0549, -0.0930], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1346, -0.0150], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1158, 0.0630], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0667], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0933, -0.1232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:50 step:11 train loss:0.466308, train acc:85.803, train f1:84.857, train precision:86.862, train recall:85.044, train kappa:85.253
self.nodes_fc[0]: tensor([ 0.0551, -0.0930], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0400], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1346, -0.0149], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1156, 0.0628], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0668], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0933, -0.1232], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0551, -0.0930], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0400], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1346, -0.0149], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1156, 0.0628], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0668], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0933, -0.1232], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:50        valid loss:0.661426, valid acc:83.334, valid f1:60.204, valid precision:57.263, valid recall:70.228, valid kappa:81.219
[1;31mTest score increased (83.149651 --> 83.333674).[0m
[83.3336741161797, 60.20432386693222, 57.26306141943927, 70.22785119311243, 81.21948407573261]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0551, -0.0930], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1346, -0.0149], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1156, 0.0628], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0668], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0933, -0.1232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:51 step:0 train loss:0.449852, train acc:86.374, train f1:85.368, train precision:86.985, train recall:85.428, train kappa:85.841
self.nodes_fc[0]: tensor([ 0.0553, -0.0930], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1347, -0.0147], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1154, 0.0627], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0387, -0.0669], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0933, -0.1231], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:51 step:1 train loss:0.448761, train acc:86.246, train f1:85.518, train precision:86.748, train recall:85.725, train kappa:85.718
self.nodes_fc[0]: tensor([ 0.0554, -0.0930], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1348, -0.0145], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1152, 0.0625], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0387, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0933, -0.1231], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:51 step:2 train loss:0.442989, train acc:86.435, train f1:85.378, train precision:86.612, train recall:85.573, train kappa:85.912
self.nodes_fc[0]: tensor([ 0.0554, -0.0931], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0402], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1348, -0.0144], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1151, 0.0624], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0387, -0.0671], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0933, -0.1230], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:51 step:3 train loss:0.447985, train acc:86.191, train f1:85.302, train precision:86.481, train recall:85.521, train kappa:85.675
self.nodes_fc[0]: tensor([ 0.0554, -0.0931], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1275,  0.0402], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1349, -0.0143], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1151, 0.0623], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0387, -0.0671], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0934, -0.1230], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:51 step:4 train loss:0.453101, train acc:86.115, train f1:85.357, train precision:86.760, train recall:85.475, train kappa:85.588
self.nodes_fc[0]: tensor([ 0.0554, -0.0931], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1276,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1349, -0.0142], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1152, 0.0621], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0387, -0.0672], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0934, -0.1230], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:51 step:5 train loss:0.441459, train acc:86.475, train f1:85.565, train precision:86.939, train recall:85.657, train kappa:85.962
self.nodes_fc[0]: tensor([ 0.0555, -0.0932], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1277,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1349, -0.0140], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1152, 0.0620], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0387, -0.0673], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0934, -0.1231], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:51 step:6 train loss:0.440705, train acc:86.401, train f1:85.498, train precision:86.842, train recall:85.646, train kappa:85.870
self.nodes_fc[0]: tensor([ 0.0556, -0.0932], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1279,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1349, -0.0138], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1151, 0.0620], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0388, -0.0673], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0934, -0.1231], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:51 step:7 train loss:0.454401, train acc:85.956, train f1:85.065, train precision:86.320, train recall:85.222, train kappa:85.415
self.nodes_fc[0]: tensor([ 0.0557, -0.0932], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1280,  0.0402], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1349, -0.0137], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1151, 0.0620], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0388, -0.0673], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0934, -0.1231], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:51 step:8 train loss:0.450658, train acc:86.264, train f1:85.335, train precision:86.766, train recall:85.568, train kappa:85.723
self.nodes_fc[0]: tensor([ 0.0559, -0.0933], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1281,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1348, -0.0135], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1151, 0.0621], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0388, -0.0673], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0934, -0.1231], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:51 step:9 train loss:0.457191, train acc:85.861, train f1:85.251, train precision:86.756, train recall:85.509, train kappa:85.328
self.nodes_fc[0]: tensor([ 0.0561, -0.0932], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1281,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1347, -0.0135], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1151, 0.0620], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0674], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0935, -0.1232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:51 step:10 train loss:0.459875, train acc:86.130, train f1:85.185, train precision:86.570, train recall:85.417, train kappa:85.593
self.nodes_fc[0]: tensor([ 0.0562, -0.0932], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1281,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1348, -0.0134], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1149, 0.0621], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0388, -0.0674], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0935, -0.1232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:51 step:11 train loss:0.455643, train acc:86.285, train f1:85.292, train precision:86.724, train recall:85.584, train kappa:85.769
self.nodes_fc[0]: tensor([ 0.0564, -0.0932], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1281,  0.0399], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1349, -0.0133], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1147, 0.0620], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0388, -0.0673], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0935, -0.1232], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0564, -0.0932], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1281,  0.0399], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1349, -0.0133], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1147, 0.0620], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0388, -0.0673], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0935, -0.1232], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:51        valid loss:0.664784, valid acc:83.215, valid f1:59.937, valid precision:56.910, valid recall:70.442, valid kappa:81.096
[1;31mEarlyStopping counter: 1 out of 50[0m
[83.3336741161797, 60.20432386693222, 57.26306141943927, 70.22785119311243, 81.21948407573261]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0564, -0.0932], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1281,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1349, -0.0133], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1147, 0.0620], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0388, -0.0673], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0935, -0.1232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:52 step:0 train loss:0.442644, train acc:86.514, train f1:85.480, train precision:86.826, train recall:85.826, train kappa:86.000
self.nodes_fc[0]: tensor([ 0.0566, -0.0931], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1282,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1350, -0.0132], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1145, 0.0620], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0387, -0.0672], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0934, -0.1231], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:52 step:1 train loss:0.443344, train acc:86.325, train f1:85.505, train precision:86.807, train recall:85.723, train kappa:85.802
self.nodes_fc[0]: tensor([ 0.0566, -0.0931], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1281,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1351, -0.0133], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1142, 0.0620], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0934, -0.1232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:52 step:2 train loss:0.441670, train acc:86.707, train f1:85.818, train precision:87.119, train recall:86.002, train kappa:86.202
self.nodes_fc[0]: tensor([ 0.0567, -0.0931], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1281,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1351, -0.0133], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1141, 0.0620], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0669], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0934, -0.1233], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:52 step:3 train loss:0.449348, train acc:86.313, train f1:85.363, train precision:86.735, train recall:85.435, train kappa:85.784
self.nodes_fc[0]: tensor([ 0.0569, -0.0932], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1281,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1352, -0.0130], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1140, 0.0622], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0385, -0.0668], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0933, -0.1233], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:52 step:4 train loss:0.455139, train acc:86.285, train f1:85.326, train precision:86.599, train recall:85.490, train kappa:85.754
self.nodes_fc[0]: tensor([ 0.0569, -0.0933], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1283,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1353, -0.0128], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1139, 0.0624], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0385, -0.0667], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0933, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:52 step:5 train loss:0.451918, train acc:86.166, train f1:85.268, train precision:86.623, train recall:85.300, train kappa:85.634
self.nodes_fc[0]: tensor([ 0.0571, -0.0932], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1284,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1353, -0.0125], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1139, 0.0626], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0385, -0.0666], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0933, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:52 step:6 train loss:0.442186, train acc:86.487, train f1:85.639, train precision:87.472, train recall:85.509, train kappa:85.972
self.nodes_fc[0]: tensor([ 0.0572, -0.0932], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1285,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1352, -0.0123], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1141, 0.0628], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0385, -0.0665], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0932, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:52 step:7 train loss:0.447813, train acc:86.188, train f1:85.450, train precision:86.973, train recall:85.411, train kappa:85.649
self.nodes_fc[0]: tensor([ 0.0573, -0.0932], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1287,  0.0396], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1352, -0.0121], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1143, 0.0630], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0385, -0.0666], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0932, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:52 step:8 train loss:0.452462, train acc:86.145, train f1:85.316, train precision:86.702, train recall:85.567, train kappa:85.615
self.nodes_fc[0]: tensor([ 0.0575, -0.0933], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1289,  0.0395], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1350, -0.0118], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1147, 0.0633], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0666], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0932, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:52 step:9 train loss:0.451045, train acc:86.215, train f1:85.211, train precision:86.177, train recall:85.866, train kappa:85.697
self.nodes_fc[0]: tensor([ 0.0576, -0.0932], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1290,  0.0395], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1348, -0.0116], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1150, 0.0635], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0665], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0932, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:52 step:10 train loss:0.458146, train acc:86.176, train f1:85.333, train precision:86.749, train recall:85.700, train kappa:85.643
self.nodes_fc[0]: tensor([ 0.0577, -0.0931], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1291,  0.0395], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1346, -0.0114], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1153, 0.0638], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0665], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0932, -0.1233], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:52 step:11 train loss:0.446380, train acc:86.353, train f1:85.315, train precision:86.409, train recall:85.619, train kappa:85.829
self.nodes_fc[0]: tensor([ 0.0579, -0.0930], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1293,  0.0395], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0113], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1156, 0.0640], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0385, -0.0665], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0932, -0.1233], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0579, -0.0930], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1293,  0.0395], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0113], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1156, 0.0640], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0385, -0.0665], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0932, -0.1233], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:52        valid loss:0.656869, valid acc:83.495, valid f1:60.444, valid precision:57.430, valid recall:70.534, valid kappa:81.405
[1;31mTest score increased (83.333674 --> 83.495205).[0m
[83.4952051853518, 60.44430949677411, 57.429957327676185, 70.53367820991735, 81.40520242504682]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0579, -0.0930], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1293,  0.0395], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0113], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1156, 0.0640], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0385, -0.0665], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0932, -0.1233], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:53 step:0 train loss:0.435655, train acc:86.642, train f1:85.916, train precision:87.439, train recall:85.946, train kappa:86.125
self.nodes_fc[0]: tensor([ 0.0580, -0.0929], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1295,  0.0395], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0114], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1159, 0.0643], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0384, -0.0664], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0932, -0.1233], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:53 step:1 train loss:0.443053, train acc:86.328, train f1:85.518, train precision:87.134, train recall:85.541, train kappa:85.803
self.nodes_fc[0]: tensor([ 0.0581, -0.0929], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1296,  0.0396], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0114], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0645], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0383, -0.0663], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0932, -0.1232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:53 step:2 train loss:0.438159, train acc:86.685, train f1:85.529, train precision:87.270, train recall:85.468, train kappa:86.173
self.nodes_fc[0]: tensor([ 0.0581, -0.0929], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1297,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0113], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0648], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0383, -0.0662], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0931, -0.1232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:53 step:3 train loss:0.447189, train acc:86.362, train f1:85.495, train precision:86.893, train recall:85.646, train kappa:85.832
self.nodes_fc[0]: tensor([ 0.0582, -0.0929], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1298,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0112], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0651], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0382, -0.0662], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0931, -0.1232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:53 step:4 train loss:0.449523, train acc:86.273, train f1:85.550, train precision:86.789, train recall:85.559, train kappa:85.756
self.nodes_fc[0]: tensor([ 0.0583, -0.0930], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1299,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0111], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0655], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0381, -0.0662], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0931, -0.1232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:53 step:5 train loss:0.445506, train acc:86.472, train f1:85.657, train precision:86.623, train recall:85.783, train kappa:85.947
self.nodes_fc[0]: tensor([ 0.0583, -0.0930], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1299,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0112], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0658], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0380, -0.0662], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0931, -0.1232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:53 step:6 train loss:0.440785, train acc:86.441, train f1:85.569, train precision:86.441, train recall:85.856, train kappa:85.928
self.nodes_fc[0]: tensor([ 0.0584, -0.0930], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1299,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1343, -0.0115], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0660], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0379, -0.0661], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0931, -0.1232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:53 step:7 train loss:0.439615, train acc:86.331, train f1:85.333, train precision:86.389, train recall:85.575, train kappa:85.807
self.nodes_fc[0]: tensor([ 0.0584, -0.0930], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1298,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0118], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0661], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0378, -0.0661], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0931, -0.1232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:53 step:8 train loss:0.455303, train acc:85.925, train f1:85.201, train precision:86.418, train recall:85.510, train kappa:85.384
self.nodes_fc[0]: tensor([ 0.0583, -0.0930], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1298,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0120], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0662], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0377, -0.0661], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0931, -0.1232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:53 step:9 train loss:0.440948, train acc:86.353, train f1:85.453, train precision:86.864, train recall:85.622, train kappa:85.833
self.nodes_fc[0]: tensor([ 0.0583, -0.0930], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1298,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0123], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0663], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0376, -0.0661], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0931, -0.1232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:53 step:10 train loss:0.437293, train acc:86.676, train f1:85.715, train precision:86.806, train recall:86.161, train kappa:86.169
self.nodes_fc[0]: tensor([ 0.0583, -0.0930], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1297,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0127], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0664], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0375, -0.0663], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0931, -0.1232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:53 step:11 train loss:0.443101, train acc:86.218, train f1:85.460, train precision:86.999, train recall:85.569, train kappa:85.681
self.nodes_fc[0]: tensor([ 0.0583, -0.0930], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1297,  0.0400], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0127], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0664], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0373, -0.0666], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0931, -0.1231], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0583, -0.0930], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1297,  0.0400], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0127], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0664], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0373, -0.0666], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0931, -0.1231], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:53        valid loss:0.663284, valid acc:83.344, valid f1:60.381, valid precision:57.279, valid recall:70.382, valid kappa:81.233
[1;31mEarlyStopping counter: 1 out of 50[0m
[83.4952051853518, 60.44430949677411, 57.429957327676185, 70.53367820991735, 81.40520242504682]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0583, -0.0930], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1297,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0127], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0664], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0373, -0.0666], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0931, -0.1231], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:54 step:0 train loss:0.441783, train acc:86.603, train f1:85.803, train precision:87.044, train recall:85.978, train kappa:86.088
self.nodes_fc[0]: tensor([ 0.0583, -0.0928], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1296,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0126], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0663], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0371, -0.0668], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0930, -0.1231], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:54 step:1 train loss:0.443547, train acc:86.563, train f1:85.595, train precision:86.767, train recall:85.705, train kappa:86.059
self.nodes_fc[0]: tensor([ 0.0583, -0.0927], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1295,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0125], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0662], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0370, -0.0669], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0930, -0.1230], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:54 step:2 train loss:0.445589, train acc:86.377, train f1:85.780, train precision:86.874, train recall:86.046, train kappa:85.859
self.nodes_fc[0]: tensor([ 0.0584, -0.0925], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1294,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0124], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0661], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0368, -0.0669], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0929, -0.1229], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:54 step:3 train loss:0.432703, train acc:86.685, train f1:85.908, train precision:86.878, train recall:86.116, train kappa:86.169
self.nodes_fc[0]: tensor([ 0.0584, -0.0925], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1293,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1343, -0.0124], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0659], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0367, -0.0671], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0929, -0.1229], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:54 step:4 train loss:0.434173, train acc:86.749, train f1:85.917, train precision:86.992, train recall:86.083, train kappa:86.248
self.nodes_fc[0]: tensor([ 0.0584, -0.0925], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1293,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1343, -0.0124], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0658], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0367, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0928, -0.1229], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:54 step:5 train loss:0.440756, train acc:86.429, train f1:85.671, train precision:86.941, train recall:85.825, train kappa:85.898
self.nodes_fc[0]: tensor([ 0.0584, -0.0925], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1292,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1343, -0.0124], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0656], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0366, -0.0668], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0928, -0.1228], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:54 step:6 train loss:0.443249, train acc:86.560, train f1:85.585, train precision:86.836, train recall:85.754, train kappa:86.033
self.nodes_fc[0]: tensor([ 0.0585, -0.0925], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1293,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1343, -0.0124], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0655], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0366, -0.0666], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0928, -0.1228], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:54 step:7 train loss:0.431269, train acc:86.572, train f1:85.852, train precision:87.448, train recall:85.818, train kappa:86.052
self.nodes_fc[0]: tensor([ 0.0585, -0.0925], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1294,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0122], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0652], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0365, -0.0665], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0927, -0.1228], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:54 step:8 train loss:0.444831, train acc:86.484, train f1:85.761, train precision:87.454, train recall:85.922, train kappa:85.966
self.nodes_fc[0]: tensor([ 0.0586, -0.0926], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1295,  0.0396], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0120], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1159, 0.0649], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0365, -0.0663], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0926, -0.1229], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:54 step:9 train loss:0.439711, train acc:86.438, train f1:85.389, train precision:86.714, train recall:85.668, train kappa:85.915
self.nodes_fc[0]: tensor([ 0.0585, -0.0927], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1296,  0.0396], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0118], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1156, 0.0647], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0365, -0.0661], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0926, -0.1229], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:54 step:10 train loss:0.440277, train acc:86.331, train f1:85.573, train precision:86.968, train recall:85.783, train kappa:85.817
self.nodes_fc[0]: tensor([ 0.0585, -0.0927], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1296,  0.0395], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0117], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1154, 0.0644], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0365, -0.0661], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0926, -0.1230], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:54 step:11 train loss:0.451816, train acc:86.218, train f1:85.236, train precision:85.970, train recall:85.595, train kappa:85.722
self.nodes_fc[0]: tensor([ 0.0585, -0.0927], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1296,  0.0396], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0117], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1152, 0.0641], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0365, -0.0659], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0925, -0.1230], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0585, -0.0927], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1296,  0.0396], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0117], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1152, 0.0641], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0365, -0.0659], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0925, -0.1230], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:54        valid loss:0.662367, valid acc:83.528, valid f1:60.392, valid precision:57.297, valid recall:70.619, valid kappa:81.446
[1;31mTest score increased (83.495205 --> 83.527920).[0m
[83.52792033860183, 60.39214202316813, 57.297385004057844, 70.61916598509504, 81.44622397668839]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0585, -0.0927], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1296,  0.0396], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0117], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1152, 0.0641], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0365, -0.0659], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0925, -0.1230], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:55 step:0 train loss:0.433991, train acc:86.829, train f1:85.965, train precision:86.927, train recall:86.286, train kappa:86.326
self.nodes_fc[0]: tensor([ 0.0585, -0.0926], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1295,  0.0396], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0120], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1150, 0.0638], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0366, -0.0658], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0925, -0.1230], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:55 step:1 train loss:0.434016, train acc:86.612, train f1:85.585, train precision:86.803, train recall:85.817, train kappa:86.092
self.nodes_fc[0]: tensor([ 0.0585, -0.0926], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1294,  0.0396], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0122], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1149, 0.0635], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0367, -0.0657], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0924, -0.1231], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:55 step:2 train loss:0.443056, train acc:86.316, train f1:85.255, train precision:86.901, train recall:85.205, train kappa:85.801
self.nodes_fc[0]: tensor([ 0.0584, -0.0926], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1293,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0124], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1148, 0.0632], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0367, -0.0655], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0924, -0.1231], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:55 step:3 train loss:0.435165, train acc:86.584, train f1:85.489, train precision:86.955, train recall:85.569, train kappa:86.067
self.nodes_fc[0]: tensor([ 0.0583, -0.0927], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1293,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0125], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1148, 0.0630], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0368, -0.0653], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0924, -0.1231], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:55 step:4 train loss:0.442864, train acc:86.377, train f1:85.687, train precision:87.217, train recall:85.818, train kappa:85.850
self.nodes_fc[0]: tensor([ 0.0582, -0.0927], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1293,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1343, -0.0125], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1147, 0.0628], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0370, -0.0652], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0923, -0.1230], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:55 step:5 train loss:0.446394, train acc:86.270, train f1:85.603, train precision:86.667, train recall:86.024, train kappa:85.745
self.nodes_fc[0]: tensor([ 0.0581, -0.0928], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1294,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0124], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1147, 0.0626], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0371, -0.0652], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0923, -0.1229], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:55 step:6 train loss:0.433768, train acc:86.673, train f1:85.724, train precision:86.626, train recall:86.037, train kappa:86.165
self.nodes_fc[0]: tensor([ 0.0579, -0.0928], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1296,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0124], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1146, 0.0625], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0372, -0.0651], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0922, -0.1229], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:55 step:7 train loss:0.437069, train acc:86.395, train f1:85.554, train precision:86.354, train recall:85.935, train kappa:85.872
self.nodes_fc[0]: tensor([ 0.0578, -0.0928], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1297,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0122], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1147, 0.0625], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0373, -0.0651], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0921, -0.1228], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:55 step:8 train loss:0.436550, train acc:86.725, train f1:85.793, train precision:87.109, train recall:85.928, train kappa:86.220
self.nodes_fc[0]: tensor([ 0.0577, -0.0927], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1297,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0121], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1147, 0.0624], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0374, -0.0651], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0921, -0.1227], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:55 step:9 train loss:0.430924, train acc:86.871, train f1:85.813, train precision:87.099, train recall:85.845, train kappa:86.365
self.nodes_fc[0]: tensor([ 0.0576, -0.0926], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1297,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0120], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1147, 0.0624], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0376, -0.0652], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0921, -0.1227], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:55 step:10 train loss:0.438831, train acc:86.618, train f1:86.032, train precision:87.604, train recall:86.148, train kappa:86.107
self.nodes_fc[0]: tensor([ 0.0575, -0.0924], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1297,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0120], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1147, 0.0624], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0376, -0.0653], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0921, -0.1227], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:55 step:11 train loss:0.431454, train acc:86.401, train f1:85.253, train precision:86.685, train recall:85.384, train kappa:85.887
self.nodes_fc[0]: tensor([ 0.0574, -0.0923], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1296,  0.0403], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1346, -0.0121], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1147, 0.0627], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0378, -0.0653], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0921, -0.1227], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0574, -0.0923], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1296,  0.0403], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1346, -0.0121], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1147, 0.0627], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0378, -0.0653], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0921, -0.1227], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:55        valid loss:0.664462, valid acc:83.544, valid f1:60.516, valid precision:57.462, valid recall:70.391, valid kappa:81.456
[1;31mTest score increased (83.527920 --> 83.544278).[0m
[83.54427791522686, 60.515675024565475, 57.461532086129694, 70.39101576308259, 81.45576978094549]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0574, -0.0923], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1296,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1346, -0.0121], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1147, 0.0627], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0378, -0.0653], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0921, -0.1227], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:56 step:0 train loss:0.424782, train acc:86.990, train f1:85.947, train precision:87.452, train recall:86.166, train kappa:86.491
self.nodes_fc[0]: tensor([ 0.0573, -0.0922], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1296,  0.0404], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1346, -0.0123], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1148, 0.0628], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0378, -0.0653], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0921, -0.1226], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:56 step:1 train loss:0.427925, train acc:86.752, train f1:85.870, train precision:87.330, train recall:85.996, train kappa:86.253
self.nodes_fc[0]: tensor([ 0.0572, -0.0922], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1296,  0.0405], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1346, -0.0125], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1148, 0.0630], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0379, -0.0653], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0921, -0.1226], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:56 step:2 train loss:0.430246, train acc:86.862, train f1:85.967, train precision:87.149, train recall:86.171, train kappa:86.355
self.nodes_fc[0]: tensor([ 0.0571, -0.0922], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1296,  0.0405], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0125], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1149, 0.0630], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0380, -0.0653], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0921, -0.1226], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:56 step:3 train loss:0.437033, train acc:86.600, train f1:85.669, train precision:86.739, train recall:85.972, train kappa:86.077
self.nodes_fc[0]: tensor([ 0.0571, -0.0923], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1297,  0.0405], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0125], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1149, 0.0630], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0381, -0.0654], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0921, -0.1227], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:56 step:4 train loss:0.420908, train acc:87.057, train f1:86.368, train precision:87.657, train recall:86.358, train kappa:86.572
self.nodes_fc[0]: tensor([ 0.0571, -0.0923], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1299,  0.0406], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0124], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1148, 0.0630], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0382, -0.0654], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0921, -0.1227], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:56 step:5 train loss:0.435699, train acc:86.667, train f1:85.740, train precision:86.726, train recall:85.978, train kappa:86.152
self.nodes_fc[0]: tensor([ 0.0571, -0.0924], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1300,  0.0406], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0121], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1147, 0.0629], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0382, -0.0654], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0920, -0.1227], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:56 step:6 train loss:0.440586, train acc:86.307, train f1:85.683, train precision:86.806, train recall:85.942, train kappa:85.791
self.nodes_fc[0]: tensor([ 0.0572, -0.0925], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1302,  0.0406], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0119], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1147, 0.0629], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0382, -0.0654], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0920, -0.1227], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:56 step:7 train loss:0.437871, train acc:86.560, train f1:85.625, train precision:86.908, train recall:85.770, train kappa:86.045
self.nodes_fc[0]: tensor([ 0.0572, -0.0925], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1302,  0.0406], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0118], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1145, 0.0629], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0383, -0.0654], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0920, -0.1228], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:56 step:8 train loss:0.433963, train acc:86.594, train f1:85.642, train precision:86.963, train recall:85.783, train kappa:86.071
self.nodes_fc[0]: tensor([ 0.0572, -0.0926], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1302,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1343, -0.0119], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1144, 0.0629], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0383, -0.0654], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0920, -0.1228], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:56 step:9 train loss:0.437557, train acc:86.383, train f1:85.687, train precision:86.908, train recall:85.743, train kappa:85.868
self.nodes_fc[0]: tensor([ 0.0572, -0.0926], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1301,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0119], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1143, 0.0629], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0383, -0.0655], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0920, -0.1228], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:56 step:10 train loss:0.423109, train acc:86.880, train f1:86.062, train precision:87.003, train recall:86.386, train kappa:86.378
self.nodes_fc[0]: tensor([ 0.0572, -0.0926], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1300,  0.0408], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0119], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1143, 0.0630], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0383, -0.0656], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0920, -0.1229], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:56 step:11 train loss:0.442925, train acc:86.266, train f1:85.480, train precision:86.447, train recall:85.941, train kappa:85.745
self.nodes_fc[0]: tensor([ 0.0572, -0.0925], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1299,  0.0408], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1340, -0.0119], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1144, 0.0628], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0384, -0.0656], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0921, -0.1229], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0572, -0.0925], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1299,  0.0408], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1340, -0.0119], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1144, 0.0628], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0384, -0.0656], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0921, -0.1229], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:56        valid loss:0.656318, valid acc:83.606, valid f1:60.831, valid precision:57.747, valid recall:70.438, valid kappa:81.531
[1;31mTest score increased (83.544278 --> 83.605619).[0m
[83.6056188275707, 60.83068283970804, 57.74747255618337, 70.43849817675898, 81.53103135271361]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0572, -0.0925], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1299,  0.0408], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1340, -0.0119], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1144, 0.0628], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0384, -0.0656], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0921, -0.1229], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:57 step:0 train loss:0.424737, train acc:86.966, train f1:86.067, train precision:87.135, train recall:86.231, train kappa:86.463
self.nodes_fc[0]: tensor([ 0.0571, -0.0925], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1297,  0.0408], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1339, -0.0118], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1143, 0.0627], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0384, -0.0655], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0921, -0.1230], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:57 step:1 train loss:0.435047, train acc:86.642, train f1:85.939, train precision:87.159, train recall:86.036, train kappa:86.129
self.nodes_fc[0]: tensor([ 0.0570, -0.0925], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1297,  0.0408], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1338, -0.0116], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1143, 0.0625], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0385, -0.0655], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0921, -0.1230], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:57 step:2 train loss:0.427629, train acc:86.880, train f1:86.116, train precision:87.330, train recall:86.228, train kappa:86.377
self.nodes_fc[0]: tensor([ 0.0570, -0.0925], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1296,  0.0408], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1338, -0.0114], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1143, 0.0624], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0655], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0921, -0.1231], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:57 step:3 train loss:0.420946, train acc:87.146, train f1:86.351, train precision:87.457, train recall:86.454, train kappa:86.653
self.nodes_fc[0]: tensor([ 0.0569, -0.0926], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1295,  0.0408], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1337, -0.0113], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1143, 0.0623], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0656], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0921, -0.1231], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:57 step:4 train loss:0.431012, train acc:86.786, train f1:85.807, train precision:86.974, train recall:85.932, train kappa:86.277
self.nodes_fc[0]: tensor([ 0.0569, -0.0927], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1294,  0.0408], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1336, -0.0113], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1143, 0.0622], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0387, -0.0657], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0921, -0.1232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:57 step:5 train loss:0.430055, train acc:86.685, train f1:85.777, train precision:87.169, train recall:85.832, train kappa:86.172
self.nodes_fc[0]: tensor([ 0.0569, -0.0928], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1292,  0.0408], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1336, -0.0114], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1144, 0.0621], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0388, -0.0658], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0921, -0.1232], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:57 step:6 train loss:0.436467, train acc:86.810, train f1:85.944, train precision:87.480, train recall:86.063, train kappa:86.321
self.nodes_fc[0]: tensor([ 0.0568, -0.0929], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1291,  0.0408], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1336, -0.0115], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1146, 0.0620], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0388, -0.0659], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0921, -0.1233], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:57 step:7 train loss:0.425408, train acc:86.874, train f1:85.847, train precision:86.993, train recall:86.212, train kappa:86.376
self.nodes_fc[0]: tensor([ 0.0568, -0.0929], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1290,  0.0408], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1337, -0.0116], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1148, 0.0619], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0660], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0921, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:57 step:8 train loss:0.439087, train acc:86.685, train f1:85.845, train precision:86.918, train recall:86.282, train kappa:86.181
self.nodes_fc[0]: tensor([ 0.0567, -0.0931], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1289,  0.0409], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1337, -0.0118], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1150, 0.0618], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0661], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0920, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:57 step:9 train loss:0.427115, train acc:86.789, train f1:85.990, train precision:86.816, train recall:86.371, train kappa:86.285
self.nodes_fc[0]: tensor([ 0.0567, -0.0931], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1289,  0.0409], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1336, -0.0122], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1153, 0.0617], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0661], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0920, -0.1234], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:57 step:10 train loss:0.442309, train acc:86.398, train f1:85.639, train precision:86.711, train recall:85.948, train kappa:85.874
self.nodes_fc[0]: tensor([ 0.0566, -0.0931], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1288,  0.0409], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1337, -0.0124], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1155, 0.0616], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0661], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0919, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:57 step:11 train loss:0.427763, train acc:86.690, train f1:85.880, train precision:87.258, train recall:86.006, train kappa:86.186
self.nodes_fc[0]: tensor([ 0.0566, -0.0930], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1289,  0.0409], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1338, -0.0125], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1157, 0.0614], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0660], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0919, -0.1235], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0566, -0.0930], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1289,  0.0409], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1338, -0.0125], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1157, 0.0614], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0660], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0919, -0.1235], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:57        valid loss:0.656850, valid acc:83.667, valid f1:60.873, valid precision:57.731, valid recall:70.499, valid kappa:81.599
[1;31mTest score increased (83.605619 --> 83.666960).[0m
[83.66695973991453, 60.87271175923783, 57.73133843999548, 70.49858297933396, 81.59918854851972]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0566, -0.0930], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1289,  0.0409], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1338, -0.0125], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1157, 0.0614], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0660], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0919, -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:58 step:0 train loss:0.426549, train acc:86.893, train f1:86.131, train precision:87.499, train recall:86.065, train kappa:86.376
self.nodes_fc[0]: tensor([ 0.0565, -0.0929], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1290,  0.0409], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1340, -0.0126], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1158, 0.0613], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0659], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0918, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:58 step:1 train loss:0.423347, train acc:87.051, train f1:86.285, train precision:87.385, train recall:86.386, train kappa:86.562
self.nodes_fc[0]: tensor([ 0.0564, -0.0927], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1292,  0.0409], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0127], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1159, 0.0612], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0659], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0918, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:58 step:2 train loss:0.430958, train acc:86.499, train f1:85.701, train precision:87.182, train recall:85.712, train kappa:85.986
self.nodes_fc[0]: tensor([ 0.0561, -0.0926], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1292,  0.0409], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0129], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0611], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0658], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0917, -0.1236], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:58 step:3 train loss:0.435616, train acc:86.691, train f1:85.943, train precision:87.444, train recall:86.034, train kappa:86.178
self.nodes_fc[0]: tensor([ 0.0560, -0.0925], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1291,  0.0409], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0132], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0609], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0658], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0917, -0.1237], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:58 step:4 train loss:0.425894, train acc:86.868, train f1:86.067, train precision:87.185, train recall:86.218, train kappa:86.355
self.nodes_fc[0]: tensor([ 0.0559, -0.0924], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1291,  0.0409], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0132], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0608], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0658], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0917, -0.1237], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:58 step:5 train loss:0.428403, train acc:86.859, train f1:85.988, train precision:86.927, train recall:86.298, train kappa:86.365
self.nodes_fc[0]: tensor([ 0.0559, -0.0922], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1291,  0.0409], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0134], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0606], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0658], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0917, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:58 step:6 train loss:0.417751, train acc:87.149, train f1:86.122, train precision:87.183, train recall:86.311, train kappa:86.661
self.nodes_fc[0]: tensor([ 0.0559, -0.0921], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1291,  0.0409], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1343, -0.0134], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0605], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0658], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0917, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:58 step:7 train loss:0.415870, train acc:87.106, train f1:86.242, train precision:87.626, train recall:86.463, train kappa:86.618
self.nodes_fc[0]: tensor([ 0.0559, -0.0919], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1291,  0.0409], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0136], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0604], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0659], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0917, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:58 step:8 train loss:0.427735, train acc:86.765, train f1:85.933, train precision:87.161, train recall:86.209, train kappa:86.261
self.nodes_fc[0]: tensor([ 0.0560, -0.0918], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1291,  0.0408], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1340, -0.0136], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0605], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0659], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0916, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:58 step:9 train loss:0.423484, train acc:87.082, train f1:86.191, train precision:87.533, train recall:86.316, train kappa:86.597
self.nodes_fc[0]: tensor([ 0.0560, -0.0917], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1290,  0.0408], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1339, -0.0135], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0605], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0659], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0916, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:58 step:10 train loss:0.437657, train acc:86.469, train f1:85.695, train precision:86.767, train recall:85.831, train kappa:85.941
self.nodes_fc[0]: tensor([ 0.0560, -0.0917], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1289,  0.0408], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1338, -0.0134], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0606], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0659], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0916, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:58 step:11 train loss:0.434316, train acc:86.729, train f1:86.111, train precision:87.461, train recall:86.199, train kappa:86.207
self.nodes_fc[0]: tensor([ 0.0561, -0.0918], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1287,  0.0407], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1338, -0.0133], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0606], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0659], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0915, -0.1238], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0561, -0.0918], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1287,  0.0407], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1338, -0.0133], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0606], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0659], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0915, -0.1238], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:58        valid loss:0.664765, valid acc:83.471, valid f1:60.302, valid precision:57.091, valid recall:70.295, valid kappa:81.382
[1;31mEarlyStopping counter: 1 out of 50[0m
[83.66695973991453, 60.87271175923783, 57.73133843999548, 70.49858297933396, 81.59918854851972]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0561, -0.0918], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1287,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1338, -0.0133], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0606], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0659], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0915, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:59 step:0 train loss:0.424049, train acc:87.033, train f1:86.273, train precision:86.876, train recall:86.766, train kappa:86.525
self.nodes_fc[0]: tensor([ 0.0561, -0.0919], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1287,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1338, -0.0132], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0608], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0661], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0915, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:59 step:1 train loss:0.416663, train acc:87.070, train f1:86.193, train precision:87.028, train recall:86.464, train kappa:86.574
self.nodes_fc[0]: tensor([ 0.0560, -0.0921], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1287,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1339, -0.0131], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0608], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0662], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0914, -0.1239], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:59 step:2 train loss:0.421785, train acc:86.990, train f1:86.316, train precision:87.248, train recall:86.575, train kappa:86.490
self.nodes_fc[0]: tensor([ 0.0560, -0.0923], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1287,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0130], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0610], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0663], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0914, -0.1239], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:59 step:3 train loss:0.419542, train acc:86.908, train f1:85.869, train precision:86.934, train recall:86.096, train kappa:86.408
self.nodes_fc[0]: tensor([ 0.0560, -0.0924], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1288,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1343, -0.0130], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0612], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0664], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0914, -0.1239], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:59 step:4 train loss:0.417521, train acc:87.030, train f1:86.254, train precision:87.356, train recall:86.355, train kappa:86.533
self.nodes_fc[0]: tensor([ 0.0560, -0.0925], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1288,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0129], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0614], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0665], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0914, -0.1239], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:59 step:5 train loss:0.426318, train acc:86.938, train f1:86.189, train precision:87.691, train recall:86.196, train kappa:86.432
self.nodes_fc[0]: tensor([ 0.0561, -0.0925], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1289,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1347, -0.0128], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0616], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0665], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0914, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:59 step:6 train loss:0.428467, train acc:86.844, train f1:86.246, train precision:87.652, train recall:86.206, train kappa:86.346
self.nodes_fc[0]: tensor([ 0.0561, -0.0924], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1289,  0.0406], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1346, -0.0127], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0619], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0665], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0914, -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:59 step:7 train loss:0.423600, train acc:86.868, train f1:85.930, train precision:87.207, train recall:86.125, train kappa:86.368
self.nodes_fc[0]: tensor([ 0.0561, -0.0923], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1289,  0.0406], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1346, -0.0127], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0620], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0665], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0914, -0.1239], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:59 step:8 train loss:0.419633, train acc:87.143, train f1:86.251, train precision:87.519, train recall:86.349, train kappa:86.661
self.nodes_fc[0]: tensor([ 0.0561, -0.0922], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1289,  0.0406], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0127], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0622], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0665], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0914, -0.1239], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:59 step:9 train loss:0.416636, train acc:87.225, train f1:86.517, train precision:87.369, train recall:86.862, train kappa:86.742
self.nodes_fc[0]: tensor([ 0.0561, -0.0922], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1290,  0.0405], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1343, -0.0128], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0625], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0666], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0913, -0.1240], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:59 step:10 train loss:0.413895, train acc:87.070, train f1:86.314, train precision:87.328, train recall:86.666, train kappa:86.578
self.nodes_fc[0]: tensor([ 0.0560, -0.0922], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1291,  0.0404], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0126], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0629], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0667], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0913, -0.1240], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:59 step:11 train loss:0.421760, train acc:86.662, train f1:86.059, train precision:87.360, train recall:86.135, train kappa:86.169
self.nodes_fc[0]: tensor([ 0.0559, -0.0923], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1294,  0.0403], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0122], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0634], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0392, -0.0668], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0913, -0.1240], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0559, -0.0923], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1294,  0.0403], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0122], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0634], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0392, -0.0668], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0913, -0.1240], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:59        valid loss:0.667186, valid acc:83.475, valid f1:60.364, valid precision:57.066, valid recall:70.375, valid kappa:81.390
[1;31mEarlyStopping counter: 2 out of 50[0m
[83.66695973991453, 60.87271175923783, 57.73133843999548, 70.49858297933396, 81.59918854851972]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0559, -0.0923], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1294,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0122], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0634], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0392, -0.0668], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0913, -0.1240], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:60 step:0 train loss:0.401777, train acc:87.509, train f1:86.693, train precision:87.651, train recall:86.993, train kappa:87.037
self.nodes_fc[0]: tensor([ 0.0557, -0.0924], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1296,  0.0402], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0117], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0638], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0392, -0.0669], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0913, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:60 step:1 train loss:0.415442, train acc:87.018, train f1:86.203, train precision:87.491, train recall:86.351, train kappa:86.519
self.nodes_fc[0]: tensor([ 0.0556, -0.0924], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1299,  0.0402], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0113], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0642], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0392, -0.0669], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0912, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:60 step:2 train loss:0.420086, train acc:87.003, train f1:86.025, train precision:87.021, train recall:86.330, train kappa:86.496
self.nodes_fc[0]: tensor([ 0.0556, -0.0925], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1300,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0110], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0645], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0392, -0.0669], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0912, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:60 step:3 train loss:0.422590, train acc:87.128, train f1:86.299, train precision:87.447, train recall:86.357, train kappa:86.633
self.nodes_fc[0]: tensor([ 0.0556, -0.0926], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1302,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0108], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1167, 0.0648], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0392, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0912, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:60 step:4 train loss:0.424484, train acc:86.807, train f1:85.847, train precision:87.151, train recall:85.914, train kappa:86.311
self.nodes_fc[0]: tensor([ 0.0556, -0.0926], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1302,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0108], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1167, 0.0650], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0392, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0911, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:60 step:5 train loss:0.417414, train acc:87.119, train f1:86.353, train precision:88.079, train recall:86.306, train kappa:86.617
self.nodes_fc[0]: tensor([ 0.0556, -0.0927], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1303,  0.0402], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1343, -0.0108], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1167, 0.0652], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0392, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0911, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:60 step:6 train loss:0.423470, train acc:86.813, train f1:86.252, train precision:87.683, train recall:86.180, train kappa:86.309
self.nodes_fc[0]: tensor([ 0.0557, -0.0928], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1304,  0.0402], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1343, -0.0108], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0653], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0392, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0911, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:60 step:7 train loss:0.411764, train acc:87.131, train f1:86.184, train precision:87.166, train recall:86.378, train kappa:86.649
self.nodes_fc[0]: tensor([ 0.0558, -0.0929], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1305,  0.0402], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0107], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0653], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0392, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0911, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:60 step:8 train loss:0.414085, train acc:87.170, train f1:86.305, train precision:87.551, train recall:86.551, train kappa:86.677
self.nodes_fc[0]: tensor([ 0.0559, -0.0931], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1305,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0106], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0653], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0911, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:60 step:9 train loss:0.418624, train acc:87.073, train f1:86.357, train precision:87.423, train recall:86.744, train kappa:86.583
self.nodes_fc[0]: tensor([ 0.0560, -0.0932], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1306,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1340, -0.0102], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0652], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0669], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0911, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:60 step:10 train loss:0.422946, train acc:86.902, train f1:86.181, train precision:87.011, train recall:86.574, train kappa:86.399
self.nodes_fc[0]: tensor([ 0.0561, -0.0934], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1308,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1338, -0.0098], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0652], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0669], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0910, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:60 step:11 train loss:0.421939, train acc:87.482, train f1:86.671, train precision:87.699, train recall:86.860, train kappa:87.020
self.nodes_fc[0]: tensor([ 0.0561, -0.0935], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1309,  0.0399], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1336, -0.0093], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0653], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0670], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0910, -0.1242], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0561, -0.0935], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1309,  0.0399], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1336, -0.0093], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0653], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0670], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0910, -0.1242], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:60        valid loss:0.658307, valid acc:83.679, valid f1:60.807, valid precision:57.565, valid recall:70.581, valid kappa:81.616
[1;31mTest score increased (83.666960 --> 83.679228).[0m
[83.6792279223833, 60.80663681378516, 57.56513063953692, 70.58091267722652, 81.61612955883115]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0561, -0.0935], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1309,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1336, -0.0093], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0653], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0910, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:61 step:0 train loss:0.409525, train acc:87.192, train f1:86.369, train precision:87.555, train recall:86.466, train kappa:86.699
self.nodes_fc[0]: tensor([ 0.0561, -0.0936], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1310,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1335, -0.0089], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0653], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0910, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:61 step:1 train loss:0.414390, train acc:87.256, train f1:86.325, train precision:87.565, train recall:86.428, train kappa:86.763
self.nodes_fc[0]: tensor([ 0.0561, -0.0938], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1311,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1333, -0.0085], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0653], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0392, -0.0670], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0909, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:61 step:2 train loss:0.405011, train acc:87.219, train f1:86.383, train precision:87.731, train recall:86.369, train kappa:86.718
self.nodes_fc[0]: tensor([ 0.0562, -0.0939], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1312,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1332, -0.0084], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0653], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0392, -0.0669], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0908, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:61 step:3 train loss:0.407999, train acc:87.347, train f1:86.472, train precision:87.516, train recall:86.518, train kappa:86.869
self.nodes_fc[0]: tensor([ 0.0562, -0.0939], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1311,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1332, -0.0083], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0654], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0668], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0908, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:61 step:4 train loss:0.418649, train acc:86.945, train f1:86.221, train precision:86.914, train recall:86.612, train kappa:86.462
self.nodes_fc[0]: tensor([ 0.0562, -0.0938], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1311,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1332, -0.0084], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1165, 0.0654], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0391, -0.0668], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0907, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:61 step:5 train loss:0.411763, train acc:87.332, train f1:86.521, train precision:87.413, train recall:86.715, train kappa:86.842
self.nodes_fc[0]: tensor([ 0.0562, -0.0937], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1310,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1333, -0.0086], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0654], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0669], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0906, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:61 step:6 train loss:0.417763, train acc:87.103, train f1:86.287, train precision:87.301, train recall:86.517, train kappa:86.628
self.nodes_fc[0]: tensor([ 0.0562, -0.0936], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1310,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1334, -0.0086], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0652], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0390, -0.0669], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0906, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:61 step:7 train loss:0.424237, train acc:86.707, train f1:86.108, train precision:87.384, train recall:86.155, train kappa:86.197
self.nodes_fc[0]: tensor([ 0.0563, -0.0934], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1311,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1335, -0.0084], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0650], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0669], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0906, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:61 step:8 train loss:0.421654, train acc:87.018, train f1:86.356, train precision:87.360, train recall:86.542, train kappa:86.520
self.nodes_fc[0]: tensor([ 0.0564, -0.0933], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1312,  0.0396], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1335, -0.0082], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0648], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0668], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0905, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:61 step:9 train loss:0.425638, train acc:86.926, train f1:86.322, train precision:87.408, train recall:86.561, train kappa:86.426
self.nodes_fc[0]: tensor([ 0.0564, -0.0933], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1315,  0.0395], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1335, -0.0078], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0646], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0667], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0905, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:61 step:10 train loss:0.418089, train acc:87.192, train f1:86.155, train precision:87.467, train recall:86.329, train kappa:86.706
self.nodes_fc[0]: tensor([ 0.0564, -0.0932], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1317,  0.0394], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1336, -0.0075], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0644], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0666], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0905, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:61 step:11 train loss:0.418207, train acc:87.627, train f1:86.777, train precision:87.896, train recall:87.021, train kappa:87.139
self.nodes_fc[0]: tensor([ 0.0564, -0.0932], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1319,  0.0393], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1337, -0.0073], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0641], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0666], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0905, -0.1247], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0564, -0.0932], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1319,  0.0393], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1337, -0.0073], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0641], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0666], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0905, -0.1247], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:61        valid loss:0.662014, valid acc:83.644, valid f1:60.893, valid precision:57.876, valid recall:70.416, valid kappa:81.575
[1;31mEarlyStopping counter: 1 out of 50[0m
[83.6792279223833, 60.80663681378516, 57.56513063953692, 70.58091267722652, 81.61612955883115]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0564, -0.0932], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1319,  0.0393], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1337, -0.0073], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0641], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0666], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0905, -0.1247], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:62 step:0 train loss:0.406558, train acc:87.360, train f1:86.482, train precision:87.464, train recall:86.798, train kappa:86.886
self.nodes_fc[0]: tensor([ 0.0564, -0.0932], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1322,  0.0392], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1338, -0.0071], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1158, 0.0639], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0389, -0.0666], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0905, -0.1248], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:62 step:1 train loss:0.420060, train acc:86.917, train f1:86.058, train precision:87.182, train recall:86.339, train kappa:86.416
self.nodes_fc[0]: tensor([ 0.0564, -0.0932], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1322,  0.0391], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1338, -0.0071], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1157, 0.0636], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0388, -0.0664], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0904, -0.1248], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:62 step:2 train loss:0.411789, train acc:87.323, train f1:86.563, train precision:87.412, train recall:86.865, train kappa:86.839
self.nodes_fc[0]: tensor([ 0.0565, -0.0931], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1321,  0.0392], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1339, -0.0073], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1156, 0.0634], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0388, -0.0663], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0904, -0.1248], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:62 step:3 train loss:0.413070, train acc:87.256, train f1:86.393, train precision:87.569, train recall:86.418, train kappa:86.765
self.nodes_fc[0]: tensor([ 0.0566, -0.0930], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1319,  0.0393], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1340, -0.0075], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1156, 0.0633], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0387, -0.0663], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0904, -0.1248], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:62 step:4 train loss:0.420155, train acc:86.823, train f1:86.208, train precision:87.317, train recall:86.264, train kappa:86.315
self.nodes_fc[0]: tensor([ 0.0567, -0.0929], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1318,  0.0394], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0076], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1158, 0.0630], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0664], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0904, -0.1249], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:62 step:5 train loss:0.402393, train acc:87.357, train f1:86.509, train precision:87.923, train recall:86.488, train kappa:86.870
self.nodes_fc[0]: tensor([ 0.0568, -0.0929], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1317,  0.0395], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1343, -0.0076], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1159, 0.0628], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0385, -0.0664], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0904, -0.1249], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:62 step:6 train loss:0.414355, train acc:87.314, train f1:86.512, train precision:87.883, train recall:86.474, train kappa:86.829
self.nodes_fc[0]: tensor([ 0.0570, -0.0929], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1318,  0.0395], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0073], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0625], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0385, -0.0665], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0904, -0.1249], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:62 step:7 train loss:0.410965, train acc:87.183, train f1:86.302, train precision:87.661, train recall:86.430, train kappa:86.695
self.nodes_fc[0]: tensor([ 0.0571, -0.0928], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1320,  0.0394], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1346, -0.0069], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0623], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0385, -0.0666], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0904, -0.1250], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:62 step:8 train loss:0.413490, train acc:86.945, train f1:86.256, train precision:87.435, train recall:86.430, train kappa:86.454
self.nodes_fc[0]: tensor([ 0.0572, -0.0929], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1322,  0.0394], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1347, -0.0066], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0621], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0384, -0.0667], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0904, -0.1249], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:62 step:9 train loss:0.414746, train acc:87.064, train f1:86.106, train precision:87.004, train recall:86.551, train kappa:86.565
self.nodes_fc[0]: tensor([ 0.0572, -0.0929], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1322,  0.0393], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1348, -0.0065], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0620], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0384, -0.0669], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0904, -0.1249], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:62 step:10 train loss:0.415111, train acc:86.981, train f1:86.078, train precision:86.979, train recall:86.432, train kappa:86.483
self.nodes_fc[0]: tensor([ 0.0573, -0.0929], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1322,  0.0393], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1347, -0.0066], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0620], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0384, -0.0669], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0904, -0.1249], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:62 step:11 train loss:0.421227, train acc:87.163, train f1:86.729, train precision:88.241, train recall:86.943, train kappa:86.669
self.nodes_fc[0]: tensor([ 0.0575, -0.0929], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1320,  0.0393], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1347, -0.0068], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0619], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0385, -0.0671], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0904, -0.1249], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0575, -0.0929], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1320,  0.0393], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1347, -0.0068], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0619], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0385, -0.0671], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0904, -0.1249], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:62        valid loss:0.655644, valid acc:84.037, valid f1:61.271, valid precision:58.308, valid recall:70.301, valid kappa:82.002
[1;31mTest score increased (83.679228 --> 84.037050).[0m
[84.03704991105568, 61.27126542782821, 58.308045840209445, 70.30083540428363, 82.00152730201475]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0575, -0.0929], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1320,  0.0393], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1347, -0.0068], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0619], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0385, -0.0671], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0904, -0.1249], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:63 step:0 train loss:0.404079, train acc:87.448, train f1:86.729, train precision:88.035, train recall:86.804, train kappa:86.976
self.nodes_fc[0]: tensor([ 0.0576, -0.0930], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1319,  0.0394], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1346, -0.0070], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0618], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0385, -0.0672], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0905, -0.1249], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:63 step:1 train loss:0.403503, train acc:87.491, train f1:86.679, train precision:87.981, train recall:86.829, train kappa:87.010
self.nodes_fc[0]: tensor([ 0.0577, -0.0930], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1317,  0.0394], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0073], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1162, 0.0618], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0385, -0.0673], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0905, -0.1249], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:63 step:2 train loss:0.415709, train acc:87.244, train f1:86.494, train precision:87.590, train recall:86.641, train kappa:86.754
self.nodes_fc[0]: tensor([ 0.0577, -0.0930], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1315,  0.0395], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0074], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0617], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0675], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0905, -0.1249], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:63 step:3 train loss:0.405291, train acc:87.427, train f1:86.687, train precision:88.057, train recall:86.628, train kappa:86.940
self.nodes_fc[0]: tensor([ 0.0577, -0.0930], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1314,  0.0396], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0073], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0617], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0677], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0905, -0.1248], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:63 step:4 train loss:0.400124, train acc:87.665, train f1:86.769, train precision:87.944, train recall:86.780, train kappa:87.184
self.nodes_fc[0]: tensor([ 0.0578, -0.0931], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1313,  0.0397], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0071], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0615], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0678], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0906, -0.1248], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:63 step:5 train loss:0.404704, train acc:87.387, train f1:86.616, train precision:87.719, train recall:86.816, train kappa:86.893
self.nodes_fc[0]: tensor([ 0.0578, -0.0931], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1313,  0.0398], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0069], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0614], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0680], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0906, -0.1248], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:63 step:6 train loss:0.418018, train acc:87.283, train f1:86.427, train precision:87.512, train recall:86.479, train kappa:86.793
self.nodes_fc[0]: tensor([ 0.0577, -0.0932], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1314,  0.0399], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0066], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1160, 0.0613], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0681], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0906, -0.1248], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:63 step:7 train loss:0.418938, train acc:87.177, train f1:86.619, train precision:87.489, train recall:86.900, train kappa:86.689
self.nodes_fc[0]: tensor([ 0.0577, -0.0932], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1315,  0.0400], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0064], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1159, 0.0614], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0682], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0907, -0.1248], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:63 step:8 train loss:0.414034, train acc:87.009, train f1:86.434, train precision:87.477, train recall:86.645, train kappa:86.528
self.nodes_fc[0]: tensor([ 0.0578, -0.0933], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1315,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1343, -0.0063], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1159, 0.0614], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0684], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0907, -0.1247], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:63 step:9 train loss:0.407715, train acc:87.604, train f1:86.677, train precision:87.744, train recall:86.960, train kappa:87.138
self.nodes_fc[0]: tensor([ 0.0578, -0.0934], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1315,  0.0401], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0063], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1158, 0.0613], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0685], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0907, -0.1247], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:63 step:10 train loss:0.420433, train acc:86.990, train f1:86.232, train precision:87.376, train recall:86.415, train kappa:86.498
self.nodes_fc[0]: tensor([ 0.0579, -0.0935], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1313,  0.0402], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1347, -0.0064], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1158, 0.0613], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0685], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0907, -0.1247], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:63 step:11 train loss:0.417606, train acc:86.710, train f1:85.950, train precision:87.005, train recall:86.117, train kappa:86.208
self.nodes_fc[0]: tensor([ 0.0580, -0.0936], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1311,  0.0403], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1349, -0.0063], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1158, 0.0612], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0684], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0907, -0.1246], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0580, -0.0936], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1311,  0.0403], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1349, -0.0063], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1158, 0.0612], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0684], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0907, -0.1246], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:63        valid loss:0.662562, valid acc:83.896, valid f1:61.115, valid precision:58.064, valid recall:70.372, valid kappa:81.853
[1;31mEarlyStopping counter: 1 out of 50[0m
[84.03704991105568, 61.27126542782821, 58.308045840209445, 70.30083540428363, 82.00152730201475]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0580, -0.0936], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1311,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1349, -0.0063], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1158, 0.0612], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0386, -0.0684], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0907, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:64 step:0 train loss:0.406828, train acc:87.329, train f1:86.331, train precision:87.657, train recall:86.526, train kappa:86.856
self.nodes_fc[0]: tensor([ 0.0580, -0.0936], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1308,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1351, -0.0063], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1159, 0.0611], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0385, -0.0684], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0907, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:64 step:1 train loss:0.412787, train acc:87.085, train f1:86.286, train precision:87.361, train recall:86.583, train kappa:86.589
self.nodes_fc[0]: tensor([ 0.0580, -0.0937], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1306,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1352, -0.0061], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1159, 0.0611], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0385, -0.0683], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0907, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:64 step:2 train loss:0.420683, train acc:87.061, train f1:86.300, train precision:87.300, train recall:86.534, train kappa:86.558
self.nodes_fc[0]: tensor([ 0.0581, -0.0937], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1304,  0.0404], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1354, -0.0059], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1161, 0.0610], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0384, -0.0682], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0907, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:64 step:3 train loss:0.410087, train acc:87.418, train f1:86.787, train precision:87.630, train recall:87.034, train kappa:86.945
self.nodes_fc[0]: tensor([ 0.0582, -0.0937], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1302,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1355, -0.0057], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1163, 0.0609], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0384, -0.0681], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0907, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:64 step:4 train loss:0.401113, train acc:87.561, train f1:86.603, train precision:87.585, train recall:86.816, train kappa:87.097
self.nodes_fc[0]: tensor([ 0.0583, -0.0937], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1299,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1355, -0.0057], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1164, 0.0609], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0383, -0.0682], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0906, -0.1246], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:64 step:5 train loss:0.401107, train acc:87.521, train f1:86.648, train precision:87.647, train recall:86.850, train kappa:87.047
self.nodes_fc[0]: tensor([ 0.0584, -0.0937], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1297,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1354, -0.0058], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0608], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0382, -0.0683], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0906, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:64 step:6 train loss:0.404751, train acc:87.479, train f1:86.570, train precision:87.604, train recall:86.814, train kappa:86.999
self.nodes_fc[0]: tensor([ 0.0586, -0.0938], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1294,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1354, -0.0058], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1167, 0.0607], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0381, -0.0683], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0906, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:64 step:7 train loss:0.409300, train acc:87.213, train f1:86.329, train precision:87.859, train recall:86.411, train kappa:86.707
self.nodes_fc[0]: tensor([ 0.0586, -0.0938], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1292,  0.0403], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1354, -0.0059], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1167, 0.0606], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0380, -0.0685], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0906, -0.1245], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:64 step:8 train loss:0.401951, train acc:87.622, train f1:86.767, train precision:88.166, train recall:86.793, train kappa:87.149
self.nodes_fc[0]: tensor([ 0.0587, -0.0939], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1290,  0.0404], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1354, -0.0059], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1168, 0.0605], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0380, -0.0686], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0906, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:64 step:9 train loss:0.410912, train acc:87.222, train f1:86.594, train precision:87.788, train recall:86.735, train kappa:86.739
self.nodes_fc[0]: tensor([ 0.0587, -0.0938], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1289,  0.0404], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1354, -0.0058], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1167, 0.0605], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0379, -0.0688], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0907, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:64 step:10 train loss:0.408205, train acc:87.164, train f1:86.624, train precision:87.778, train recall:86.534, train kappa:86.672
self.nodes_fc[0]: tensor([ 0.0586, -0.0937], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1289,  0.0404], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1354, -0.0059], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1167, 0.0605], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0379, -0.0690], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0907, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:64 step:11 train loss:0.425083, train acc:86.700, train f1:86.074, train precision:86.916, train recall:86.472, train kappa:86.191
self.nodes_fc[0]: tensor([ 0.0585, -0.0935], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1288,  0.0404], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1355, -0.0059], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1167, 0.0606], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0379, -0.0691], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0907, -0.1244], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0585, -0.0935], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1288,  0.0404], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1355, -0.0059], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1167, 0.0606], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0379, -0.0691], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0907, -0.1244], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:64        valid loss:0.665099, valid acc:83.689, valid f1:60.689, valid precision:57.340, valid recall:70.323, valid kappa:81.629
[1;31mEarlyStopping counter: 2 out of 50[0m
[84.03704991105568, 61.27126542782821, 58.308045840209445, 70.30083540428363, 82.00152730201475]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0585, -0.0935], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1288,  0.0404], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1355, -0.0059], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1167, 0.0606], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0379, -0.0691], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0907, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:65 step:0 train loss:0.415625, train acc:86.948, train f1:85.999, train precision:86.798, train recall:86.323, train kappa:86.442
self.nodes_fc[0]: tensor([ 0.0584, -0.0932], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1287,  0.0405], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1355, -0.0059], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0607], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0380, -0.0692], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0907, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:65 step:1 train loss:0.406239, train acc:87.338, train f1:86.648, train precision:87.439, train recall:86.860, train kappa:86.858
self.nodes_fc[0]: tensor([ 0.0583, -0.0930], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1286,  0.0405], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1356, -0.0060], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0608], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0380, -0.0694], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0907, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:65 step:2 train loss:0.408927, train acc:87.268, train f1:86.480, train precision:87.760, train recall:86.510, train kappa:86.784
self.nodes_fc[0]: tensor([ 0.0582, -0.0927], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1286,  0.0405], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1357, -0.0060], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0610], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0380, -0.0696], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0908, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:65 step:3 train loss:0.407475, train acc:87.274, train f1:86.442, train precision:87.680, train recall:86.512, train kappa:86.783
self.nodes_fc[0]: tensor([ 0.0580, -0.0925], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1285,  0.0405], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1358, -0.0061], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1166, 0.0612], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0381, -0.0699], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0908, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:65 step:4 train loss:0.394315, train acc:87.823, train f1:86.991, train precision:88.307, train recall:87.037, train kappa:87.358
self.nodes_fc[0]: tensor([ 0.0578, -0.0923], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1284,  0.0405], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1358, -0.0061], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1167, 0.0614], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0382, -0.0701], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0909, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:65 step:5 train loss:0.403956, train acc:87.265, train f1:86.516, train precision:87.496, train recall:86.670, train kappa:86.795
self.nodes_fc[0]: tensor([ 0.0577, -0.0923], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1283,  0.0406], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1357, -0.0061], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1168, 0.0615], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0382, -0.0704], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0909, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:65 step:6 train loss:0.402034, train acc:87.460, train f1:86.628, train precision:87.404, train recall:87.014, train kappa:86.977
self.nodes_fc[0]: tensor([ 0.0576, -0.0923], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1283,  0.0406], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1357, -0.0061], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1169, 0.0615], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0383, -0.0706], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0909, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:65 step:7 train loss:0.399375, train acc:87.549, train f1:86.734, train precision:87.386, train recall:87.222, train kappa:87.082
self.nodes_fc[0]: tensor([ 0.0575, -0.0924], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1284,  0.0406], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1355, -0.0061], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1171, 0.0616], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0383, -0.0708], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0909, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:65 step:8 train loss:0.404416, train acc:87.344, train f1:86.688, train precision:87.777, train recall:87.171, train kappa:86.864
self.nodes_fc[0]: tensor([ 0.0575, -0.0925], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1285,  0.0406], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1352, -0.0063], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1173, 0.0618], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0384, -0.0711], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0909, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:65 step:9 train loss:0.410171, train acc:87.354, train f1:86.382, train precision:87.507, train recall:86.617, train kappa:86.875
self.nodes_fc[0]: tensor([ 0.0575, -0.0925], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1287,  0.0406], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1350, -0.0064], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1174, 0.0619], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0384, -0.0713], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0909, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:65 step:10 train loss:0.400537, train acc:87.573, train f1:86.820, train precision:87.802, train recall:87.009, train kappa:87.098
self.nodes_fc[0]: tensor([ 0.0576, -0.0925], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1289,  0.0406], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1347, -0.0066], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1176, 0.0622], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0384, -0.0715], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0909, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:65 step:11 train loss:0.413428, train acc:87.115, train f1:86.396, train precision:88.039, train recall:86.338, train kappa:86.619
self.nodes_fc[0]: tensor([ 0.0576, -0.0926], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1290,  0.0406], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0069], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1176, 0.0622], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0384, -0.0716], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0909, -0.1243], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0576, -0.0926], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1290,  0.0406], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0069], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1176, 0.0622], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0384, -0.0716], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0909, -0.1243], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:65        valid loss:0.656970, valid acc:83.984, valid f1:61.209, valid precision:58.227, valid recall:70.274, valid kappa:81.945
[1;31mEarlyStopping counter: 3 out of 50[0m
[84.03704991105568, 61.27126542782821, 58.308045840209445, 70.30083540428363, 82.00152730201475]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0576, -0.0926], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1290,  0.0406], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1345, -0.0069], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1176, 0.0622], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0384, -0.0716], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0909, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:66 step:0 train loss:0.409062, train acc:87.311, train f1:86.518, train precision:87.911, train recall:86.660, train kappa:86.817
self.nodes_fc[0]: tensor([ 0.0577, -0.0926], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1290,  0.0406], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0073], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1176, 0.0621], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0383, -0.0717], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0909, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:66 step:1 train loss:0.400068, train acc:87.723, train f1:86.834, train precision:88.093, train recall:87.030, train kappa:87.252
self.nodes_fc[0]: tensor([ 0.0576, -0.0926], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1291,  0.0406], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0076], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1176, 0.0621], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0382, -0.0718], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0909, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:66 step:2 train loss:0.402156, train acc:87.369, train f1:86.757, train precision:87.657, train recall:86.977, train kappa:86.898
self.nodes_fc[0]: tensor([ 0.0577, -0.0926], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1291,  0.0406], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0078], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1177, 0.0621], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0381, -0.0720], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0908, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:66 step:3 train loss:0.392121, train acc:87.698, train f1:86.815, train precision:87.677, train recall:87.032, train kappa:87.232
self.nodes_fc[0]: tensor([ 0.0576, -0.0925], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1290,  0.0406], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1344, -0.0079], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1177, 0.0620], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0381, -0.0721], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0908, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:66 step:4 train loss:0.409249, train acc:87.387, train f1:86.802, train precision:87.423, train recall:87.175, train kappa:86.911
self.nodes_fc[0]: tensor([ 0.0575, -0.0925], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1290,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1343, -0.0078], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1177, 0.0619], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0381, -0.0722], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0908, -0.1244], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:66 step:5 train loss:0.401014, train acc:87.366, train f1:86.602, train precision:87.444, train recall:86.755, train kappa:86.881
self.nodes_fc[0]: tensor([ 0.0574, -0.0926], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1290,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1342, -0.0077], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1177, 0.0618], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0381, -0.0723], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0908, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:66 step:6 train loss:0.405038, train acc:87.396, train f1:86.716, train precision:87.580, train recall:86.976, train kappa:86.916
self.nodes_fc[0]: tensor([ 0.0572, -0.0927], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1291,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0075], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1177, 0.0616], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0381, -0.0724], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0908, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:66 step:7 train loss:0.402826, train acc:87.244, train f1:86.545, train precision:87.742, train recall:86.629, train kappa:86.763
self.nodes_fc[0]: tensor([ 0.0572, -0.0927], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1292,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1340, -0.0073], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1176, 0.0615], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0381, -0.0725], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0908, -0.1243], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:66 step:8 train loss:0.391824, train acc:87.750, train f1:86.838, train precision:88.014, train recall:86.883, train kappa:87.280
self.nodes_fc[0]: tensor([ 0.0570, -0.0928], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1293,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1339, -0.0072], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1176, 0.0615], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0381, -0.0726], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0907, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:66 step:9 train loss:0.400201, train acc:87.430, train f1:86.646, train precision:88.065, train recall:86.640, train kappa:86.942
self.nodes_fc[0]: tensor([ 0.0569, -0.0928], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1293,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1340, -0.0069], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1176, 0.0616], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0380, -0.0728], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0907, -0.1242], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:66 step:10 train loss:0.410203, train acc:87.238, train f1:86.394, train precision:87.565, train recall:86.502, train kappa:86.752
self.nodes_fc[0]: tensor([ 0.0569, -0.0928], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1295,  0.0407], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1341, -0.0066], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1175, 0.0617], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0380, -0.0730], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0907, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([10361, 86])
ty.shape: torch.Size([10361])
ty.min(): 0
ty.max(): 85
fold:0 epoch:66 step:11 train loss:0.416887, train acc:87.221, train f1:86.497, train precision:87.290, train recall:86.972, train kappa:86.740
self.nodes_fc[0]: tensor([ 0.0569, -0.0927], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1295,  0.0408], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1343, -0.0064], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1173, 0.0618], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0379, -0.0733], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0906, -0.1241], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
self.nodes_fc[0]: tensor([ 0.0569, -0.0927], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1295,  0.0408], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1343, -0.0064], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1173, 0.0618], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0379, -0.0733], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0906, -0.1241], device='cuda:0', requires_grad=True)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:66        valid loss:0.664536, valid acc:83.767, valid f1:60.938, valid precision:57.746, valid recall:70.402, valid kappa:81.711
[1;31mEarlyStopping counter: 4 out of 50[0m
[84.03704991105568, 61.27126542782821, 58.308045840209445, 70.30083540428363, 82.00152730201475]
====================================================================================================
self.nodes_fc[0]: tensor([ 0.0569, -0.0927], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([-0.1295,  0.0408], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.1343, -0.0064], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([15, 300])
nodes_feat.shape: torch.Size([98743, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
self.nodes_fc[0]: tensor([0.1173, 0.0618], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0379, -0.0733], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
self.nodes_fc[0]: tensor([ 0.0906, -0.1241], device='cuda:0', grad_fn=<SliceBackward0>)
NaN in self.nodes_fc[0]: tensor(False, device='cuda:0')
Inf in self.nodes_fc[0]: tensor(False, device='cuda:0')
self.nodes_fc.shape: torch.Size([88, 300])
nodes_feat.shape: torch.Size([47600, 300])
self.nodes_attn.shape: torch.Size([1, 300])
nodes_feat device: cuda:0
self.nodes_attn device: cuda:0
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
output.shape: torch.Size([32768, 86])
ty.shape: torch.Size([32768])
ty.min(): 0
ty.max(): 85
fold:0 epoch:67 step:0 train loss:0.396877, train acc:87.680, train f1:87.014, train precision:87.834, train recall:87.305, train kappa:87.208

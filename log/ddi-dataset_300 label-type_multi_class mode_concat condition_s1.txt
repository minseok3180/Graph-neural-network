running on cuda:0
â–¶ Experiment Parameters
     batch_size: 32768
       fold_num: 5
     hidden_dim: 300
      num_layer: 3
          epoch: 100
       patience: 50
             lr: 0.001
   weight_decay: 1e-05
     label_type: multi_class
      condition: s1
           mode: concat
      data_path: ./data
        kg_name: FOODRKG
       ddi_name: DrugBank
            set: all
train setting...
HetDDI(
  (kg): HGNN(
    (dropout): Dropout(p=0.2, inplace=False)
    (node_embedding): Embedding(98744, 300)
    (gat_layers): ModuleList(
      (0): HetConv(
        (edge_embedding): Embedding(109, 300)
        (bn): Sequential(
          (0): Linear(in_features=300, out_features=300, bias=True)
          (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (leaky_relu): LeakyReLU(negative_slope=0.02)
      )
      (1): HetConv(
        (edge_embedding): Embedding(109, 300)
        (bn): Sequential(
          (0): Linear(in_features=300, out_features=300, bias=True)
          (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (leaky_relu): LeakyReLU(negative_slope=0.02)
      )
      (2): HetConv(
        (edge_embedding): Embedding(109, 300)
        (bn): Sequential(
          (0): Linear(in_features=300, out_features=300, bias=True)
          (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (leaky_relu): LeakyReLU(negative_slope=0.02)
      )
    )
  )
  (kg_fc): Sequential(
    (0): Linear(in_features=300, out_features=300, bias=True)
    (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Dropout(p=0.1, inplace=False)
    (3): ReLU()
    (4): Linear(in_features=300, out_features=300, bias=True)
    (5): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Dropout(p=0.1, inplace=False)
    (7): ReLU()
    (8): Linear(in_features=300, out_features=300, bias=True)
    (9): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Dropout(p=0.1, inplace=False)
    (11): ReLU()
  )
  (mol): Mol(
    (readout): AvgPooling()
    (gnn): HGNN(
      (dropout): Dropout(p=0.2, inplace=False)
      (node_embedding): Embedding(47601, 300)
      (gat_layers): ModuleList(
        (0): HetConv(
          (edge_embedding): Embedding(5, 300)
          (bn): Sequential(
            (0): Linear(in_features=300, out_features=300, bias=True)
            (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (leaky_relu): LeakyReLU(negative_slope=0.02)
        )
        (1): HetConv(
          (edge_embedding): Embedding(5, 300)
          (bn): Sequential(
            (0): Linear(in_features=300, out_features=300, bias=True)
            (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (leaky_relu): LeakyReLU(negative_slope=0.02)
        )
        (2): HetConv(
          (edge_embedding): Embedding(5, 300)
          (bn): Sequential(
            (0): Linear(in_features=300, out_features=300, bias=True)
            (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (leaky_relu): LeakyReLU(negative_slope=0.02)
        )
      )
    )
  )
  (mol_fc): Sequential(
    (0): Linear(in_features=300, out_features=300, bias=True)
    (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Dropout(p=0.1, inplace=False)
    (3): ReLU()
    (4): Linear(in_features=300, out_features=300, bias=True)
    (5): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Dropout(p=0.1, inplace=False)
    (7): ReLU()
    (8): Linear(in_features=300, out_features=300, bias=True)
    (9): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Dropout(p=0.1, inplace=False)
    (11): ReLU()
  )
  (decoder): Mlp(
    (fc_layer): Sequential(
      (0): Linear(in_features=1200, out_features=1200, bias=True)
      (1): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.5, inplace=False)
      (4): Linear(in_features=1200, out_features=1200, bias=True)
      (5): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): ReLU()
      (7): Dropout(p=0.5, inplace=False)
      (8): Linear(in_features=1200, out_features=1200, bias=True)
      (9): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): ReLU()
      (11): Dropout(p=0.5, inplace=False)
    )
    (output_layer): Sequential(
      (0): Linear(in_features=1200, out_features=86, bias=False)
    )
  )
)
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:0 step:0 train loss:4.547227, train acc:1.819, train f1:0.891, train precision:1.156, train recall:1.209, train kappa:0.091
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:0 step:1 train loss:4.247983, train acc:11.148, train f1:1.319, train precision:1.746, train recall:1.955, train kappa:2.700
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:0 step:2 train loss:4.023349, train acc:14.035, train f1:1.689, train precision:1.979, train recall:2.444, train kappa:4.594
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:0 step:3 train loss:3.910156, train acc:15.726, train f1:2.079, train precision:3.696, train recall:2.956, train kappa:6.032
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:0 step:4 train loss:3.856239, train acc:16.904, train f1:2.976, train precision:5.096, train recall:3.612, train kappa:6.830
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:0 step:5 train loss:3.819167, train acc:16.537, train f1:3.476, train precision:5.160, train recall:3.942, train kappa:7.453
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:0 step:6 train loss:3.729437, train acc:18.063, train f1:4.357, train precision:7.135, train recall:4.768, train kappa:8.786
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:0 step:7 train loss:3.688593, train acc:18.942, train f1:4.694, train precision:8.371, train recall:5.088, train kappa:9.907
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:0 step:8 train loss:3.614374, train acc:20.044, train f1:5.152, train precision:11.016, train recall:5.551, train kappa:11.451
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:0 step:9 train loss:3.553186, train acc:20.660, train f1:5.810, train precision:9.811, train recall:6.007, train kappa:12.386
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:0 step:10 train loss:3.496513, train acc:21.463, train f1:6.493, train precision:10.387, train recall:6.838, train kappa:13.133
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:0 step:11 train loss:3.438478, train acc:21.870, train f1:7.414, train precision:13.142, train recall:7.698, train kappa:14.052
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:0        valid loss:3.354579, valid acc:26.925, valid f1:2.248, valid precision:3.002, valid recall:3.013, valid kappa:11.449
None
====================================================================================================
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:1 step:0 train loss:3.333739, train acc:24.277, train f1:8.779, train precision:14.383, train recall:9.038, train kappa:16.716
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:1 step:1 train loss:3.278725, train acc:24.881, train f1:9.693, train precision:15.834, train recall:10.027, train kappa:17.482
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:1 step:2 train loss:3.197378, train acc:26.782, train f1:11.519, train precision:15.502, train recall:11.867, train kappa:19.736
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:1 step:3 train loss:3.134800, train acc:27.585, train f1:11.434, train precision:16.984, train recall:12.339, train kappa:20.566
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:1 step:4 train loss:3.113253, train acc:27.307, train f1:11.529, train precision:17.275, train recall:12.851, train kappa:20.472
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:1 step:5 train loss:3.023344, train acc:28.549, train f1:13.094, train precision:21.378, train recall:13.875, train kappa:21.784
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:1 step:6 train loss:2.969625, train acc:28.723, train f1:13.860, train precision:20.410, train recall:14.507, train kappa:22.224
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:1 step:7 train loss:2.917740, train acc:29.779, train f1:15.417, train precision:23.650, train recall:15.598, train kappa:23.517
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:1 step:8 train loss:2.846430, train acc:30.609, train f1:16.716, train precision:26.970, train recall:16.502, train kappa:24.509
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:1 step:9 train loss:2.792665, train acc:31.937, train f1:18.198, train precision:28.977, train recall:17.666, train kappa:26.112
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:1 step:10 train loss:2.737561, train acc:32.471, train f1:19.428, train precision:28.891, train recall:18.796, train kappa:26.966
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:1 step:11 train loss:2.707154, train acc:32.854, train f1:20.617, train precision:32.865, train recall:20.523, train kappa:27.604
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:1        valid loss:3.927294, valid acc:17.529, valid f1:2.086, valid precision:3.379, valid recall:3.057, valid kappa:5.540
None
====================================================================================================
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:2 step:0 train loss:2.635713, train acc:34.134, train f1:22.166, train precision:31.563, train recall:21.305, train kappa:29.054
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:2 step:1 train loss:2.616971, train acc:34.579, train f1:23.939, train precision:31.757, train recall:23.200, train kappa:29.918
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:2 step:2 train loss:2.519227, train acc:36.670, train f1:26.078, train precision:36.478, train recall:25.671, train kappa:32.302
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:2 step:3 train loss:2.473135, train acc:38.104, train f1:27.563, train precision:34.269, train recall:27.089, train kappa:33.900
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:2 step:4 train loss:2.424838, train acc:38.791, train f1:29.188, train precision:35.975, train recall:28.758, train kappa:34.811
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:2 step:5 train loss:2.416216, train acc:38.702, train f1:29.303, train precision:36.907, train recall:28.725, train kappa:34.818
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:2 step:6 train loss:2.378179, train acc:39.487, train f1:30.787, train precision:39.274, train recall:30.113, train kappa:35.787
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
fold:0 epoch:2 step:7 train loss:2.315157, train acc:40.726, train f1:32.312, train precision:40.257, train recall:31.335, train kappa:37.197
kg_emb.shape: torch.Size([98743, 300])
kg_emb.shape after fc: torch.Size([98743, 300])
mol_emb.shape before fc: torch.Size([1706, 300])
mol_emb.shape: torch.Size([1706, 300])
